#!/usr/bin/env python3
"""
LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯• v2

æµ‹è¯•æŒ‡æ ‡ï¼š
- TTFT (Time to First Token): é¦–å­—å»¶è¿Ÿ
- Generation Speed: ç”Ÿæˆé€Ÿåº¦ (tokens/s)
- Max Concurrent Requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
- Total Throughput: æ€»ååé‡ (tokens/s)
"""

import argparse
import asyncio
import statistics
import time
from dataclasses import dataclass, field
from typing import List, Optional

from openai import OpenAI, AsyncOpenAI


# ============ é…ç½® ============

API_KEY = "not-needed"
BASE_URL = "http://10.1.1.212:8080/v1"
MODEL = "Qwen2.5-32B-Instruct"
MAX_TOKENS = 2048
TEST_PROMPT = "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ï¼ŒåŒ…æ‹¬é‡å­æ¯”ç‰¹ã€å åŠ æ€ã€çº ç¼ ç­‰æ¦‚å¿µï¼Œä»¥åŠå®ƒä¸ç»å…¸è®¡ç®—çš„åŒºåˆ«ã€‚"

# å¹¶å‘æµ‹è¯•çš„ TTFT é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºä¸å¯ç”¨
TTFT_THRESHOLD = 5.0


# ============ æ•°æ®ç»“æ„ ============

@dataclass
class StreamingMetrics:
    """æµå¼è¾“å‡ºæ€§èƒ½æŒ‡æ ‡"""
    ttft: float = 0.0
    total_time: float = 0.0
    generation_time: float = 0.0
    total_tokens: int = 0
    output_tokens: int = 0
    tokens_per_second: float = 0.0
    success: bool = False
    error: Optional[str] = None


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    ttft_results: List[float] = field(default_factory=list)
    speed_results: List[float] = field(default_factory=list)
    total_times: List[float] = field(default_factory=list)
    total_tokens_list: List[int] = field(default_factory=list)
    success_count: int = 0
    fail_count: int = 0
    errors: List[str] = field(default_factory=list)
    # æ–°å¢ï¼šæ€»ååé‡ç›¸å…³
    total_output_tokens: int = 0
    total_duration: float = 0.0


# ============ å•æ¬¡æµå¼è¯·æ±‚ ============

def run_streaming_request(prompt: str, verbose: bool = True) -> StreamingMetrics:
    """æ‰§è¡Œå•æ¬¡æµå¼è¯·æ±‚å¹¶æµ‹é‡æ€§èƒ½æŒ‡æ ‡"""
    metrics = StreamingMetrics()
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    
    try:
        request_start_time = time.perf_counter()
        first_token_received = False
        
        if verbose:
            print("      --- å®æ—¶è¾“å‡º ---", flush=True)
        
        stream = client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=MAX_TOKENS,
            stream=True,
            stream_options={"include_usage": True},
        )
        
        first_token_time = None
        chunk_count = 0
        
        for chunk in stream:
            if hasattr(chunk, 'usage') and chunk.usage:
                metrics.total_tokens = chunk.usage.total_tokens
                metrics.output_tokens = chunk.usage.completion_tokens
            
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                chunk_count += 1
                
                if not first_token_received:
                    metrics.ttft = time.perf_counter() - request_start_time
                    first_token_time = time.perf_counter()
                    first_token_received = True
                    if verbose:
                        print(f"\n      [é¦–å­—åˆ°è¾¾] TTFT={metrics.ttft:.3f}s\n", flush=True)
                
                if verbose:
                    print(content, end="", flush=True)
        
        metrics.total_time = time.perf_counter() - request_start_time
        
        if first_token_time:
            metrics.generation_time = time.perf_counter() - first_token_time
        
        # å¦‚æœ API æ²¡è¿”å› usageï¼Œç”¨ chunk æ•°ä¼°ç®—
        if metrics.output_tokens == 0:
            metrics.output_tokens = chunk_count
        
        if metrics.generation_time > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.generation_time
        elif metrics.total_time > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.total_time
        
        metrics.success = True
        
        if verbose:
            print(f"\n      --- è¾“å‡ºç»“æŸ ---", flush=True)
            print(f"      [å®Œæˆ] æ€»è€—æ—¶ {metrics.total_time:.2f}s, ç”Ÿæˆ {metrics.output_tokens} tokens", flush=True)
        
    except Exception as e:
        metrics.error = f"{type(e).__name__}: {str(e)}"
        if verbose:
            print(f"\n      [é”™è¯¯] {metrics.error}", flush=True)
    
    return metrics


async def run_streaming_request_async(prompt: str, request_id: int = 0, max_tokens: int = 256) -> StreamingMetrics:
    """å¼‚æ­¥æ‰§è¡Œå•æ¬¡æµå¼è¯·æ±‚ï¼ˆç”¨äºå¹¶å‘æµ‹è¯•ï¼‰"""
    metrics = StreamingMetrics()
    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
    
    try:
        request_start_time = time.perf_counter()
        first_token_received = False
        first_token_time = None
        chunk_count = 0  # ç”¨äºä¼°ç®— tokens
        
        stream = await client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            stream=True,
            stream_options={"include_usage": True},
        )
        
        async for chunk in stream:
            if hasattr(chunk, 'usage') and chunk.usage:
                metrics.total_tokens = chunk.usage.total_tokens
                metrics.output_tokens = chunk.usage.completion_tokens
            
            if chunk.choices and chunk.choices[0].delta.content:
                chunk_count += 1  # ç»Ÿè®¡æœ‰å†…å®¹çš„ chunk æ•°
                if not first_token_received:
                    metrics.ttft = time.perf_counter() - request_start_time
                    first_token_time = time.perf_counter()
                    first_token_received = True
        
        metrics.total_time = time.perf_counter() - request_start_time
        if first_token_time:
            metrics.generation_time = time.perf_counter() - first_token_time
        
        # ä¿®å¤ï¼šå¦‚æœ API æ²¡è¿”å› output_tokensï¼Œç”¨ chunk_count ä¼°ç®—
        if metrics.output_tokens == 0:
            metrics.output_tokens = chunk_count
        
        # åŸºäºç”Ÿæˆæ—¶é—´è®¡ç®—é€Ÿåº¦
        if metrics.generation_time > 0 and metrics.output_tokens > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.generation_time
        elif metrics.total_time > 0 and metrics.output_tokens > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.total_time
        
        metrics.success = True
        
    except Exception as e:
        metrics.error = f"{type(e).__name__}: {str(e)}"
    
    return metrics


# ============ TTFT & Speed æµ‹è¯• ============

def run_ttft_speed_benchmark(iterations: int = 3, verbose: bool = True) -> BenchmarkResult:
    """è¿è¡Œ TTFT å’Œ Generation Speed åŸºå‡†æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š TTFT & Generation Speed æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)")
    print(f"{'='*60}\n")
    
    result = BenchmarkResult()
    
    for i in range(iterations):
        print(f"è¿­ä»£ {i+1}/{iterations}:")
        metrics = run_streaming_request(TEST_PROMPT, verbose=verbose)
        
        if metrics.success:
            result.ttft_results.append(metrics.ttft)
            result.speed_results.append(metrics.tokens_per_second)
            result.total_times.append(metrics.total_time)
            result.total_tokens_list.append(metrics.output_tokens)
            result.success_count += 1
            print(f"    TTFT: {metrics.ttft:.3f}s | "
                  f"Speed: {metrics.tokens_per_second:.1f} tokens/s | "
                  f"GenTime: {metrics.generation_time:.2f}s | "
                  f"Tokens: {metrics.output_tokens}")
        else:
            result.fail_count += 1
            result.errors.append(metrics.error or "Unknown error")
            print(f"    âŒ å¤±è´¥: {metrics.error}")
        
        print()
    
    return result


# ============ å¹¶å‘æµ‹è¯• ============

async def run_concurrent_test(concurrency: int, max_tokens: int = 256) -> BenchmarkResult:
    """è¿è¡ŒæŒ‡å®šå¹¶å‘æ•°çš„æµ‹è¯•"""
    result = BenchmarkResult()
    
    start_time = time.perf_counter()
    
    tasks = [
        run_streaming_request_async(TEST_PROMPT, i, max_tokens)
        for i in range(concurrency)
    ]
    
    metrics_list = await asyncio.gather(*tasks, return_exceptions=True)
    
    result.total_duration = time.perf_counter() - start_time
    
    for metrics in metrics_list:
        if isinstance(metrics, Exception):
            result.fail_count += 1
            result.errors.append(str(metrics))
        elif metrics.success:
            result.ttft_results.append(metrics.ttft)
            result.speed_results.append(metrics.tokens_per_second)
            result.total_times.append(metrics.total_time)
            tokens = metrics.output_tokens if metrics.output_tokens > 0 else metrics.total_tokens
            result.total_tokens_list.append(tokens)
            result.total_output_tokens += tokens
            result.success_count += 1
        else:
            result.fail_count += 1
            result.errors.append(metrics.error or "Unknown error")
    
    return result


def run_concurrency_benchmark(max_concurrency: int = 10, step: int = 2, start: int = 1, 
                               max_tokens: int = 256, ttft_threshold: float = TTFT_THRESHOLD) -> dict:
    """è¿è¡Œæœ€å¤§å¹¶å‘è¯·æ±‚æ•°æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Max Concurrent Requests æµ‹è¯• (max_tokens={max_tokens})")
    print(f"   TTFT é˜ˆå€¼: {ttft_threshold}s (è¶…è¿‡è§†ä¸ºä¸å¯ç”¨)")
    print(f"{'='*60}\n")
    
    results = {}
    
    if step == 0:
        concurrency_list = [start]
    else:
        concurrency_list = list(range(start, max_concurrency + 1, step))
        if max_concurrency not in concurrency_list:
            concurrency_list.append(max_concurrency)
    
    max_usable_concurrency = 0
    
    for concurrency in concurrency_list:
        print(f"æµ‹è¯•å¹¶å‘æ•°: {concurrency}")
        
        result = asyncio.run(run_concurrent_test(concurrency, max_tokens))
        results[concurrency] = result
        
        if result.success_count > 0:
            avg_ttft = statistics.mean(result.ttft_results)
            avg_speed = statistics.mean(result.speed_results) if result.speed_results else 0
            total_throughput = result.total_output_tokens / result.total_duration if result.total_duration > 0 else 0
            
            # åˆ¤æ–­æ˜¯å¦å¯ç”¨
            is_usable = avg_ttft <= ttft_threshold and result.fail_count == 0
            status = "âœ…" if is_usable else "âš ï¸"
            
            if is_usable:
                max_usable_concurrency = concurrency
            
            print(f"  {status} æˆåŠŸ: {result.success_count}/{concurrency} | "
                  f"å¹³å‡ TTFT: {avg_ttft:.3f}s | "
                  f"å¹³å‡ Speed: {avg_speed:.1f} tokens/s | "
                  f"æ€»åå: {total_throughput:.1f} tokens/s")
            
            if not is_usable:
                print(f"     (TTFT {avg_ttft:.1f}s > é˜ˆå€¼ {ttft_threshold}sï¼Œè§†ä¸ºä¸å¯ç”¨)")
        else:
            print(f"  âŒ å…¨éƒ¨å¤±è´¥!")
        
        # å¦‚æœå¤±è´¥ç‡è¶…è¿‡ 50%ï¼Œåœæ­¢æµ‹è¯•
        if result.fail_count > concurrency / 2:
            print(f"\nâš ï¸ å¤±è´¥ç‡è¿‡é«˜ï¼Œåœæ­¢å¹¶å‘æµ‹è¯•")
            break
    
    # è®°å½•æœ€å¤§å¯ç”¨å¹¶å‘æ•°
    results['max_usable'] = max_usable_concurrency
    
    return results


# ============ ç»“æœæ±‡æ€» ============

def print_summary(ttft_result: Optional[BenchmarkResult], concurrency_results: Optional[dict] = None,
                  ttft_threshold: float = TTFT_THRESHOLD):
    """æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœæ‘˜è¦")
    print(f"{'='*60}\n")
    
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  - API åœ°å€: {BASE_URL}")
    print(f"  - æ¨¡å‹: {MODEL}")
    if ttft_result:
        print(f"  - æˆåŠŸç‡: {ttft_result.success_count}/{ttft_result.success_count + ttft_result.fail_count}")
    print()
    
    if ttft_result and ttft_result.ttft_results:
        print("ã€TTFT (Time to First Token)ã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.ttft_results):.3f}s")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.ttft_results):.3f}s")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.ttft_results):.3f}s")
        if len(ttft_result.ttft_results) > 1:
            print(f"  æ ‡å‡†å·®: {statistics.stdev(ttft_result.ttft_results):.3f}s")
        print(f"  P50:    {statistics.median(ttft_result.ttft_results):.3f}s")
        print()
    
    if ttft_result and ttft_result.speed_results:
        print("ã€Generation Speedã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.speed_results):.1f} tokens/s")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.speed_results):.1f} tokens/s")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.speed_results):.1f} tokens/s")
        if len(ttft_result.speed_results) > 1:
            print(f"  æ ‡å‡†å·®: {statistics.stdev(ttft_result.speed_results):.1f} tokens/s")
        print(f"  P50:    {statistics.median(ttft_result.speed_results):.1f} tokens/s")
        print()
    
    if ttft_result and ttft_result.total_tokens_list:
        print("ã€Total Tokensã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.total_tokens_list)}")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.total_tokens_list)}")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.total_tokens_list):.0f}")
        print()
    
    if concurrency_results:
        print("ã€Max Concurrent Requestsã€‘")
        max_usable = concurrency_results.get('max_usable', 0)
        
        # æ‰¾å‡ºæŠ€æœ¯ä¸ŠæˆåŠŸçš„æœ€å¤§å¹¶å‘ï¼ˆä¸è€ƒè™‘ TTFTï¼‰
        max_technical = 0
        for concurrency, result in concurrency_results.items():
            if isinstance(concurrency, int) and result.success_count == concurrency:
                max_technical = max(max_technical, concurrency)
        
        print(f"  âœ… æœ€å¤§å¯ç”¨å¹¶å‘æ•° (TTFT<{ttft_threshold}s): {max_usable if max_usable > 0 else 'æ— '}")
        print(f"  ğŸ“Š æœ€å¤§æŠ€æœ¯å¹¶å‘æ•° (ä»…çœ‹æˆåŠŸç‡): {max_technical}")
        
        # æ˜¾ç¤ºå„å¹¶å‘çº§åˆ«çš„ååé‡
        print(f"\n  ã€å„å¹¶å‘çº§åˆ«ååé‡ã€‘")
        for concurrency, result in sorted((k, v) for k, v in concurrency_results.items() if isinstance(k, int)):
            if isinstance(concurrency, int) and result.success_count > 0:
                throughput = result.total_output_tokens / result.total_duration if result.total_duration > 0 else 0
                avg_ttft = statistics.mean(result.ttft_results)
                status = "âœ…" if avg_ttft <= ttft_threshold else "âš ï¸"
                print(f"    {status} {concurrency:4d} å¹¶å‘: {throughput:7.1f} tokens/s (TTFT={avg_ttft:.2f}s)")
        print()
    
    if ttft_result and ttft_result.errors:
        print("ã€é”™è¯¯æ±‡æ€»ã€‘")
        for i, err in enumerate(ttft_result.errors[:5], 1):
            print(f"  {i}. {err}")
        if len(ttft_result.errors) > 5:
            print(f"  ... è¿˜æœ‰ {len(ttft_result.errors) - 5} ä¸ªé”™è¯¯")
        print()


# ============ ä¸»å‡½æ•° ============

def main():
    parser = argparse.ArgumentParser(description="LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯• v2")
    parser.add_argument("-n", "--iterations", type=int, default=3,
                        help="TTFT/Speed æµ‹è¯•è¿­ä»£æ¬¡æ•° (é»˜è®¤: 3)")
    parser.add_argument("-c", "--max-concurrency", type=int, default=0,
                        help="æœ€å¤§å¹¶å‘æµ‹è¯•æ•° (é»˜è®¤: 0, ä¸æµ‹è¯•å¹¶å‘)")
    parser.add_argument("--concurrency-start", type=int, default=1,
                        help="å¹¶å‘æµ‹è¯•èµ·å§‹å€¼ (é»˜è®¤: 1)")
    parser.add_argument("--concurrency-step", type=int, default=2,
                        help="å¹¶å‘æµ‹è¯•æ­¥é•¿ (é»˜è®¤: 2, è®¾ä¸º 0 åˆ™åªæµ‹è¯•èµ·å§‹å€¼)")
    parser.add_argument("--concurrency-only", action="store_true",
                        help="åªè¿è¡Œå¹¶å‘æµ‹è¯•ï¼Œè·³è¿‡ TTFT/Speed æµ‹è¯•")
    parser.add_argument("--concurrency-tokens", type=int, default=256,
                        help="å¹¶å‘æµ‹è¯•æ—¶çš„ max_tokens (é»˜è®¤: 256)")
    parser.add_argument("--ttft-threshold", type=float, default=TTFT_THRESHOLD,
                        help=f"TTFT é˜ˆå€¼ç§’æ•°ï¼Œè¶…è¿‡è§†ä¸ºä¸å¯ç”¨ (é»˜è®¤: {TTFT_THRESHOLD})")
    parser.add_argument("-q", "--quiet", action="store_true",
                        help="å®‰é™æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºå®æ—¶è¾“å‡º")
    parser.add_argument("--prompt", type=str, default=None,
                        help="è‡ªå®šä¹‰æµ‹è¯• prompt")
    args = parser.parse_args()
    
    global TEST_PROMPT
    if args.prompt:
        TEST_PROMPT = args.prompt
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯• v2")
    print(f"{'='*60}\n")
    
    print(f"é…ç½®:")
    print(f"  - API åœ°å€: {BASE_URL}")
    print(f"  - æ¨¡å‹: {MODEL}")
    if not args.concurrency_only:
        print(f"  - TTFT/Speed è¿­ä»£æ¬¡æ•°: {args.iterations}")
    print(f"  - æœ€å¤§å¹¶å‘æµ‹è¯•: {'è·³è¿‡' if args.max_concurrency == 0 else args.max_concurrency}")
    if args.max_concurrency > 0:
        print(f"  - å¹¶å‘æµ‹è¯• max_tokens: {args.concurrency_tokens}")
        print(f"  - TTFT å¯ç”¨é˜ˆå€¼: {args.ttft_threshold}s")
    
    ttft_result = None
    if not args.concurrency_only:
        ttft_result = run_ttft_speed_benchmark(args.iterations, verbose=not args.quiet)
    
    concurrency_results = None
    if args.max_concurrency > 0:
        concurrency_results = run_concurrency_benchmark(
            args.max_concurrency, 
            step=args.concurrency_step,
            start=args.concurrency_start,
            max_tokens=args.concurrency_tokens,
            ttft_threshold=args.ttft_threshold
        )
    
    print_summary(ttft_result, concurrency_results, args.ttft_threshold)


if __name__ == "__main__":
    main()
