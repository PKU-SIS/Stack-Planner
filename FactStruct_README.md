# FactStruct 使用指南

## 🚀 快速开始

基本上沿用Stack-Planner的环境配置即可，只需要多安装几个包。

### 1. 安装依赖

FactStruct Stage 1 依赖以下 Python 包（已在原始的 `requirements.txt` 中追加这部分）：

```bash
# 核心依赖
sentence-transformers  # 用于文档嵌入生成
faiss-cpu             # 用于向量索引和相似度搜索
scikit-learn          # 用于向量计算和相似度度量
loguru                # 用于日志记录
```

如果使用 `uv` 管理依赖，运行：

```bash
uv sync
```

如果使用 `pip`，运行：

```bash
pip install -r requirements.txt
```


### 2. 验证测试

运行诊断脚本检查FactStruct部分是否可正常运行：

```bash
python examples/factstruct_diagnose.py
```

该脚本会检查：
- LLM 配置是否正确
- API key 是否设置
- LLM 连接是否正常


## 实现思路

目前只实现了Stage 1。

#### **Stage 1: 动态大纲生成与优化 (Batch-MAB 框架)**

##### **1. 动机：为什么需要动态大纲？**

本模块旨在解决开放式深度研究（OEDR）中的“冷启动”和“结构构建”问题。传统的 RAG（检索增强生成）系统或简单的报告生成器，往往采用“一次性”大纲：即先根据用户查询生成一个静态大纲，然后才去检索文档。

这种方法的根本缺陷在于**它错误地假设我们在研究开始前就已经知道了答案的轮廓**。在真实的科研（Research）过程中，大纲并不是一成不变的，它是一个随着新信息的发现而不断演进、修正、生长的“活”结构。

因此，我们的核心动机是**必须将“大纲规划”与“信息检索”深度耦合、交错执行 (interleave)**。系统必须能够根据检索到的新证据，动态地、智能地调整其后续的研究方向（即大纲）。

##### **2. 现有工作的不足 (如 WebWeaver)**

现有的动态大纲系统（如 WebWeaver 等）虽然也采用了“边检索边规划”的思想，但它们大多依赖于“蛮力”或“固定”的探索策略。例如：

1. **成本高昂 (High Cost):** 采用“逐节点扩展”（为树中的每个新节点都执行一次检索和 LLM 修订），其 API 总成本与最终大纲的_节点总数_ $O(N_{\text{nodes}})$ 线性相关。
    
2. **策略盲目 (Blind Strategy):** 采用“逐层扩展”（先完成所有 L2 节点，再完成所有 L3 节点），这虽然可能降低成本，但策略是“盲目”的。它无法区分大纲中的“重要分支”和“次要分支”，在两者身上花费了同等的预算。
    

这些方案缺乏一个关键的机制：**智能的“预算分配”**。它们不知道应该在何时“深入探索”（Exploit）一个信息富矿，在何时“浅尝辄止”（Explore）一个信息贫瘠的分支。

##### **3. 我们的方案: Batch-IF-MAB 框架**

为了解决上述问题，我们设计了一个**“批量-信息觅食多臂老虎机” (Batch-IF-MAB)** 框架。该框架旨在完美地平衡一个核心的“铁三角困境” (Trilemma)：

1. **API 成本 (Cost):** 必须严格控制 LLM API 的总调用次数。
    
2. **大纲质量 (Quality):** 必须是自适应的（Adaptive），能智能地将预算花在“信息增益”最高的分支上。
    
3. **上下文纯净度 (Purity):** 必须避免将所有检索到的文档混合在一起，导致 LLM“上下文污染”和“主题漂移”。
    

**Batch-IF-MAB 如何解决这个铁三角问题？**

- **(解决质量问题)** 我们使用 **MAB (UCB1)** 策略，将大纲的叶子节点视为“手臂”。系统会计算每个节点的 UCB 分数（结合“平均奖励”和“探索次数”），从而**智能地识别**出最值得探索的分支。
    
- **(解决成本问题)** 我们引入**批量处理 (Batching)**。我们不一次只选 1 个最优节点（这会导致 $O(T_{\text{max}})$ 次 LLM 调用），而是一次**选择 Top-K 个**（例如 K=5）最优节点。
    
- **(解决纯净度问题)** 我们使用**批量 Prompt** (`batch_refine_outline`)，它强制 LLM 在一次调用中，对 K 个（节点-文档）对进行**独立的、并行的、局部的**优化，从而避免了上下文污染。
    

**成本优势 (示例):**

- 总预算 $T_{max} = 20$ (即总共检索 20 次)
    
- 批量大小 $K = 5$  
    
- 总轮数 = $T_{max} / K = 4$ 轮
    
- **总 LLM 调用 = 1 (初始大纲) + 4 轮** $\times$ **(1 批量查询 + 1 批量修订) = 9 次**
    
- 这（9 次）远低于标准 MAB（41 次）或“逐节点扩展”（$O(N_{\text{nodes}})$）的成本，同时保持了 MAB 的智能性。
    

##### **4. 核心算法流程**

Batch-IF-MAB 的执行流程分为**初始化**和**批量迭代循环**两个阶段。

**A：初始化 (LLM Call #1)**

1. **初始检索:** 接收用户的 `Initial_Query`，检索第一批文档 $D_0$。
    
2. **生成初始大纲:** 调用 `LLM.generate_initial_outline(Query, D_0)`，生成一个基础的大纲树 $O_{\text{root}}$（例如只包含 L1 层节点）。
    
3. **初始化:** 设置总迭代次数 `t = 0`。
    

**B：批量 MAB 迭代循环**

系统将执行固定的 `num_rounds = T_max / K` 轮（例如 4 轮）。在每一轮中，按顺序执行以下步骤：

1. **Step 1: 选择 Top-K“手臂” (UCB1 策略)**
    
    - 系统遍历当前大纲树 $O_{\text{root}}$ 上的所有**叶子节点**（即“手臂”）。
        
    - 为每个叶子节点 $n_i$ 计算其 **UCB 分数**，以平衡“利用”与“探索”：
        
        $$ \text{UCB\_Score}(n_i) = \underbrace{\bar{x}_i}_{\text{Exploitation}} + \underbrace{\sqrt{\frac{2 \ln t}{N_i(t)}}}_{\text{Exploration}}$$
    - $\bar{x}_i$ 是该节点已获得的**平均奖励** (`node.avg_reward()`)。
        
    - $N_i(t)$ 是该节点已被检索的**次数** (`node.pull_count`)。
        
    - 系统选择分数最高的 **Top-K** 个节点（例如 K=5）作为本轮要处理的目标 `SelectedNodes`。
        
2. **Step 2: 批量生成查询 (LLM Call #Round*2)**
    
    - 系统将这 K 个 `SelectedNodes` 传递给 `LLM.batch_generate_queries`。
        
    - 这是一个**单次 LLM 调用**，它接收 K 个节点信息，并一次性返回 K 个对应的搜索查询字符串 `Queries`。
        
3. **Step 3: 并行执行检索**
    
    - 系统调用 `SearchEngine.parallel_search(Queries)`，并行执行 K 次搜索，返回 K 组新文档 `D_new_list`。
        
4. **Step 4: 批量计算奖励与更新状态 (本地计算)**
    
    - 系统在本地循环 K 次（不涉及 API 调用）：
        
    - 对于第 $i$ 个节点 $n_i$ 及其新文档 $D_i$：
        
        - $t += 1$（全局迭代次数加 1）。
            
        - 计算“信息增益”**奖励** $r_t = w_{\text{rel}} \cdot \text{Relevance} + w_{\text{nov}} \cdot \text{Novelty}$。
            
        - 更新该节点 $n_i$ 的 MAB 状态：`n_i.reward_history.append(r_t)` 且 `n_i.pull_count += 1`。
            
5. **Step 5: 批量修订大纲 (LLM Call #Round*2 + 1)**
    
    - 系统调用 `LLM.batch_refine_outline`，这是本轮的**第二次（也是最后一次）LLM 调用**。
        
    - 它将当前大纲树 $O_{\text{root}}$ 和 K 组 `(node, D_new)` 数据对作为输入。
        
    - LLM 被 Prompt 约束，在一次响应中对 K 个节点_分别_进行_独立的_局部优化，最后输出一个**完整的新大纲树** $O_{\text{root}}$。
        
6. **Step 6: MAB 状态继承 (关键机制)**
    
    - `batch_refine_outline` 步骤会返回哪些节点被扩展了的信息（例如 $n$ 扩展出了 $n_a, n_b$）。
        
    - 此时，系统**必须**执行“状态继承”：将父节点 $n$ 的 MAB 状态（`pull_count` 和 `reward_history` 副本）**复制**给所有新生成的子节点 ($n_a, n_b, ...$)。
        
    - **(继承的动机)** 这是确保算法智能性的核心。如果不这样做，新节点 $n_a, n_b$ 的“利用项” $\bar{x}_i$ 将为 0，导致 MAB 策略在下一轮退化为“纯探索”。状态继承确保了父分支的“经验”得以保留和传递。
        

在所有轮次（例如 4 轮）完成后，Stage 1 结束，系统输出最终优化的大纲树 $O_{\text{root}}$。