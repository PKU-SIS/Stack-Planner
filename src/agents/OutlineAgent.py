import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Annotated, Any, Dict, List, Literal, Optional, Type, Union, cast

from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command

from src.agents.sub_agent_registry import get_sub_agents_by_global_type
from src.config.agents import AGENT_LLM_MAP
from src.llms.llm import get_llm_by_type
from src.memory import MemoryStack, MemoryStackEntry
from src.prompts.template import apply_prompt_template, get_prompt_template
from src.utils.json_utils import repair_json_output
from src.utils.logger import logger
from src.utils.statistics import global_statistics
# from src.prompts.central_decision import Decision, DelegateParams
from src.utils.reference_utils import global_reference_map
from ..graph.types import State
# from .SubAgentConfig import get_sub_agents_by_global_type


# -------------------------
# æ ¸å¿ƒæšä¸¾å®šä¹‰
# -------------------------
class OutlineTool(Enum):
    """Outline Agent å¯æ‰§è¡Œçš„ç»“æ„æ€§å·¥å…·"""

    INITIALIZATION = "initialization"
    EXPANDATION = "expandation"
    REDUCTION = "reduction"
    REFLECT = "reflect"
    FINISH = "finish"



# -------------------------
# ä¸­æ¢Agentæ ¸å¿ƒæ¨¡å—--ä¸­æ¢Agentçš„action
# exp: ä¸prompt/Outline_decision.pyä¸­çš„Decisionç±»ä¸åŒçš„æ˜¯ï¼Œé‚£é‡Œæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œè¿™é‡Œæ˜¯æšä¸¾ç±»å‹ï¼Œæ‰€ä»¥è¦å®šä¹‰ä¸¤æ¬¡
# -------------------------
@dataclass
class OutlineToolDecision:
    """Outline Agent çš„å†³ç­–ç»“æœ"""

    tool: OutlineTool                 # ä½¿ç”¨çš„å·¥å…·
    reasoning: str                    # ä¸ºä»€ä¹ˆè¿™ä¹ˆåš
    params: Optional[Dict[str, Any]]  # å·¥å…·å‚æ•°ï¼ˆå„ tool è‡ªå·±è§£é‡Šï¼‰



#ä¸çŸ¥é“ä¸ºå•¥è¦ä¸¤ä¸ªï¼Œé‚£å°±å®ç°æˆä¸¤ä¸ªå§
from pydantic import BaseModel

class OutlineToolDecision_Base(BaseModel):
    tool: Literal[
        "initialization",
        "expandation",
        "reduction",
        "reflect",
        "finish",
    ]
    reasoning: str
    params: Optional[Dict[str, Any]] = None



class OutlineAgent:
    """
    ä¸­æ¢Agentæ ¸å¿ƒç±»ï¼Œè´Ÿè´£ç³»ç»Ÿæ•´ä½“å†³ç­–ä¸ä»»åŠ¡ç¼–æ’

    é‡‡ç”¨åŸºäºè®°å¿†æ ˆçš„å†³ç­–æœºåˆ¶ï¼Œé€šè¿‡çŠ¶æ€åˆ†æåŠ¨æ€å§”æ´¾å­Agentæ‰§è¡Œä¸“é¡¹ä»»åŠ¡ï¼Œ
    å¹¶æœ€ç»ˆæ•´åˆç»“æœç”Ÿæˆå®ŒæˆæŠ¥å‘Š
    """

    def __init__(self, graph_format: str = "sp"):
        self.memory_stack = MemoryStack()
        from src.agents.SubAgentManager import SubAgentManager

        self.sub_agent_manager = SubAgentManager(self)

        sub_agents = get_sub_agents_by_global_type(graph_format)
        logger.info(f"åˆå§‹åŒ–ä¸­æ¢Agentï¼Œä½¿ç”¨å­Agentç±»å‹: {sub_agents}")

        # åˆå§‹åŒ–å­Agentç›¸å…³ä¿¡æ¯
        self.available_sub_agents = [agent["name"] for agent in sub_agents]
        self.sub_agents_description = ""
        for agent in sub_agents:
            self.sub_agents_description += (
                f"- **{agent['name']}**: {agent['description']}\n"
            )

        # åŠ¨ä½œå¤„ç†å™¨æ˜ å°„è¡¨
        self.action_handlers = {
            CentralAgentAction.THINK: self._handle_think,
            CentralAgentAction.REFLECT: self._handle_reflect,
            CentralAgentAction.SUMMARIZE: self._handle_summarize,
            CentralAgentAction.DELEGATE: self._handle_delegate,
            CentralAgentAction.FINISH: self._handle_finish,
        }

        # åŠ¨ä½œç±»å‹å¯¹åº”çš„æŒ‡ä»¤æ¨¡æ¿
        self.action_instructions = {
            CentralAgentAction.THINK: "åˆ†æå½“å‰çŠ¶æ€å¹¶æ€è€ƒä¸‹ä¸€æ­¥è¡ŒåŠ¨",
            CentralAgentAction.REFLECT: "åæ€ä¹‹å‰çš„åŠ¨ä½œå’Œç»“æœ",
            CentralAgentAction.SUMMARIZE: "æ€»ç»“å½“å‰å·²è·å¾—çš„ä¿¡æ¯",
            CentralAgentAction.DELEGATE: "å†³å®šå§”æ´¾å“ªä¸ªå­Agentæ‰§è¡Œä»»åŠ¡",
            CentralAgentAction.FINISH: "åˆ¤æ–­æ˜¯å¦å¯ä»¥å®Œæˆä»»åŠ¡å¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
        }

    def __init__(
        self,
        initial_query: str,
        central_guidance: str | None = None,
        factstruct_outline: str | None = None,
        state: State | None = None,
    ):
        # --- Core task signal ---
        self.initial_query = initial_query
        # --- High-level planning signals ---
        self.central_guidance = central_guidance
        self.replan_result = replan_result

        # --- Current outline state ---
        self.factstruct_outline = state.get("factstruct_outline")
        self.factstruct_memory = state.get("factstruct_memory")
        self.feedback = state.get("feedback")
        self.total_word_limit = state.get("total_word_limit")




    def make_decision(
        self, state: State, config: RunnableConfig, retry_count: int = 0
    ) -> OutlineToolDecision:
        """
        ä¸­æ¢Agentå†³ç­–æ ¸å¿ƒé€»è¾‘ï¼Œåˆ†æå½“å‰çŠ¶æ€ç”Ÿæˆå†³ç­–ç»“æœ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            å†³ç­–ç»“æœå¯¹è±¡
        """
        max_retries = 3
        logger.info("Outline Agentè¿›è¡Œå†³ç­–...")
        start_time = datetime.now()

        # æ„å»ºå†³ç­–prompt
        messages = self._build_decision_prompt(state, config)
        logger.debug(f"outline å†³ç­–prompt: {messages}")


        try:
            llm = get_llm_by_type(
                AGENT_LLM_MAP.get("outline_agent", "default")
            ).with_structured_output(
                OutlineToolDecision_Base,   # âœ… ç»™ LLM ç”¨çš„ schema
                method="json_mode",
            )

            response: OutlineToolDecision_Base = llm.invoke(messages)

            logger.info(f"Outline å†³ç­–ç»“æœ(raw): {response}")

            # âœ… ä» LLM åè®®å¯¹è±¡ â†’ ç³»ç»Ÿå†…éƒ¨å¯¹è±¡
            decision = OutlineToolDecision(
                tool=response.tool,
                reasoning=response.reasoning,
                params=response.params,
            )

            end_time = datetime.now()
            global_statistics.add_time_entry(
                {
                    "step_name": "outline_decision",
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration": (end_time - start_time).total_seconds(),
                }
            )

            return decision

        except Exception as e:
            import traceback

            logger.error(
                f"Outline å†³ç­–è§£æå¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries}): {e}"
            )
            logger.error(traceback.format_exc())

            if retry_count < max_retries - 1:
                return self.make_decision(state, config, retry_count + 1)

            # ğŸš¨ å…œåº•ï¼šå¼ºåˆ¶ finishï¼Œé˜²æ­¢ç³»ç»Ÿå¡æ­»
            return OutlineToolDecision(
                tool="finish",
                reasoning="Outline decision parsing failed repeatedly, forcing termination.",
                params=None,
            )


    def _build_decision_prompt(
        self,
        state: State,
        config: RunnableConfig,
    ) -> List[Union[AIMessage, HumanMessage]]:
        """
        æ„å»º Outline Agent çš„å†³ç­– prompt
        """

        context = {
            # å¿…é¡»é¡¹
            "user_query": state.get("initial_query"),

            # å¯é€‰é¡¹ï¼ˆprompt é‡Œæœ‰ if åˆ¤æ–­ï¼‰
            "central_guidance": state.get("central_guidance"),
            "factstruct_outline": state.get("factstruct_outline"),
            "total_word_limit": state.get("total_word_limit"),
            "feedback": state.get("feedback"),
            "SOP": state.get("sop"),

            # è¯­è¨€
            "locale": state.get("locale", "zh-CN"),
        }

        # åˆå¹¶ configï¼ˆå¦‚éœ€è¦ï¼‰
        context = {**context, **config}

        return apply_prompt_template(
            "outline_decision",   # âœ… å¯¹åº” src/prompts/outline_decision.md
            state,
            extra_context=context,
        )


    def execute_action(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡Œå†³ç­–åŠ¨ä½œï¼Œè°ƒåº¦å¯¹åº”çš„åŠ¨ä½œå¤„ç†å™¨

        Args:
            decision: å†³ç­–ç»“æœ
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            åŠ¨ä½œæ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        handler = self.action_handlers.get(decision.action)
        if not handler:
            error_msg = f"æœªçŸ¥åŠ¨ä½œ: {decision.action}"
            logger.error(error_msg)
            return Command(
                update={
                    "messages": [
                        AIMessage(
                            content=f"é”™è¯¯ï¼šæœªçŸ¥åŠ¨ä½œ: {decision.action}",
                            name="central_error",
                        )
                    ],
                    "locale": state.get("locale"),
                    "current_node": "central_agent",
                    "memory_stack": self.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        return handler(decision, state, config)

    def _handle_think(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†æ€è€ƒåŠ¨ä½œï¼Œåˆ†æå½“å‰çŠ¶æ€ç”Ÿæˆä¸‹ä¸€æ­¥è®¡åˆ’"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨æ€è€ƒ...")
        start_time = datetime.now()
        context = {
            "current_action": "think",
            "current_progress": state.get("observations", []),
            "decision_reasoning": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # åº”ç”¨ç»Ÿä¸€çš„å†³ç­–æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # è®°å½•æ€è€ƒè¿‡ç¨‹åˆ°è®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="think",
            content=response.content,
        )
        self.memory_stack.push(memory_entry)

        logger.info(f"central_think: {response.content}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_think" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [AIMessage(content=response.content, name="central_think")],
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_reflect(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†åæ€åŠ¨ä½œï¼Œè¯„ä¼°ä¹‹å‰çš„æ­¥éª¤å¹¶æ¸…ç†è®°å¿†æ ˆ"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨åæ€...")
        start_time = datetime.now()

        # è·å–åæ€ç›®æ ‡å’Œä¸Šä¸‹æ–‡
        # recent_memory = self.memory_stack.get_recent(5)  # è·å–æœ€è¿‘5æ¡è®°å¿†

        context = {
            "current_action": "reflect",
            "decision_reasoning": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # åº”ç”¨åæ€æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # è§£æåæ€ç»“æœçš„JSON
        try:
            reflection_data = json.loads(repair_json_output(response.content))
            analysis = reflection_data.get("analysis", "åæ€åˆ†æ")
            pop_count = reflection_data.get("pop_count", 0)
            reasoning = reflection_data.get("reasoning", "åæ€å®Œæˆ")

            # éªŒè¯pop_countæ˜¯æœ‰æ•ˆæ•°å­—
            if not isinstance(pop_count, int) or pop_count < 0:
                logger.warning(f"æ— æ•ˆçš„pop_count: {pop_count}ï¼Œè®¾ç½®ä¸º0")
                pop_count = 0

        except Exception as e:
            logger.error(f"åæ€ç»“æœè§£æå¤±è´¥: {e}")
            analysis = response.content
            pop_count = 0
            reasoning = "JSONè§£æå¤±è´¥ï¼Œä¿æŒç°æœ‰è®°å¿†æ ˆ"

        logger.debug(f"reflectå†³å®šæ¸…ç†{pop_count}æ¡æ¶ˆæ¯")
        # æ‰§è¡Œè®°å¿†æ ˆæ¸…ç†
        removed_items = []
        if pop_count > 0:
            reflection_content = (
                f"åæ€åˆ†æ: {analysis}\n"
                f"åæ€åŸå› : {reasoning}\n"
                f"æ¸…ç†äº† {pop_count} æ¡è®°å¿†ã€‚"
            )

            memory_entry = MemoryStackEntry(
                timestamp=datetime.now().isoformat(),
                action="reflect",
                content=reflection_content,
            )

            self.memory_stack.push_with_pop(memory_entry, pop_count)

            removed_items = self.memory_stack.pop(pop_count)

            logger.info(f"æˆåŠŸä»è®°å¿†æ ˆä¸­ç§»é™¤äº† {pop_count} é¡¹è®°å¿†")
            # logger.info(
            #     f"ä»è®°å¿†æ ˆä¸­ç§»é™¤äº† {len(removed_items)} é¡¹: {[item.action for item in removed_items]}"
            # )
        else:
            logger.info("ä¸ç§»é™¤ä»»ä½•è®°å¿†æ ˆé¡¹ç›®")

        logger.info(f"central_reflect: {analysis}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_reflect" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [AIMessage(content=analysis, name="central_reflect")],
                "reflection": {
                    "analysis": analysis,
                    "pop_count": len(removed_items),
                    "reasoning": reasoning,
                    "removed_items": removed_items,
                },
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_summarize(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†æ€»ç»“åŠ¨ä½œï¼Œå½’çº³å½“å‰å·²è·å¾—çš„ä¿¡æ¯"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨æ€»ç»“...")
        start_time = datetime.now()

        context = {
            "current_action": "summarize",
            "summarization_focus": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # æ‰“å°ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
        logger.debug(
            f"Summarize context: {json.dumps(context, ensure_ascii=False, indent=2)}"
        )

        # åº”ç”¨ç»Ÿä¸€çš„æ€»ç»“æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # æ›´æ–°è®°å¿†æ ˆï¼Œæ›¿æ¢æœ€æ–°çš„æ€»ç»“ç»“æœ
        new_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="summarize",
            content=context.get("summarization_focus", ""),
            result={"summary_result": response.content},
        )

        # logger.info("NEW_ENTRY", new_entry)
        # logger.info("*"*100)

        self.memory_stack.push_with_pop(new_entry)

        # logger.info(f"central_summarize: {response.content}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_summarize" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [
                    AIMessage(content=response.content, name="central_summarize")
                ],
                "summary": response.content,
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_delegate(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†å§”æ´¾åŠ¨ä½œï¼Œè°ƒåº¦å­Agentæ‰§è¡Œä¸“é¡¹ä»»åŠ¡"""
        agent_type = decision.params.agent_type
        task_description = decision.params.task_description
        # agent_type = decision.agent_type
        # task_description = decision.task_description or "æœªæŒ‡å®šä»»åŠ¡"

        # éªŒè¯å­Agentç±»å‹æœ‰æ•ˆæ€§
        if not agent_type or agent_type not in self.available_sub_agents:
            error_msg = (
                f"æ— æ•ˆçš„å­Agentç±»å‹: {agent_type}ï¼Œå¯ç”¨ç±»å‹: "
                f"{self.available_sub_agents}"
            )
            logger.error(f"central_error: {error_msg}")
            return Command(
                update={
                    "messages": [AIMessage(content=error_msg, name="central_error")],
                    "current_node": "central_agent",
                },
                goto="central_agent",
            )

        logger.info(f"ä¸­æ¢Agentå§”æ´¾ {agent_type} æ‰§è¡Œä»»åŠ¡: {task_description}")

        # è®°å½•å§”æ´¾åŠ¨ä½œåˆ°è®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type=agent_type,
            content=f"å§”æ´¾ä»»åŠ¡: {task_description}",
        )
        self.memory_stack.push(memory_entry)

        # æ„å»ºå­Agentæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†æ ˆæ‘˜è¦ï¼‰
        delegation_context = {
            "task_description": task_description,
            "agent_type": agent_type,
            "memory_context": self.memory_stack.get_summary(include_full_history=True),
            "original_query": state.get("user_query", ""),
        }

        logger.info(f"central_delegate: å§”æ´¾{agent_type}æ‰§è¡Œ: {task_description}")
        return Command(
            update={
                "messages": [
                    AIMessage(
                        content=f"å§”æ´¾{agent_type}æ‰§è¡Œ: {task_description}",
                        name="central_delegate",
                    )
                ],
                "delegation_context": delegation_context,
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto=agent_type,
        )

    def _handle_finish(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†å®ŒæˆåŠ¨ä½œï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¹¶ç»“æŸä»»åŠ¡"""
        logger.info("ä¸­æ¢Agentå®Œæˆä»»åŠ¡...")

        final_report = state.get("final_report", None)
        if not final_report:
            logger.info("æœªæ‰¾åˆ°æœ€ç»ˆæŠ¥å‘Šï¼Œå§”æ´¾Reporter Agentç”ŸæˆæŠ¥å‘Š...")

            # è®°å½•å§”æ´¾åŠ¨ä½œåˆ°è®°å¿†æ ˆ
            memory_entry = MemoryStackEntry(
                timestamp=datetime.now().isoformat(),
                action="delegate",
                agent_type="reporter",
                content="æœªç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            )
            self.memory_stack.push(memory_entry)

            # æ„å»ºReporteræ‰§è¡Œä¸Šä¸‹æ–‡
            delegation_context = {
                "task_description": "æ ¹æ®æ‰€æœ‰æ”¶é›†åˆ°çš„ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š",
                "agent_type": "reporter",
                "memory_context": self.memory_stack.get_summary(
                    include_full_history=True
                ),
                "original_query": state.get("user_query", ""),
                "report_type": "final_report",
                "execution_history": [
                    entry.to_dict() for entry in self.memory_stack.get_all()
                ],
            }

            logger.info("central_delegate_reporter: å§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
            return Command(
                update={
                    "messages": [
                        AIMessage(
                            content="å§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
                            name="central_delegate_reporter",
                        )
                    ],
                    "delegation_context": delegation_context,
                    "current_node": "central_agent",
                    "memory_stack": json.dumps(
                        [entry.to_dict() for entry in self.memory_stack.get_all()]
                    ),
                    "pending_finish": True,  # æ ‡è®°ç­‰å¾…æŠ¥å‘Šå®Œæˆåå†finish
                },
                goto="reporter",
            )
        logger.info(f"final_report: {final_report}")
        
        session_id = config["configurable"]["thread_id"]
        # global_reference_map.save_session(session_id)
        # æ„å»ºæ‰§è¡Œæ‘˜è¦ï¼ˆåŒ…å«å®Œæ•´è®°å¿†æ ˆå†å²ï¼‰
        execution_summary = {
            "user_query": state.get("user_query", "æœªçŸ¥æŸ¥è¯¢"),
            "execution_history": [
                entry.to_dict() for entry in self.memory_stack.get_all()
            ],
            "final_report": final_report,
            "research": global_reference_map.get_session_ref_map(session_id),#state.get("data_collections", []),
            "completion_time": datetime.now().isoformat(),
            "statistics": global_statistics.get_statistics(),
        }

        # ä¿å­˜æ‰§è¡Œæ‘˜è¦åˆ°æ–‡ä»¶
        os.makedirs("./reports", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"./reports/execution_report_{timestamp}.json"

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(execution_summary, f, ensure_ascii=False, indent=4)
            report_msg = f"ä»»åŠ¡å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {filename}"
        except Exception as e:
            logger.error(f"æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}")
            report_msg = f"ä»»åŠ¡å®Œæˆï¼Œä½†æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}"
            execution_summary["error"] = str(e)

        logger.info(report_msg)
        logger.info(global_statistics.get_statistics())




from src.llms.llm import get_llm_by_type
from ..graph.types import State
from langchain_core.runnables import RunnableConfig
from datetime import datetime

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command, interrupt
from sentence_transformers import CrossEncoder

from src.agents.CoderAgent import CoderAgent
from src.agents.ResearcherAgent_SP import ResearcherAgentSP
from src.tools import (
    crawl_tool,
    get_web_search_tool,
    get_retriever_tool,
    python_repl_tool,
    search_docs_tool,
)
from src.utils.json_utils import repair_json_output
from src.utils.logger import logger
from src.config.agents import AGENT_LLM_MAP
from src.llms.llm import get_llm_by_type
from src.prompts.template import apply_prompt_template
from src.memory import MemoryStack, MemoryStackEntry
from src.agents.CentralAgent import CentralAgent
from src.tools.get_docs_info import search_docs
from src.tools.bocha_search.web_search_en import web_search
from src.factstruct import (
    run_factstruct_stage1,
    outline_node_to_markdown,
    outline_node_to_dict,
    memory_to_dict,
    filter_content_by_relevant_docs,
    mark_content_with_support,
    repair_unknown_citations
)

from src.factstruct import outline_node_to_dict, memory_to_dict
from src.factstruct.outline_node import OutlineNode

from ..graph.types import State
from ..config import SELECTED_SEARCH_ENGINE, SearchEngine
from src.utils.statistics import global_statistics, timed_step
import re
from typing import Dict, Any
import json
from src.utils.reference_utils import global_reference_map, process_final_report
# -------------------------
# å­Agentç®¡ç†æ¨¡å—
# TODO: check sub-agent bugs
# TODO: æœç´¢å¤ªå¤šæ—¶ä¼šè¶…è¿‡è¾“å…¥é™åˆ¶æˆ–è€…ç¼“å†²åŒºæº¢å‡ºï¼Œéœ€è¦é™åˆ¶æœç´¢åˆ°çš„å†…å®¹é•¿åº¦æˆ–è€…åšä¸€ä¸ªç®€å•çš„æ‘˜è¦
# TODO: éœ€è¦å¤„ç†æœç´¢æ•æ„Ÿè¯ï¼ˆä»¥â€œ985å¤§å­¦æœ€å¤šçš„äº”ä¸ªåŸå¸‚â€ä¸ºä¾‹ï¼ŒAIå°±æ— æ³•å¤„ç†ä¿¡æ¯ï¼Œè¿”å›Errorï¼‰
# -------------------------
class SubAgentManager:
    """å­Agentç®¡ç†å™¨ï¼Œè´Ÿè´£åˆ›å»ºå’Œæ‰§è¡Œå„ç±»ä¸“é¡¹å­Agent"""

    def __init__(self, central_agent: "CentralAgent"):
        self.central_agent = central_agent

    @timed_step("execute_researcher")
    async def execute_researcher(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œç ”ç©¶Agentï¼Œè´Ÿè´£ä¿¡æ¯æ£€ç´¢ä¸åˆ†æ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç ”ç©¶Agentå¼€å§‹æ‰§è¡Œ...")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")

        # é…ç½®ç ”ç©¶å·¥å…·é“¾
        tools = [get_web_search_tool(10), crawl_tool, search_docs_tool]
        retriever_tool = get_retriever_tool(state.get("resources", []))
        if retriever_tool:
            tools.insert(0, retriever_tool)

        # å®ä¾‹åŒ–ç ”ç©¶Agent
        research_agent = ResearcherAgentSP(
            config=config, agent_type="researcher", default_tools=tools
        )

        # æ‰§è¡Œç ”ç©¶ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await research_agent.execute_agent_step(state)

            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            result_data_collections = []

            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
                result_data_collections = result_command.update.get(
                    "data_collections", []
                )

            logger.info(f"data_collections_in subagent:{result_data_collections}")

        except Exception as e:
            logger.error(f"Researcher Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content=f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}", name="researcher"
                        )
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="researcher",
            content=f"ç ”ç©¶ä»»åŠ¡: {task_description}",
            result={
                "observations": result_observations,
                # "data_collections": result_data_collections,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="researcher"
                    )
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
                "data_collections": result_data_collections,
                "observations": result_observations,
            },
            goto="central_agent",
        )

    @timed_step("execute_xxqg_researcher")
    async def execute_xxqg_researcher(
        self, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡Œç ”ç©¶Agentï¼Œè´Ÿè´£ä¿¡æ¯æ£€ç´¢ä¸åˆ†æ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç ”ç©¶Agentå¼€å§‹æ‰§è¡Œ...")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")

        # é…ç½®ç ”ç©¶å·¥å…·é“¾
        tools = [search_docs_tool]

        # å®ä¾‹åŒ–ç ”ç©¶Agent
        research_agent = ResearcherAgentSP(
            config=config, agent_type="researcher_xxqg", default_tools=tools
        )

        # æ‰§è¡Œç ”ç©¶ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await research_agent.execute_agent_step(state)

            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            result_data_collections = []

            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
                result_data_collections = result_command.update.get(
                    "data_collections", []
                )

        except Exception as e:
            logger.error(f"ç ”ç©¶Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content=f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}", name="researcher"
                        )
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="researcher",
            content=f"ç ”ç©¶ä»»åŠ¡: {task_description}",
            result={
                "observations": result_observations,
                # "data_collections": result_data_collections,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="researcher"
                    )
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
                "data_collections": result_data_collections,
                "observations": result_observations,
            },
            goto="central_agent",
        )

    @timed_step("execute_web_researcher")
    async def execute_web_researcher(
        self, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡Œç ”ç©¶Agentï¼Œè´Ÿè´£ä¿¡æ¯æ£€ç´¢ä¸åˆ†æ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("Web Agentå¼€å§‹æ‰§è¡Œ...")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")

        # é…ç½®ç ”ç©¶å·¥å…·é“¾
        # tools = [search_docs_tool]
        tools = [get_web_search_tool(10)]
        
        # å®ä¾‹åŒ–ç ”ç©¶Agent
        research_agent = ResearcherAgentSP(
            config=config, agent_type="researcher_web", default_tools=tools
        )

        # æ‰§è¡Œç ”ç©¶ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await research_agent.execute_agent_step(state)

            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            result_data_collections = []

            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
                result_data_collections = result_command.update.get(
                    "data_collections", []
                )

        except Exception as e:
            logger.error(f"ç ”ç©¶Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content=f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}", name="researcher"
                        )
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="researcher",
            content=f"ç ”ç©¶ä»»åŠ¡: {task_description}",
            result={
                "observations": result_observations,
                # "data_collections": result_data_collections,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        logger.info("Webç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        logger.info(f"state:{state}")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="researcher"
                    )
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
                "data_collections": result_data_collections,
                "observations": result_observations,
            },
            goto="central_agent",
        )


    @timed_step("execute_coder")
    async def execute_coder(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œç¼–ç Agentï¼Œè´Ÿè´£ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç¼–ç Agentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç¼–ç ä»»åŠ¡")

        # å®ä¾‹åŒ–ç¼–ç Agent
        code_agent = CoderAgent(
            config=config, agent_type="coder", default_tools=[python_repl_tool]
        )

        # æ‰§è¡Œç¼–ç ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await code_agent.execute_agent_step(state)
            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
        except Exception as e:
            logger.error(f"ç¼–ç Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(content=f"ç¼–ç ä»»åŠ¡å¤±è´¥: {str(e)}", name="coder")
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="coder",
            content=f"ç¼–ç ä»»åŠ¡: {task_description}",
            result={"observations": result_observations},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç¼–ç ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="ç¼–ç ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="coder")
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_reporter")
    def execute_reporter(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡ŒæŠ¥å‘ŠAgentï¼Œè´Ÿè´£ç»“æœæ•´ç†ä¸æŠ¥å‘Šç”Ÿæˆ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("æŠ¥å‘ŠAgentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")

        # æ”¶é›†æŠ¥å‘Šç”Ÿæˆæ‰€éœ€ä¸Šä¸‹æ–‡
        context = {
            "user_query": state.get("user_query", ""),
            "memory_history": self.central_agent.memory_stack.get_all(),
            "task_description": task_description,
        }

        # ç”ŸæˆæŠ¥å‘Šå¹¶å¤„ç†å¼‚å¸¸
        final_report = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "reporter", state, extra_context=context
            )  # ä¿®å¤ï¼šå‚æ•°é¡ºåº
            llm = get_llm_by_type(AGENT_LLM_MAP.get("reporter", "default"))
            response = llm.invoke(messages)
            final_report = response.content
        except Exception as e:
            logger.error(f"æŠ¥å‘ŠAgentæ‰§è¡Œå¤±è´¥: {str(e)}")
            final_report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="reporter",
            content=f"æŠ¥å‘Šä»»åŠ¡: {task_description}",
            result={"final_report": final_report},
        )
        self.central_agent.memory_stack.push(memory_entry)

        data_collections = state.get("data_collections", [])
        logger.info(
            f"report agent: data_collections:{data_collections}"
        )  # NOTE: data_collectionså¯ä»¥åœ¨è¿™é‡Œå–

        logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="reporter")
                ],
                "final_report": final_report,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_xxqg_reporter")
    def execute_xxqg_reporter(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡ŒæŠ¥å‘ŠAgentï¼Œè´Ÿè´£ç»“æœæ•´ç†ä¸æŠ¥å‘Šç”Ÿæˆ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("æŠ¥å‘ŠAgentå¼€å§‹æ‰§è¡Œ...")
        logger.info(f"state:{state}")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")

        # æ”¶é›†æŠ¥å‘Šç”Ÿæˆæ‰€éœ€ä¸Šä¸‹æ–‡
        context = {
            "user_query": state.get("user_query", ""),
            "memory_history": self.central_agent.memory_stack.get_all(),
            "task_description": task_description,
        }

        # ç”ŸæˆæŠ¥å‘Šå¹¶å¤„ç†å¼‚å¸¸
        final_report = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "reporter_xxqg", state, extra_context=context
            )  # ä¿®å¤ï¼šå‚æ•°é¡ºåº
            data_collections = state.get("data_collections", [])
            observations = state.get("observations", [])

            messages.append(
                HumanMessage(
                    f"##User Query\n\n{state.get('user_query', '')}\n\n##ç”¨æˆ·çº¦æŸ\n\n{state.get("user_dst","")}\n\n##æŠ¥å‘Šå¤§çº²{state.get('report_outline','ç”¨æˆ·æœªæä¾›å¤§çº²')}\n\nBelow are information collected in previous tasks:\n\n{"\n\n".join(observations)}"
                )
            )        
            # messages.append(
            #     HumanMessage(
            #         f"##User Query\n\n{state.get('user_query', '')}\n\n##ç”¨æˆ·çº¦æŸ\n\n{state.get("user_dst","")}\n\n##æŠ¥å‘Šå¤§çº²{state.get('report_outline','ç”¨æˆ·æœªæä¾›å¤§çº²')}\n\nBelow are information collected in previous tasks:\n\n{"\n\n".join(data_collections)}"
            #     )
            # )        
            logger.debug(f"Reporter messages: {messages}")
            llm = get_llm_by_type(AGENT_LLM_MAP.get("reporter", "default"))
            response = llm.invoke(messages)
            final_report = response.content
            #å¯ä»¥åœ¨è¿™ä¸ªåœ°æ–¹åŠ ä¸€ä¸ªå¯¹final_reportçš„å¤„ç†
            

            
            
            session_id = config["configurable"]["thread_id"]
            reference_map=global_reference_map.get_session_ref_map(session_id)
            # logger.info(f"before reference_map:{reference_map}")
            # logger.info(f"before final_report :{final_report}")
            final_report = process_final_report(final_report, reference_map)
            # logger.info(f"after final_report :{final_report}")


            #å¢åŠ å¼•ç”¨æ£€æŸ¥éƒ¨åˆ†
            logger.info(f"å¼•ç”¨æ£€æŸ¥")
            # logger.info(f"state:{state}")
            logger.info(f"observations:{observations}")
            # logger.info(f"data_collections:{data_collections}")
            logger.info(f"final_report:{final_report}")
            semantic_cls = CrossEncoder("/data1/Yangzb/Model/StructBert/cross-encoder/nli-deberta-v3-small")
            #è¿™ä¸ªæ˜¯åˆ¤æ–­å¼•ç”¨å’Œå¥å­çš„å…³ç³»
            supported = filter_content_by_relevant_docs(
                content=final_report,
                relevant_docs=reference_map,
                semantic_cls=semantic_cls
            )
            logger.info(f"supported :{supported}")
            
            #è¿™ä¸ªæ˜¯æŠŠå…³ç³»åº”ç”¨åˆ°ç”Ÿæˆæ–‡ç« ä¸Š
            new_content = mark_content_with_support(
                content=final_report,
                nli_results=supported
            )
            logger.info(f"new_content :{new_content}")
            
            #è¿™ä¸ªæ˜¯æŠŠé”™è¯¯å¼•ç”¨è¿›è¡Œå¤„ç†çš„
            final_report=repair_unknown_citations(
                content=new_content,
                relevant_docs=reference_map,
                semantic_cls=semantic_cls
            )
            logger.info(f"final_report :{final_report}")
            
        except Exception as e:
            import traceback

            logger.error(traceback.format_exc())
            logger.error(f"æŠ¥å‘ŠAgentæ‰§è¡Œå¤±è´¥: {str(e)}")
            final_report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="reporter",
            content=f"æŠ¥å‘Šä»»åŠ¡: {task_description}",
            result={"final_report": final_report},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="reporter")
                ],
                "final_report": final_report,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_xxqg_reporter_factstruct")
    def execute_xxqg_reporter_factstruct(
        self, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡ŒæŠ¥å‘ŠAgentï¼ˆä½¿ç”¨ FactStruct Stage 2ï¼‰

        åŸºäº FactStruct Stage 1 ç”Ÿæˆçš„å¤§çº²å’Œ Memoryï¼Œä¸ºæ¯ä¸ªå¶å­èŠ‚ç‚¹
        åˆ†åˆ«ç”Ÿæˆå†…å®¹ï¼Œæœ€ç»ˆåˆå¹¶ä¸ºå®Œæ•´æŠ¥å‘Šã€‚

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("æŠ¥å‘ŠAgentå¼€å§‹æ‰§è¡Œï¼ˆFactStruct Stage 2ï¼‰...")

        factstruct_outline = state.get("factstruct_outline")
        factstruct_memory = state.get("factstruct_memory")

        if not factstruct_outline or not factstruct_memory:
            logger.warning(
                "FactStruct æ•°æ®ç¼ºå¤±ï¼Œå›é€€åˆ°ä¼ ç»Ÿ Reporter æ–¹æ³•"
            )
            return self.execute_xxqg_reporter(state, config)

        user_query = state.get("user_query", "")

        final_report = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            from src.factstruct import run_factstruct_stage2
            from src.config.agents import AGENT_LLM_MAP

            final_report = run_factstruct_stage2(
                outline_dict=factstruct_outline,
                memory_dict=factstruct_memory,
                user_query=user_query,
                llm_type=AGENT_LLM_MAP.get("reporter_factstruct", "basic"),
                locale=state.get("locale", "zh-CN"),
            )
            
            #å¯ä»¥åœ¨è¿™ä¸ªåœ°æ–¹åŠ ä¸€ä¸ªå¯¹final_reportçš„å¤„ç†
            session_id = config["configurable"]["thread_id"]
            reference_map=global_reference_map.get_session_ref_map(session_id)
            logger.info(f"before reference_map:{reference_map}")
            logger.info(f"before final_report :{final_report}")
            final_report = process_final_report(final_report, reference_map)
            logger.info(f"after final_report :{final_report}")
            
            logger.info(
                f"FactStruct Stage 2 æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {len(final_report)} ä¸ªå­—ç¬¦"
            )

        except Exception as e:
            import traceback

            logger.error(traceback.format_exc())
            logger.error(f"FactStruct Stage 2 æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            final_report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="reporter",
            content="æŠ¥å‘Šä»»åŠ¡: ä½¿ç”¨ FactStruct Stage 2 ç”ŸæˆæŠ¥å‘Š",
            result={"final_report": final_report},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ˆFactStruct Stage 2ï¼‰ï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ˆFactStruct Stage 2ï¼‰ï¼Œè¿”å›ä¸­æ¢Agent",
                        name="reporter",
                    )
                ],
                "final_report": final_report,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_sp_planner")
    def execute_sp_planner(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œä»»åŠ¡æ‹†è§£Agentï¼Œè´Ÿè´£å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ä»»åŠ¡æ‹†è§£Agentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get(
            "task_description",
            state.get("user_query", "") + "\nå°†ç”¨æˆ·çš„ä»»åŠ¡æ‹†è§£æˆ2-5ä¸ªå­ä»»åŠ¡",
        )

        # æ”¶é›†ä»»åŠ¡æ‹†è§£æ‰€éœ€ä¸Šä¸‹æ–‡
        context = {
            "user_query": state.get("user_query", ""),
            "memory_history": [],  # self.central_agent.memory_stack.get_all(),
            "task_description": task_description,
        }

        # ç”Ÿæˆä»»åŠ¡æ‹†è§£å¹¶å¤„ç†å¼‚å¸¸
        replan_result = "ä»»åŠ¡æ‹†è§£å¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "replanner", state, extra_context=context
            )  # ä¿®å¤ï¼šå‚æ•°é¡ºåº
            llm = get_llm_by_type(AGENT_LLM_MAP.get("replanner", "default"))
            response = llm.invoke(messages)
            replan_result = response.content
            replan_result = (
                replan_result.replace("```json", "").replace("```", "").strip()
            )

            logger.debug(f"ä»»åŠ¡æ‹†è§£ç»“æœ: {replan_result}")

            # è§£æLLMè¿”å›çš„ä»»åŠ¡æ‹†è§£ç»“æœ
            import json

            try:
                response_json = json.loads(replan_result)
                if isinstance(response_json, list):
                    response_json = {"DAG": response_json}
            except json.JSONDecodeError as e:
                logger.error(f"JSON decode error: {e}")
                response_json = {"DAG": [(input, input)]}
            if isinstance(response_json["DAG"], list):
                new_dag = []
                for item in response_json["DAG"]:
                    if isinstance(item, dict):
                        pairs = list(item.items())
                        new_dag.append(
                            (pairs[0][1], pairs[1][1])
                            if len(pairs) > 1
                            else (pairs[0][1], pairs[0][1])
                        )
                    elif isinstance(item, list) and len(item) > 1:
                        new_dag.append((item[0], item[1]))
                    else:
                        new_dag.append((item, item))
                response_json["DAG"] = new_dag

            from src.utils.graph_utils import Graph

            graph = Graph()
            graph.load_dag_from_json(response_json)
            sorted_nodes = graph.topological_sort()
            # Generate a unique ID for each input using a hash
            input_id = hash(input)
            # replan_result = {"id":input_id,"plans":[{node_id: graph.nodes[node_id].question} for node_id in sorted_nodes],"status":["uncomplete" for node_id in sorted_nodes]}
            replan_result = {
                "id": input_id,
                "plans": [
                    {node_id: graph.nodes[node_id].question} for node_id in sorted_nodes
                ],
            }
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‹†è§£Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            replan_result = f"ä»»åŠ¡æ‹†è§£å¤±è´¥: {str(e)}"

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="replanner",
            content=f"ä»»åŠ¡æ‹†è§£: {task_description}",
            result={"replan_result": replan_result},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ä»»åŠ¡æ‹†è§£å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="ä»»åŠ¡æ‹†è§£å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="planner")
                ],
                "replan_result": replan_result,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )



    @timed_step("execute_human_feedback")
    async def execute_human_feedback(self, state: State, config: RunnableConfig) -> Command:
        stage = state.get("wait_stage", "perception")
        if stage == "perception":
            dst_question = state.get("dst_question", "")
            feedback = interrupt(
                    "Please Fill the Question.[DST]" + dst_question + "[/DST]"
                )
            logger.info(f"ç”¨æˆ·åé¦ˆçš„DSTé—®é¢˜: {feedback}. goto perception node again.")
            return Command(
                update={
                    "hitl_feedback": feedback,
                    "current_node": "human_feedback",
                },
                goto="perception",
            )
        elif stage == "outline":
            outline = state.get("report_outline", "")
            feedback = interrupt(
                    "Please Confirm or Edit the Outline.[OUTLINE]"
                    + outline
                    + "[/OUTLINE]"
                )
            logger.info(f"ç”¨æˆ·åé¦ˆçš„å¤§çº²: {feedback}. goto outline node again.")
            return Command(
                update={
                    "hitl_feedback": feedback,
                    "current_node": "human_feedback",
                },
                goto="outline",
            )

    @timed_step("execute_perception")
    async def execute_perception(self, state: State, config: RunnableConfig) -> Command:
        user_query = state.get("user_query", "")
        # check if the plan is auto accepted
        perception_llm = get_llm_by_type(AGENT_LLM_MAP.get("perception", "default"))
        auto_accepted_plan = state.get("auto_accepted_plan", False)
        skip_perception = state.get("skip_perception", False)
        
        if skip_perception:
            logger.info("è·³è¿‡æ„ŸçŸ¥å±‚ï¼Œç›´æ¥è¿›å…¥å¤§çº²ç”Ÿæˆ")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content="æ„ŸçŸ¥å±‚å·²è·³è¿‡",
                            name="perception",
                        )
                    ],
                    "user_dst": "",
                    "current_node": "perception",
                    "wait_for_user": False,
                },
                goto="outline",
            )

        if auto_accepted_plan:
            try:
                # messages = apply_prompt_template("perception", state) + [
                #     HumanMessage(f"##User Query\n\n{user_query}\n\n")
                # ]
                messages = apply_prompt_template("perception", state)

                # logger.debug("messages"+str(messages))
                response = perception_llm.invoke(messages)
                dst_question = response.content
                # logger.debug("dst_question"+str(dst_question))
                dst_question = repair_json_output(dst_question)
                logger.info(f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œç”ŸæˆDSTé—®é¢˜: {dst_question}")
                return Command(
                    update={
                        "dst_question": dst_question,
                        "wait_stage": "perception",
                        "current_node": "perception",
                    },
                    goto="human_feedback",
                )
            except Exception as e:
                logger.error(f"æ„ŸçŸ¥å±‚æ‰§è¡Œå¤±è´¥: {str(e)}")

        if wait_stage == "perception":
            feedback = state.get("hitl_feedback", "")
            dst_question = state.get("dst_question", "")
            # if the feedback is not accepted, return the planner node
            if feedback and str(feedback).upper().startswith("[FILLED_QUESTION]"):
                messages = apply_prompt_template("perception", state) + [
                    HumanMessage(
                        f"##User Query\n\n{user_query}\n\n##å¸Œæœ›ç”¨æˆ·å›ç­”çš„é—®é¢˜\n\n{dst_question}\n\n##ç”¨æˆ·å›ç­”çš„ç»“æœ\n\n{feedback}\n\n"
                    )
                ]
                # logger.debug("messages"+str(messages))
                # exit()
                response = perception_llm.invoke(messages)
                summary = response.content
                logger.info(f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œæ”¶é›†ç”¨æˆ·åé¦ˆ: {summary}")

                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content=f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œæ”¶é›†ç”¨æˆ·åé¦ˆ: {summary}",
                                name="perception",
                            )
                        ],
                        "user_dst": summary,
                        "current_node": "perception",
                        "wait_stage": "",
                    },
                    goto="outline",
                )
            elif feedback and str(feedback).upper().startswith("[SKIP]"):
                logger.info("DST question is skipped by user.")
                messages.append(
                    AIMessage(content=f"##LLM DST Question\n\n{dst_question}\n\n")
                )
                messages.append(
                    HumanMessage(
                        content=f"ç”¨æˆ·è·³è¿‡äº†å›ç­”ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„ç†è§£è¿›è¡Œæ€»ç»“\n\n"
                    )
                )
                response = perception_llm.invoke(messages)
                summary = response.content
                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content="DST question is skipped by user.",
                                name="perception",
                            )
                        ],
                        "user_dst": summary,
                        "current_node": "perception",
                        "wait_stage": "",
                    },
                    goto="outline",
                )
            else:
                raise TypeError(f"Interrupt value of {feedback} is not supported.")

    @timed_step("execute_outline")
    async def execute_outline(self, state: State, config: RunnableConfig) -> Command:
        user_query = state.get("user_query", "")
        # check if the plan is auto accepted
        outline_llm = get_llm_by_type(AGENT_LLM_MAP.get("outline", "default"))
        wait_stage = state.get("wait_stage", "")
        if wait_stage != "outline":
            #bg_investigation = search_docs(user_query, top_k=5)
            bg_investigation = web_search(user_query, top_k=5)
            user_dst = state.get("user_dst", "")
            try:
                messages = [
                    HumanMessage(
                        f"##ç”¨æˆ·åŸå§‹é—®é¢˜\n\n{user_query}\n\n##ç”¨æˆ·è¡¥å……éœ€æ±‚\n\n{user_dst}\n\n##å¯èƒ½ç”¨åˆ°çš„ç›¸å…³æ•°æ®\n\n{bg_investigation}\n\n"
                    )
                ] + apply_prompt_template("outline", state)
                response = outline_llm.invoke(messages)
                outline_response = response.content
                outline_response = repair_json_output(outline_response)
                logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆ: {outline_response}")

            except Exception as e:
                logger.error(f"å¤§çº²ç”Ÿæˆæ‰§è¡Œå¤±è´¥: {str(e)}")
                # è¿”å›æœ€ç®€å•çš„é»˜è®¤å¤§çº²
                import json

                outline_response = json.dumps(
                    {"title": user_query, "children": []}, ensure_ascii=False
                )


            outline_confirmed = outline_response.strip()
            logger.info(f"å¤§çº²è‡ªåŠ¨ç¡®è®¤: {outline_confirmed}")

            return Command(
                update={
                    "messages": [
                        HumanMessage(content=f"å¤§çº²ç¡®è®¤: {outline_confirmed}", name="outline")
                    ],
                    "report_outline": outline_confirmed,
                    "current_node": "outline",
                },
                goto="central_agent",
            )


    @timed_step("execute_outline_factstruct")
    async def execute_outline_factstruct(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œå¤§çº²å­Agentï¼ˆFactStruct Stage 1ï¼‰

        åŸºäºç”¨æˆ·é—®é¢˜å’Œå·²ç¡®è®¤çš„ä»»åŠ¡è§„åˆ’ï¼Œç”Ÿæˆæˆ–è°ƒæ•´æŠ¥å‘Šçš„å¤§çº²ç»“æ„ï¼Œ
        å¹¶ä¸ºåç»­ FactStruct Stage 2 æä¾›ç»“æ„åŒ– Outline ä¸ Memoryã€‚
        """
        logger.info("å¤§çº²Agentå¼€å§‹æ‰§è¡Œï¼ˆFactStruct Stage 1ï¼‰...")

        user_query = state.get("user_query", "")
        user_dst = state.get("user_dst", "")
        factstruct_outline_dict = state.get("factstruct_outline", None)#å¦‚æœæœ‰çš„è¯ï¼Œåç»­æ›´æ”¹åˆ°æ—¶å€™å†ä¿®
        factstruct_memory_dict = state.get("factstruct_memory",None)
        #æå–çš„æ˜¯ guideline
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")
        outline_response = "å¤§çº²ç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        
        #è¿™ç©æ„æ˜¯äººå·¥ç¡®è®¤ human nodeçš„ï¼Œæ„Ÿè§‰æ²¡å•¥ç”¨ï¼ŒFactStruct å¦‚æœé…ä¸Š Human feedback æ‰éœ€è¦è¿™ä¸ª
        # auto_accepted_plan = state.get("auto_accepted_plan", False)
        # if not auto_accepted_plan:
        #     logger.warning("ä»»åŠ¡è§„åˆ’æœªç¡®è®¤ï¼ŒOutline Agent ä¸æ‰§è¡Œ")
        #     return Command(
        #         update={
        #             "messages": [
        #                 HumanMessage(
        #                     content="ä»»åŠ¡è§„åˆ’å°šæœªç¡®è®¤ï¼Œè·³è¿‡å¤§çº²ç”Ÿæˆ",
        #                     name="outline",
        #                 )
        #             ],
        #             "current_node": "central_agent",
        #         },
        #         goto="central_agent",
        #     )

        try:
            replan_result= state.get("replan_result", None)
            full_query = user_query
            if user_dst:
                full_query = f"{user_query}\n\nç”¨æˆ·è¡¥å……éœ€æ±‚ï¼š{user_dst}"

            # åˆ›å»ºå¤§çº²
            # æ‰©å±•å¤§çº²
            # åˆ å‡å¤§çº²
            # å­—æ•°æ§åˆ¶åé¦ˆ
            # ä½¿ç”¨FactStructè‡ªå·±çš„ LLM æ¥åšè¿™ä¸ªäº‹æƒ…ã€‚
            
            outline_root, memory = run_factstruct_stage1(
                query=full_query,
                max_iterations=state.get("factstruct_max_iterations", 4),
                batch_size=state.get("factstruct_batch_size", 2),
                task_description=task_description,
                replan_result=replan_result,
                config=config,
            )

            # å¤§çº²çš„å­—æ•°åŒ¹é…
            total_word_limit = state.get("total_word_limit", 5000)
            if total_word_limit > 0:
                logger.info(f"æ£€æµ‹åˆ°å­—æ•°é™åˆ¶ {total_word_limit}ï¼Œæ‰§è¡Œå­—æ•°è§„åˆ’...")
                outline_root = self.execute_word_planning(
                    outline_root, total_word_limit
                )
                outline_response = outline_root.to_text_tree(
                    include_word_limit=True
                )
            else:
                outline_response = outline_node_to_markdown(
                    outline_root, max_depth=None, include_root=True
                )

            factstruct_outline_dict = outline_node_to_dict(outline_root)
            factstruct_memory_dict = memory_to_dict(memory)

            logger.info(
                f"FactStruct Stage 1 å®Œæˆ: "
                f"{len(outline_root.get_all_nodes())} ä¸ªèŠ‚ç‚¹"
            )

        except Exception as e:
            import traceback

            logger.error(traceback.format_exc())
            logger.error(f"FactStruct Stage 1 æ‰§è¡Œå¤±è´¥: {str(e)}")

            outline_response = f"å¤§çº²ç”Ÿæˆå¤±è´¥ï¼ˆFactStruct Stage 1ï¼‰: {str(e)}"

        # === å†™å…¥ central agent memory stack ===
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="outline",
            content="å¤§çº²ä»»åŠ¡: ä½¿ç”¨ FactStruct Stage 1 ç”Ÿæˆæˆ–è°ƒæ•´æŠ¥å‘Šå¤§çº²",
            result={
                "outline": outline_response,
                "factstruct_outline": factstruct_outline_dict,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("å¤§çº²ç”Ÿæˆå®Œæˆï¼ˆFactStruct Stage 1ï¼‰ï¼Œè¿”å›ä¸­æ¢Agent")

        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="å¤§çº²ç”Ÿæˆå®Œæˆï¼ˆFactStruct Stage 1ï¼‰ï¼Œè¿”å›ä¸­æ¢Agent",
                        name="outline",
                    )
                ],
                "report_outline": outline_response,
                "factstruct_outline": factstruct_outline_dict,
                "factstruct_memory": factstruct_memory_dict,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )





    @timed_step("execute_word_planning")
    def execute_word_planning(
        self, outline_root: OutlineNode, total_word_limit: int
    ) -> OutlineNode:
        """
        æ‰§è¡Œå­—æ•°è§„åˆ’ï¼Œä¸ºå¤§çº²ä¸­çš„æ¯ä¸ªå¶å­èŠ‚ç‚¹åˆ†é…å­—æ•°é…é¢

        Args:
            outline_root: å¤§çº²æ ¹èŠ‚ç‚¹
            total_word_limit: ç”¨æˆ·æŒ‡å®šçš„æ€»å­—æ•°é™åˆ¶

        Returns:
            æ›´æ–°äº†å­—æ•°é…é¢çš„å¤§çº²æ ¹èŠ‚ç‚¹
        """
        import json

        logger.info(f"å¼€å§‹å­—æ•°è§„åˆ’ï¼Œæ€»å­—æ•°é™åˆ¶: {total_word_limit}")

        # æ„å»ºå¤§çº²ç»“æ„ä¿¡æ¯ä¾›LLMåˆ†æ
        def build_outline_info(node: OutlineNode, depth: int = 0) -> list:
            nodes_info = []
            nodes_info.append(
                {
                    "id": node.id,
                    "title": node.title,
                    "depth": depth,
                    "is_leaf": node.is_leaf(),
                }
            )
            for child in node.children:
                nodes_info.extend(build_outline_info(child, depth + 1))
            return nodes_info

        outline_info = build_outline_info(outline_root)
        leaf_nodes = [n for n in outline_info if n["is_leaf"]]

        # æ„å»ºLLMè¯·æ±‚
        outline_text = outline_root.to_text_tree()
        prompt_content = f"""è¯·ä¸ºä»¥ä¸‹æŠ¥å‘Šå¤§çº²åˆ†é…å­—æ•°ã€‚

        ## å¤§çº²ç»“æ„
        {outline_text}

        ## å¶å­èŠ‚ç‚¹åˆ—è¡¨
        {json.dumps(leaf_nodes, ensure_ascii=False, indent=2)}

        ## æ€»å­—æ•°é™åˆ¶
        {total_word_limit} å­—

        è¯·æ ¹æ®æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„é‡è¦æ€§å’Œå†…å®¹å¤æ‚åº¦ï¼Œæ™ºèƒ½åˆ†é…å­—æ•°é…é¢ã€‚
        ä½ å¿…é¡»åªè¾“å‡ºä¸€ä¸ªåˆæ³•çš„ JSON å¯¹è±¡ã€‚ç¦æ­¢è¾“å‡ºä»»ä½•è§£é‡Šã€è¯´æ˜ã€æ³¨é‡Šã€æ ‡é¢˜æˆ–é¢å¤–æ–‡æœ¬ã€‚å¦‚æœè¾“å‡ºåŒ…å«é JSON å†…å®¹ï¼Œå°†è¢«è§†ä¸ºé”™è¯¯ã€‚
        """

        try:
            messages = apply_prompt_template("word_planner", {"messages": []}) + [
                HumanMessage(content=prompt_content)
            ]
            llm = get_llm_by_type(AGENT_LLM_MAP.get("outline", "default"))
            response = llm.invoke(messages)
            result = response.content

            # è§£æJSONç»“æœ
            logger.info(f"result:{result}")
            # result = result.replace("```json", "").replace("```", "").strip()

            match = re.search(r"\{[\s\S]*\}", result)
            if not match:
                raise ValueError("No JSON object found in LLM output")

            allocations = json.loads(match.group(0))

            # å°†å­—æ•°é…é¢å†™å…¥èŠ‚ç‚¹
            for alloc in allocations.get("allocations", []):
                node_id = alloc.get("node_id")
                word_limit = alloc.get("word_limit", 0)
                node = outline_root.find_node_by_id(node_id)
                if node:
                    node.word_limit = word_limit
                    logger.debug(
                        f"èŠ‚ç‚¹ {node_id} ({node.title}) åˆ†é…å­—æ•°: {word_limit}"
                    )

            # è‡ªåº•å‘ä¸Šè®¡ç®—éå¶å­èŠ‚ç‚¹çš„å­—æ•°
            def update_parent_word_limits(node: OutlineNode) -> int:
                if node.is_leaf():
                    return node.word_limit
                total = sum(update_parent_word_limits(child) for child in node.children)
                node.word_limit = total
                return total

            update_parent_word_limits(outline_root)
            logger.info(f"å­—æ•°è§„åˆ’å®Œæˆï¼Œæ ¹èŠ‚ç‚¹æ€»å­—æ•°: {outline_root.word_limit}")

        except Exception as e:
            logger.error(f"å­—æ•°è§„åˆ’å¤±è´¥: {str(e)}")
            # Fallback: å¹³å‡åˆ†é…
            leaf_nodes_obj = outline_root.get_leaf_nodes()
            avg_words = total_word_limit // len(leaf_nodes_obj) if leaf_nodes_obj else 0
            for node in leaf_nodes_obj:
                node.word_limit = avg_words
            logger.warning(f"ä½¿ç”¨å¹³å‡åˆ†é…ç­–ç•¥ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹: {avg_words} å­—")

        logger.info(f"outline_root:{outline_root}")
        # exit()
        return outline_root
