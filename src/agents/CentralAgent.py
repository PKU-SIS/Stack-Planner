import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Annotated, Any, Dict, List, Literal, Optional, Type, Union, cast

from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command

from src.agents.sub_agent_registry import get_sub_agents_by_global_type
from src.config.agents import AGENT_LLM_MAP
from src.llms.llm import get_llm_by_type
from src.memory import MemoryStack, MemoryStackEntry
from src.prompts.template import apply_prompt_template, get_prompt_template
from src.utils.json_utils import repair_json_output
from src.utils.logger import logger
from src.utils.statistics import global_statistics
from src.prompts.central_decision import Decision, DelegateParams

from ..graph.types import State

# from .SubAgentConfig import get_sub_agents_by_global_type


# -------------------------
# æ ¸å¿ƒæšä¸¾å®šä¹‰
# -------------------------
class CentralAgentAction(Enum):
    """ä¸­æ¢AgentåŠ¨ä½œæšä¸¾ï¼Œå®šä¹‰ç³»ç»Ÿæ ¸å¿ƒå†³ç­–ç±»å‹"""

    THINK = "think"  # åˆ†æå½“å‰çŠ¶æ€å¹¶æ€è€ƒä¸‹ä¸€æ­¥è¡ŒåŠ¨
    REFLECT = "reflect"  # åæ€ä¹‹å‰çš„åŠ¨ä½œå’Œç»“æœ
    SUMMARIZE = "summarize"  # æ€»ç»“å½“å‰å·²è·å¾—çš„ä¿¡æ¯
    DELEGATE = "delegate"  # å§”æ´¾å­Agentæ‰§è¡Œä¸“é¡¹ä»»åŠ¡
    FINISH = "finish"  # åˆ¤æ–­ä»»åŠ¡å®Œæˆå¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š


# -------------------------
# ä¸­æ¢Agentæ ¸å¿ƒæ¨¡å—--ä¸­æ¢Agentçš„action
# exp: ä¸prompt/central_decision.pyä¸­çš„Decisionç±»ä¸åŒçš„æ˜¯ï¼Œé‚£é‡Œæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œè¿™é‡Œæ˜¯æšä¸¾ç±»å‹ï¼Œæ‰€ä»¥è¦å®šä¹‰ä¸¤æ¬¡
# -------------------------
@dataclass
class CentralDecision:
    """ä¸­æ¢Agentå†³ç­–ç»“æœæ•°æ®æ¨¡å‹"""

    action: CentralAgentAction  # å†³ç­–åŠ¨ä½œ
    reasoning: str  # å†³ç­–æ¨ç†è¿‡ç¨‹
    params: Dict[DelegateParams, Any] = field(
        default_factory=dict
    )  # åŠ¨ä½œå‚æ•°=>delegateæœ‰å‚æ•°
    instruction: Optional[str] = None  # åŠ¨ä½œå¯¹åº”çš„æŒ‡ä»¤è¯´æ˜


class CentralAgent:
    """
    ä¸­æ¢Agentæ ¸å¿ƒç±»ï¼Œè´Ÿè´£ç³»ç»Ÿæ•´ä½“å†³ç­–ä¸ä»»åŠ¡ç¼–æ’

    é‡‡ç”¨åŸºäºè®°å¿†æ ˆçš„å†³ç­–æœºåˆ¶ï¼Œé€šè¿‡çŠ¶æ€åˆ†æåŠ¨æ€å§”æ´¾å­Agentæ‰§è¡Œä¸“é¡¹ä»»åŠ¡ï¼Œ
    å¹¶æœ€ç»ˆæ•´åˆç»“æœç”Ÿæˆå®ŒæˆæŠ¥å‘Š
    """

    def __init__(self, graph_format: str = "sp"):
        self.memory_stack = MemoryStack()
        from src.agents.SubAgentManager import SubAgentManager

        self.sub_agent_manager = SubAgentManager(self)

        sub_agents = get_sub_agents_by_global_type(graph_format)
        logger.info(f"åˆå§‹åŒ–ä¸­æ¢Agentï¼Œä½¿ç”¨å­Agentç±»å‹: {sub_agents}")

        # åˆå§‹åŒ–å­Agentç›¸å…³ä¿¡æ¯
        self.available_sub_agents = [agent["name"] for agent in sub_agents]
        self.sub_agents_description = ""
        for agent in sub_agents:
            self.sub_agents_description += (
                f"- **{agent['name']}**: {agent['description']}\n"
            )

        # åŠ¨ä½œå¤„ç†å™¨æ˜ å°„è¡¨
        self.action_handlers = {
            CentralAgentAction.THINK: self._handle_think,
            CentralAgentAction.REFLECT: self._handle_reflect,
            CentralAgentAction.SUMMARIZE: self._handle_summarize,
            CentralAgentAction.DELEGATE: self._handle_delegate,
            CentralAgentAction.FINISH: self._handle_finish,
        }

        # åŠ¨ä½œç±»å‹å¯¹åº”çš„æŒ‡ä»¤æ¨¡æ¿
        self.action_instructions = {
            CentralAgentAction.THINK: "åˆ†æå½“å‰çŠ¶æ€å¹¶æ€è€ƒä¸‹ä¸€æ­¥è¡ŒåŠ¨",
            CentralAgentAction.REFLECT: "åæ€ä¹‹å‰çš„åŠ¨ä½œå’Œç»“æœ",
            CentralAgentAction.SUMMARIZE: "æ€»ç»“å½“å‰å·²è·å¾—çš„ä¿¡æ¯",
            CentralAgentAction.DELEGATE: "å†³å®šå§”æ´¾å“ªä¸ªå­Agentæ‰§è¡Œä»»åŠ¡",
            CentralAgentAction.FINISH: "åˆ¤æ–­æ˜¯å¦å¯ä»¥å®Œæˆä»»åŠ¡å¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
        }

    def make_decision(
        self, state: State, config: RunnableConfig, retry_count: int = 0
    ) -> CentralDecision:
        """
        ä¸­æ¢Agentå†³ç­–æ ¸å¿ƒé€»è¾‘ï¼Œåˆ†æå½“å‰çŠ¶æ€ç”Ÿæˆå†³ç­–ç»“æœ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            å†³ç­–ç»“æœå¯¹è±¡
        """
        max_retries = 3
        logger.info("ä¸­æ¢Agentæ­£åœ¨è¿›è¡Œå†³ç­–...")
        start_time = datetime.now()

        # æ„å»ºå†³ç­–prompt
        messages = self._build_decision_prompt(state, config)
        # logger.debug(f"å†³ç­–prompt: {messages}")

        # è·å–LLMå†³ç­–å¹¶å¤„ç†å¼‚å¸¸
        try:
            llm = get_llm_by_type(
                AGENT_LLM_MAP.get("central_agent", "default")
            ).with_structured_output(
                Decision,
                method="json_mode",
            )
            response = llm.invoke(messages)

            # è§£æå†³ç­–ç»“æœ
            action = CentralAgentAction(response.action)
            reasoning = response.reasoning
            params = response.params or {}
            instruction = response.instruction or self.action_instructions.get(
                action, ""
            )
            if state.get("locale") == None:
                locale = response.locale or "zh-CN"
                # å°† locale æ·»åŠ åˆ° state
                state["locale"] = locale

            logger.info(f"å†³ç­–ç»“æœ: {response}")
            end_time = datetime.now()
            time_entry = {
                "step_name": "central decision" + start_time.isoformat(),
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration": (end_time - start_time).total_seconds(),
            }
            global_statistics.add_time_entry(time_entry)

            return CentralDecision(
                action=action,
                reasoning=reasoning,
                params=params,
                instruction=instruction,
            )

        except Exception as e:
            import traceback

            logger.error(
                f"å†³ç­–è§£æå¤±è´¥:  (å°è¯• {retry_count + 1}/{max_retries}): {str(e)}"
            )
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š\n" + traceback.format_exc())
            if retry_count < max_retries - 1:
                return self.make_decision(state, config, retry_count + 1)
            # å¼‚å¸¸æƒ…å†µä¸‹è¿”å›é»˜è®¤å†³ç­–
            end_time = datetime.now()
            time_entry = {
                "step_name": "central_decision" + start_time.isoformat(),
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration": (end_time - start_time).total_seconds(),
            }
            global_statistics.add_time_entry(time_entry)
            return CentralDecision(
                action=CentralAgentAction.THINK,
                reasoning="å†³ç­–è§£æå¤±è´¥ï¼Œé»˜è®¤é€‰æ‹©æ€è€ƒåŠ¨ä½œ",
                params={},
                instruction=self.action_instructions[CentralAgentAction.THINK],
            )

    def _build_decision_prompt(
        self,
        state: State,
        config: RunnableConfig,
    ) -> List[Union[AIMessage, HumanMessage]]:
        """
        æ„å»ºä¸­æ¢Agentå†³ç­–æç¤ºè¯ï¼Œä½¿ç”¨ç»Ÿä¸€çš„promptæ¨¡æ¿

        Args:
            context: å†³ç­–ä¸Šä¸‹æ–‡ï¼ˆå·²åŒ…å«æ‰€æœ‰å…³é”®å‚æ•°ï¼‰
            config: è¿è¡Œé…ç½®
            action_options: å¯ç”¨åŠ¨ä½œé€‰é¡¹

        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯æ¶ˆæ¯åˆ—è¡¨
        """
        messages_history = state.get("messages", [])
        converted_messages = []
        for msg in messages_history:
            if isinstance(msg, (HumanMessage, AIMessage)):
                converted_messages.append(
                    {
                        "role": msg.type,
                        "content": msg.content,
                        "additional_kwargs": getattr(msg, "additional_kwargs", {}),
                    }
                )
            else:
                converted_messages.append(msg)

        # æå–ç”¨æˆ·åé¦ˆå¹¶æ ¼å¼åŒ–ä¸ºå¼ºè°ƒæ–‡æœ¬
        user_feedback_text = ""
        hitl_feedback = state.get("hitl_feedback", "")
        if hitl_feedback:
            user_feedback_text = f"\n\nğŸ”´ **CRITICAL USER FEEDBACK**: {hitl_feedback}\n\nThis feedback MUST be considered in your decision-making process."

        # ä»è®°å¿†æ ˆä¸­æå–æ‰€æœ‰ç”¨æˆ·åé¦ˆ
        user_feedbacks_from_memory = []
        for entry in self.memory_stack.get_all():
            if entry.action == "human_feedback":
                feedback_content = entry.content
                if entry.result:
                    feedback_type = entry.result.get("feedback_type", "")
                    if feedback_type == "content_modify":
                        request = entry.result.get("request", "")
                        user_feedbacks_from_memory.append(f"- {request}")
                    else:
                        user_feedbacks_from_memory.append(f"- {feedback_content}")
                else:
                    user_feedbacks_from_memory.append(f"- {feedback_content}")

        if user_feedbacks_from_memory:
            user_feedback_text += (
                "\n\nğŸ”´ **USER FEEDBACK HISTORY**:\n"
                + "\n".join(user_feedbacks_from_memory)
                + "\n\nâš ï¸ All feedback above MUST be addressed. When delegating to reporter, ensure these requirements are fulfilled."
            )

        context = {
            "available_actions": [action.value for action in CentralAgentAction],
            "available_sub_agents": self.available_sub_agents,
            "sub_agents_description": self.sub_agents_description,
            "current_action": "decision",
            "messages_history": converted_messages,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
            "user_feedback": user_feedback_text,  # æ·»åŠ ç”¨æˆ·åé¦ˆåˆ°ä¸Šä¸‹æ–‡
        }
        action_options = list(CentralAgentAction)
        # åŠ è½½æ­£ç¡®çš„æ¨¡æ¿åç§°å¹¶åˆå¹¶åŠ¨ä½œé€‰é¡¹
        context_with_actions = {
            **context,
            **config,
            "available_actions": ", ".join([a.value for a in action_options]),
        }
        return apply_prompt_template(
            "central_agent", state, extra_context=context_with_actions
        )

    def execute_action(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡Œå†³ç­–åŠ¨ä½œï¼Œè°ƒåº¦å¯¹åº”çš„åŠ¨ä½œå¤„ç†å™¨

        Args:
            decision: å†³ç­–ç»“æœ
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            åŠ¨ä½œæ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        handler = self.action_handlers.get(decision.action)
        if not handler:
            error_msg = f"æœªçŸ¥åŠ¨ä½œ: {decision.action}"
            logger.error(error_msg)
            return Command(
                update={
                    "messages": [
                        AIMessage(
                            content=f"é”™è¯¯ï¼šæœªçŸ¥åŠ¨ä½œ: {decision.action}",
                            name="central_error",
                        )
                    ],
                    "locale": state.get("locale"),
                    "current_node": "central_agent",
                    "memory_stack": self.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        return handler(decision, state, config)

    def _handle_think(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†æ€è€ƒåŠ¨ä½œï¼Œåˆ†æå½“å‰çŠ¶æ€ç”Ÿæˆä¸‹ä¸€æ­¥è®¡åˆ’"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨æ€è€ƒ...")
        start_time = datetime.now()
        context = {
            "current_action": "think",
            "current_progress": state.get("observations", []),
            "decision_reasoning": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # åº”ç”¨ç»Ÿä¸€çš„å†³ç­–æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # è®°å½•æ€è€ƒè¿‡ç¨‹åˆ°è®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="think",
            content=response.content,
        )
        self.memory_stack.push(memory_entry)

        logger.info(f"central_think: {response.content}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_think" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [AIMessage(content=response.content, name="central_think")],
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_reflect(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†åæ€åŠ¨ä½œï¼Œè¯„ä¼°ä¹‹å‰çš„æ­¥éª¤å¹¶æ¸…ç†è®°å¿†æ ˆ"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨åæ€...")
        start_time = datetime.now()

        # è·å–åæ€ç›®æ ‡å’Œä¸Šä¸‹æ–‡
        # recent_memory = self.memory_stack.get_recent(5)  # è·å–æœ€è¿‘5æ¡è®°å¿†

        context = {
            "current_action": "reflect",
            "decision_reasoning": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # åº”ç”¨åæ€æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # è§£æåæ€ç»“æœçš„JSON
        try:
            reflection_data = json.loads(repair_json_output(response.content))
            analysis = reflection_data.get("analysis", "åæ€åˆ†æ")
            pop_count = reflection_data.get("pop_count", 0)
            reasoning = reflection_data.get("reasoning", "åæ€å®Œæˆ")

            # éªŒè¯pop_countæ˜¯æœ‰æ•ˆæ•°å­—
            if not isinstance(pop_count, int) or pop_count < 0:
                logger.warning(f"æ— æ•ˆçš„pop_count: {pop_count}ï¼Œè®¾ç½®ä¸º0")
                pop_count = 0

        except Exception as e:
            logger.error(f"åæ€ç»“æœè§£æå¤±è´¥: {e}")
            analysis = response.content
            pop_count = 0
            reasoning = "JSONè§£æå¤±è´¥ï¼Œä¿æŒç°æœ‰è®°å¿†æ ˆ"

        logger.debug(f"reflectå†³å®šæ¸…ç†{pop_count}æ¡æ¶ˆæ¯")
        # æ‰§è¡Œè®°å¿†æ ˆæ¸…ç†
        removed_items = []
        if pop_count > 0:
            reflection_content = (
                f"åæ€åˆ†æ: {analysis}\n"
                f"åæ€åŸå› : {reasoning}\n"
                f"æ¸…ç†äº† {pop_count} æ¡è®°å¿†ã€‚"
            )

            memory_entry = MemoryStackEntry(
                timestamp=datetime.now().isoformat(),
                action="reflect",
                content=reflection_content,
            )

            self.memory_stack.push_with_pop(memory_entry, pop_count)

            removed_items = self.memory_stack.pop(pop_count)

            logger.info(f"æˆåŠŸä»è®°å¿†æ ˆä¸­ç§»é™¤äº† {pop_count} é¡¹è®°å¿†")
            # logger.info(
            #     f"ä»è®°å¿†æ ˆä¸­ç§»é™¤äº† {len(removed_items)} é¡¹: {[item.action for item in removed_items]}"
            # )
        else:
            logger.info("ä¸ç§»é™¤ä»»ä½•è®°å¿†æ ˆé¡¹ç›®")

        logger.info(f"central_reflect: {analysis}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_reflect" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [AIMessage(content=analysis, name="central_reflect")],
                "reflection": {
                    "analysis": analysis,
                    "pop_count": len(removed_items),
                    "reasoning": reasoning,
                    "removed_items": removed_items,
                },
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_summarize(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†æ€»ç»“åŠ¨ä½œï¼Œå½’çº³å½“å‰å·²è·å¾—çš„ä¿¡æ¯"""
        logger.info("ä¸­æ¢Agentæ­£åœ¨æ€»ç»“...")
        start_time = datetime.now()

        context = {
            "current_action": "summarize",
            "summarization_focus": decision.reasoning,
            "instruction": decision.instruction,
            "locale": state.get("locale", "zh-CN"),  # ç¡®ä¿localeè¢«ä¼ é€’åˆ°æ¨¡æ¿
        }

        # æ‰“å°ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
        logger.debug(
            f"Summarize context: {json.dumps(context, ensure_ascii=False, indent=2)}"
        )

        # åº”ç”¨ç»Ÿä¸€çš„æ€»ç»“æç¤ºæ¨¡æ¿
        messages = apply_prompt_template("central_agent", state, extra_context=context)

        llm = get_llm_by_type(AGENT_LLM_MAP.get("central_agent", "default"))
        response = llm.invoke(messages)

        # æ›´æ–°è®°å¿†æ ˆï¼Œæ›¿æ¢æœ€æ–°çš„æ€»ç»“ç»“æœ
        new_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="summarize",
            content=context.get("summarization_focus", ""),
            result={"summary_result": response.content},
        )

        # logger.info("NEW_ENTRY", new_entry)
        # logger.info("*"*100)

        self.memory_stack.push_with_pop(new_entry)

        # logger.info(f"central_summarize: {response.content}")
        end_time = datetime.now()
        time_entry = {
            "step_name": "central_summarize" + start_time.isoformat(),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration": (end_time - start_time).total_seconds(),
        }
        global_statistics.add_time_entry(time_entry)
        return Command(
            update={
                "messages": [
                    AIMessage(content=response.content, name="central_summarize")
                ],
                "summary": response.content,
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto="central_agent",
        )

    def _handle_delegate(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†å§”æ´¾åŠ¨ä½œï¼Œè°ƒåº¦å­Agentæ‰§è¡Œä¸“é¡¹ä»»åŠ¡"""
        agent_type = decision.params.agent_type
        task_description = decision.params.task_description
        # agent_type = decision.agent_type
        # task_description = decision.task_description or "æœªæŒ‡å®šä»»åŠ¡"

        # éªŒè¯å­Agentç±»å‹æœ‰æ•ˆæ€§
        if not agent_type or agent_type not in self.available_sub_agents:
            error_msg = (
                f"æ— æ•ˆçš„å­Agentç±»å‹: {agent_type}ï¼Œå¯ç”¨ç±»å‹: "
                f"{self.available_sub_agents}"
            )
            logger.error(f"central_error: {error_msg}")
            return Command(
                update={
                    "messages": [AIMessage(content=error_msg, name="central_error")],
                    "current_node": "central_agent",
                },
                goto="central_agent",
            )

        logger.info(f"ä¸­æ¢Agentå§”æ´¾ {agent_type} æ‰§è¡Œä»»åŠ¡: {task_description}")

        # è®°å½•å§”æ´¾åŠ¨ä½œåˆ°è®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type=agent_type,
            content=f"å§”æ´¾ä»»åŠ¡: {task_description}",
        )
        self.memory_stack.push(memory_entry)

        # æ„å»ºå­Agentæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†æ ˆæ‘˜è¦ï¼‰
        delegation_context = {
            "task_description": task_description,
            "agent_type": agent_type,
            "memory_context": self.memory_stack.get_summary(include_full_history=True),
            "original_query": state.get("user_query", ""),
        }

        logger.info(f"central_delegate: å§”æ´¾{agent_type}æ‰§è¡Œ: {task_description}")
        return Command(
            update={
                "messages": [
                    AIMessage(
                        content=f"å§”æ´¾{agent_type}æ‰§è¡Œ: {task_description}",
                        name="central_delegate",
                    )
                ],
                "delegation_context": delegation_context,
                "current_node": "central_agent",
                "memory_stack": json.dumps(
                    [entry.to_dict() for entry in self.memory_stack.get_all()]
                ),
                "locale": state.get("locale"),
            },
            goto=agent_type,
        )

    def _handle_finish(
        self, decision: CentralDecision, state: State, config: RunnableConfig
    ) -> Command:
        """å¤„ç†å®ŒæˆåŠ¨ä½œï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¹¶ç»“æŸä»»åŠ¡"""
        logger.info("ä¸­æ¢Agentå®Œæˆä»»åŠ¡...")

        final_report = state.get("final_report", None)
        if not final_report:
            logger.info("æœªæ‰¾åˆ°æœ€ç»ˆæŠ¥å‘Šï¼Œå§”æ´¾Reporter Agentç”ŸæˆæŠ¥å‘Š...")

            # è®°å½•å§”æ´¾åŠ¨ä½œåˆ°è®°å¿†æ ˆ
            memory_entry = MemoryStackEntry(
                timestamp=datetime.now().isoformat(),
                action="delegate",
                agent_type="reporter",
                content="æœªç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            )
            self.memory_stack.push(memory_entry)

            # æ„å»ºReporteræ‰§è¡Œä¸Šä¸‹æ–‡
            delegation_context = {
                "task_description": "æ ¹æ®æ‰€æœ‰æ”¶é›†åˆ°çš„ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š",
                "agent_type": "reporter",
                "memory_context": self.memory_stack.get_summary(
                    include_full_history=True
                ),
                "original_query": state.get("user_query", ""),
                "report_type": "final_report",
                "execution_history": [
                    entry.to_dict() for entry in self.memory_stack.get_all()
                ],
            }

            logger.info("central_delegate_reporter: å§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
            return Command(
                update={
                    "messages": [
                        AIMessage(
                            content="å§”æ´¾Reporter Agentç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
                            name="central_delegate_reporter",
                        )
                    ],
                    "delegation_context": delegation_context,
                    "current_node": "central_agent",
                    "memory_stack": json.dumps(
                        [entry.to_dict() for entry in self.memory_stack.get_all()]
                    ),
                    "pending_finish": True,  # æ ‡è®°ç­‰å¾…æŠ¥å‘Šå®Œæˆåå†finish
                },
                goto="reporter",
            )
        logger.info(f"final_report: {final_report}")

        # æ„å»ºæ‰§è¡Œæ‘˜è¦ï¼ˆåŒ…å«å®Œæ•´è®°å¿†æ ˆå†å²ï¼‰
        execution_summary = {
            "user_query": state.get("user_query", "æœªçŸ¥æŸ¥è¯¢"),
            "execution_history": [
                entry.to_dict() for entry in self.memory_stack.get_all()
            ],
            "final_report": final_report,
            "completion_time": datetime.now().isoformat(),
            "statistics": global_statistics.get_statistics(),
        }

        # ä¿å­˜æ‰§è¡Œæ‘˜è¦åˆ°æ–‡ä»¶
        os.makedirs("./reports", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"./reports/execution_report_{timestamp}.json"

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(execution_summary, f, ensure_ascii=False, indent=4)
            report_msg = f"ä»»åŠ¡å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {filename}"
        except Exception as e:
            logger.error(f"æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}")
            report_msg = f"ä»»åŠ¡å®Œæˆï¼Œä½†æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}"
            execution_summary["error"] = str(e)

        logger.info(report_msg)
        logger.info(global_statistics.get_statistics())
        return Command(
            goto="zip_data",  # ç»“æŸæ‰§è¡Œ
        )
