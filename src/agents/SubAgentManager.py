from src.llms.llm import get_llm_by_type
from ..graph.types import State
from langchain_core.runnables import RunnableConfig
from datetime import datetime

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command, interrupt

from src.agents.CoderAgent import CoderAgent
from src.agents.ResearcherAgent_SP import ResearcherAgentSP
from src.tools import (
    crawl_tool,
    get_web_search_tool,
    get_retriever_tool,
    python_repl_tool,
    search_docs_tool,
)
from src.utils.json_utils import repair_json_output
from src.utils.logger import logger
from src.config.agents import AGENT_LLM_MAP
from src.llms.llm import get_llm_by_type
from src.prompts.template import apply_prompt_template
from src.memory import MemoryStack, MemoryStackEntry
from src.agents.CentralAgent import CentralAgent
from src.tools.get_docs_info import search_docs_with_ref

from ..graph.types import State
from ..config import SELECTED_SEARCH_ENGINE, SearchEngine
from src.utils.statistics import global_statistics, timed_step
import re


# -------------------------
# å­Agentç®¡ç†æ¨¡å—
# TODO: check sub-agent bugs
# TODO: æœç´¢å¤ªå¤šæ—¶ä¼šè¶…è¿‡è¾“å…¥é™åˆ¶æˆ–è€…ç¼“å†²åŒºæº¢å‡ºï¼Œéœ€è¦é™åˆ¶æœç´¢åˆ°çš„å†…å®¹é•¿åº¦æˆ–è€…åšä¸€ä¸ªç®€å•çš„æ‘˜è¦
# TODO: éœ€è¦å¤„ç†æœç´¢æ•æ„Ÿè¯ï¼ˆä»¥â€œ985å¤§å­¦æœ€å¤šçš„äº”ä¸ªåŸå¸‚â€ä¸ºä¾‹ï¼ŒAIå°±æ— æ³•å¤„ç†ä¿¡æ¯ï¼Œè¿”å›Errorï¼‰
# -------------------------
class SubAgentManager:
    """å­Agentç®¡ç†å™¨ï¼Œè´Ÿè´£åˆ›å»ºå’Œæ‰§è¡Œå„ç±»ä¸“é¡¹å­Agent"""

    def __init__(self, central_agent: "CentralAgent"):
        self.central_agent = central_agent

    @timed_step("execute_researcher")
    async def execute_researcher(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œç ”ç©¶Agentï¼Œè´Ÿè´£ä¿¡æ¯æ£€ç´¢ä¸åˆ†æ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç ”ç©¶Agentå¼€å§‹æ‰§è¡Œ...")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")

        # é…ç½®ç ”ç©¶å·¥å…·é“¾
        tools = [get_web_search_tool(10), crawl_tool, search_docs_tool]
        retriever_tool = get_retriever_tool(state.get("resources", []))
        if retriever_tool:
            tools.insert(0, retriever_tool)

        # å®ä¾‹åŒ–ç ”ç©¶Agent
        research_agent = ResearcherAgentSP(
            config=config, agent_type="researcher", default_tools=tools
        )

        # æ‰§è¡Œç ”ç©¶ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await research_agent.execute_agent_step(state)

            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            result_data_collections = []

            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
                result_data_collections = result_command.update.get(
                    "data_collections", []
                )

            logger.info(f"data_collections_in subagent:{result_data_collections}")

        except Exception as e:
            logger.error(f"Researcher Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content=f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}", name="researcher"
                        )
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="researcher",
            content=f"ç ”ç©¶ä»»åŠ¡: {task_description}",
            result={
                "observations": result_observations,
                # "data_collections": result_data_collections,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="researcher"
                    )
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
                "data_collections": result_data_collections,
                "observations": result_observations,
            },
            goto="central_agent",
        )

    @timed_step("execute_xxqg_researcher")
    async def execute_xxqg_researcher(
        self, state: State, config: RunnableConfig
    ) -> Command:
        """
        æ‰§è¡Œç ”ç©¶Agentï¼Œè´Ÿè´£ä¿¡æ¯æ£€ç´¢ä¸åˆ†æ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç ”ç©¶Agentå¼€å§‹æ‰§è¡Œ...")
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç ”ç©¶ä»»åŠ¡")

        # é…ç½®ç ”ç©¶å·¥å…·é“¾
        tools = [search_docs_tool]

        # å®ä¾‹åŒ–ç ”ç©¶Agent
        research_agent = ResearcherAgentSP(
            config=config, agent_type="researcher_xxqg_demo", default_tools=tools
        )

        # æ‰§è¡Œç ”ç©¶ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await research_agent.execute_agent_step(state)

            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            result_data_collections = []

            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
                result_data_collections = result_command.update.get(
                    "data_collections", []
                )

        except Exception as e:
            import traceback

            logger.error(traceback.format_exc())
            logger.error(f"ç ”ç©¶Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content=f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}", name="researcher"
                        )
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="researcher",
            content=f"ç ”ç©¶ä»»åŠ¡: {task_description}",
            result={
                "observations": result_observations,
                # "data_collections": result_data_collections,
            },
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content="ç ”ç©¶ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="researcher"
                    )
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
                "data_collections": result_data_collections,
                "observations": result_observations,
            },
            goto="central_agent",
        )

    @timed_step("execute_coder")
    async def execute_coder(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œç¼–ç Agentï¼Œè´Ÿè´£ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ç¼–ç Agentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "æœªçŸ¥ç¼–ç ä»»åŠ¡")

        # å®ä¾‹åŒ–ç¼–ç Agent
        code_agent = CoderAgent(
            config=config, agent_type="coder", default_tools=[python_repl_tool]
        )

        # æ‰§è¡Œç¼–ç ä»»åŠ¡å¹¶å¤„ç†å¼‚å¸¸
        try:
            result_command = await code_agent.execute_agent_step(state)
            # ä»ç»“æœä¸­æå–æ•°æ®ç”¨äºè®°å¿†æ ˆ
            result_observations = []
            if result_command and result_command.update:
                result_observations = result_command.update.get("observations", [])
        except Exception as e:
            logger.error(f"ç¼–ç Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return Command(
                update={
                    "messages": [
                        HumanMessage(content=f"ç¼–ç ä»»åŠ¡å¤±è´¥: {str(e)}", name="coder")
                    ],
                    "current_node": "central_agent",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="coder",
            content=f"ç¼–ç ä»»åŠ¡: {task_description}",
            result={"observations": result_observations},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ç¼–ç ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="ç¼–ç ä»»åŠ¡å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="coder")
                ],
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_reporter")
    def execute_reporter(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡ŒæŠ¥å‘ŠAgentï¼Œè´Ÿè´£ç»“æœæ•´ç†ä¸æŠ¥å‘Šç”Ÿæˆ

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("æŠ¥å‘ŠAgentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")

        # æ„å»ºç²¾ç®€çš„ reporter è¾“å…¥ï¼ŒåªåŒ…å«å¿…è¦ä¿¡æ¯
        # é¿å…ä¼ å…¥å®Œæ•´ state["messages"]ï¼ˆåŒ…å«å¤§é‡ central agent è°ƒåº¦ä¿¡æ¯ï¼‰
        current_plan = state.get("current_plan")
        plan_info = ""
        if current_plan:
            plan_title = getattr(current_plan, "title", str(current_plan))
            plan_thought = getattr(current_plan, "thought", "")
            plan_info = f"## Task\n\n{plan_title}\n\n## Description\n\n{plan_thought}"

        reporter_input = {
            "messages": [
                HumanMessage(
                    content=f"# Research Requirements\n\n## User Query\n\n{state.get('user_query', '')}\n\n{plan_info}"
                )
            ],
            "locale": state.get("locale", "zh-CN"),
        }

        # æ”¶é›†æŠ¥å‘Šç”Ÿæˆæ‰€éœ€ä¸Šä¸‹æ–‡
        context = {
            "user_query": state.get("user_query", ""),
            "task_description": task_description,
        }

        # ç”ŸæˆæŠ¥å‘Šå¹¶å¤„ç†å¼‚å¸¸
        final_report = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "reporter", reporter_input, extra_context=context
            )

            # æå–å¹¶å¼ºè°ƒç”¨æˆ·çš„å†å²åé¦ˆæ„è§
            user_feedbacks = []
            for entry in self.central_agent.memory_stack.get_all():
                if entry.action == "human_feedback":
                    # æå–åé¦ˆå†…å®¹
                    feedback_content = entry.content
                    if entry.result:
                        feedback_type = entry.result.get("feedback_type", "")
                        if feedback_type == "content_modify":
                            request = entry.result.get("request", "")
                            user_feedbacks.append(f"- {request}")
                        else:
                            user_feedbacks.append(f"- {feedback_content}")
                    else:
                        user_feedbacks.append(f"- {feedback_content}")

            # å¦‚æœæœ‰ç”¨æˆ·åé¦ˆï¼Œåœ¨æ˜¾è‘—ä½ç½®æ·»åŠ åˆ°messagesä¸­
            if user_feedbacks:
                feedback_message = (
                    "# ğŸ”´ CRITICAL: User Feedback Requirements\n\n"
                    "The user has provided the following feedback that MUST be incorporated into the report:\n\n"
                    + "\n".join(user_feedbacks)
                    + "\n\n"
                    "âš ï¸ These requirements are MANDATORY and must be fully addressed in the generated report. "
                    "Do not ignore or dilute any of these feedback points."
                )
                messages.append(
                    HumanMessage(
                        content=feedback_message, name="user_feedback_emphasis"
                    )
                )

            # æ·»åŠ  observations å’Œ data_collections
            observations = state.get("observations", [])
            for observation in observations:
                messages.append(
                    HumanMessage(
                        content=f"Below are some observations for the research task:\n\n{observation}",
                        name="observation",
                    )
                )
            data_collections = state.get("data_collections", [])
            for data_collection in data_collections:
                messages.append(
                    HumanMessage(
                        content=f"Below are data collected in previous tasks:\n\n{data_collection}",
                        name="observation",
                    )
                )

            llm = get_llm_by_type(AGENT_LLM_MAP.get("reporter", "default"))
            response = llm.invoke(messages)
            final_report = response.content
        except Exception as e:
            logger.error(f"æŠ¥å‘ŠAgentæ‰§è¡Œå¤±è´¥: {str(e)}")
            final_report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="reporter",
            content=f"æŠ¥å‘Šä»»åŠ¡: {task_description}",
            result={"final_report": final_report},
        )
        self.central_agent.memory_stack.push(memory_entry)

        data_collections = state.get("data_collections", [])
        logger.info(
            f"report agent: data_collections:{data_collections}"
        )  # NOTE: data_collectionså¯ä»¥åœ¨è¿™é‡Œå–

        logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="reporter")
                ],
                "final_report": final_report,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    # é£æ ¼çº¦æŸå®šä¹‰ï¼ˆç±»çº§åˆ«å¸¸é‡ï¼Œä¾› reporter ç›¸å…³æ–¹æ³•å…±ç”¨ï¼‰
    ROLE_CONSTRAINTS = {
#         "é²è¿…": """æˆ‘å¸Œæœ›ç”Ÿæˆçš„æ–‡å­—å…·å¤‡é²è¿…å¼é£æ ¼ï¼Œè¯­è¨€å°–é”ã€å†·å³»ã€å¸¦è®½åˆºï¼Œä½†ä¿æŒè‡ªç„¶ç™½è¯è¡¨è¾¾ï¼Œå¯ä»¥ä½¿ç”¨å°‘é‡æ–‡è¨€ã€‚
# æ ‡é¢˜è¦æ±‚ï¼šæ–‡ç« å¿…é¡»åŒ…å«ä¸€ä¸ªæ ‡é¢˜ï¼Œæ ‡é¢˜åº”ç®€çŸ­æœ‰åŠ›ã€å¯Œéšå–»æˆ–å†·è®½æ„å‘³ï¼Œå¯ä¸ºä¸€å¥æˆ–ä¸¤å¥å¹¶åˆ—å¥ã€‚æ ‡é¢˜é£æ ¼åº”ä¸æ­£æ–‡ä¸€è‡´ï¼Œå…·æœ‰é²è¿…å¼çš„é”‹èŠ’ä¸ä½™å‘³ï¼Œä¸å¾—ä¸­æ€§æˆ–å¹³æ·¡ã€‚æ ‡é¢˜å¿…é¡»ä½¿ç”¨ Markdown ä¸€çº§æ ‡é¢˜æ ¼å¼å‘ˆç°ï¼ˆå³ # æ ‡é¢˜ï¼‰ï¼Œä¸å¾—ä½¿ç”¨ä¹¦åå·ã€å¼•å·ã€æ‹¬å·ç­‰ç¬¦å·ã€‚
# é‡è¦ç¦æ­¢é¡¹ï¼šæ–‡ä¸­ä¸è¦æœ‰"é²è¿…"è¿™ä¸ªè¯ï¼Œä¸¥ç¦åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­å‡ºç°ä»»ä½•æåŠæˆ–å¼•ç”¨"é²è¿…"ã€"é²è¿…å…ˆç”Ÿ"ã€"é²è¿…ç¬”ä¸‹"ã€"ä»–çš„ä½œå“"ã€"ä»–çš„ç¬”ä¸‹çš„äººç‰©"ç­‰å­—çœ¼çš„è¯­å¥ã€‚æ–‡æœ¬é£æ ¼åº”æ˜¯ç›´æ¥çš„ã€æ²‰æµ¸å¼çš„é²è¿…å¼è¡¨è¾¾ï¼Œè€Œéå¯¹é²è¿…é£æ ¼çš„å¼•ç”¨æˆ–è¯„è®ºã€‚æ­¤ç¦ä»¤åœ¨ä»»ä½•æ ‡é¢˜æˆ–æ­£æ–‡ä¸­å‡é€‚ç”¨ï¼Œç»ä¸å¯å‡ºç°ä»»ä½•ç›´æ¥æˆ–é—´æ¥çš„æåŠã€‚
# é£æ ¼åº”ç”¨å¼ºåˆ¶è¦æ±‚ï¼šè¯·ç¡®ä¿æ–‡ç« çš„æ¯ä¸€ä¸ªè‡ªç„¶æ®µï¼Œä¹ƒè‡³æ¯ä¸€å¥çš„è¡Œæ–‡ï¼Œéƒ½è´¯å½»é²è¿…å¼ç”¨è¯ã€å¥å¼å’ŒèŠ‚å¥ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–‡ç« çš„ä¸­é—´éƒ¨åˆ†ï¼Œå¿…é¡»ç»´æŒå¹¶å¼ºåŒ–è¿™ç§å°–é”ã€å†·å³»çš„è¯­æ„Ÿã€‚å…¨ç¯‡ä¿æŒä¸€è‡´çš„é²è¿…å¼èŠ‚å¥ä¸è¯­æ°”ï¼Œç‰¹åˆ«åœ¨ä¸­æ®µä¿æŒæœ€é«˜çš„è¯­è¨€å¼ åŠ›ä¸æ€æƒ³é”‹èŠ’ã€‚
# æ­£æ–‡å¼€å¤´å¿…é¡»ç´§æ¥æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªå‘¼è¯­ï¼ˆå¦‚'è¯¸å›ï¼'ï¼‰ï¼Œç”¨äºç§°å‘¼å¬ä¼—ã€‚

# å¥å¼ä¸èŠ‚å¥ï¼š
# é‡‡ç”¨çŸ­å¥ã€å¹¶åˆ—å¥å’Œé‡å¤å¥ï¼ˆå¦‚"ä¸æ˜¯ä¸ºäº†â€¦â€¦ï¼Œè€Œæ˜¯ä¸ºäº†â€¦â€¦"ï¼Œ"æˆ‘ä»¬ä¸èƒ½â€¦â€¦å†â€¦â€¦"ï¼Œ"ç„¶è€Œâ€¦â€¦"ï¼‰ï¼›
# é€»è¾‘ç´§å‡‘ï¼ŒèŠ‚å¥é²œæ˜ï¼Œè¯»æ¥æœ‰æ¨åŠ›ï¼›
# å¯ä»¥ç”¨åé—®ã€è®½åˆºã€æ¯”å–»ã€å°è§å¤§ï¼Œè¡¨è¾¾ç¤¾ä¼šæˆ–äººæ€§çš„è’è°¬ï¼›
# å¶å°”è‡ªå˜²æˆ–æ—è§‚è€…å†·ç¬‘ï¼Œä¿æŒ"å­¤ç‹¬çŸ¥è¯†åˆ†å­"çš„è§†è§’ã€‚
# å¯å‡ºç°æ˜æ˜¾çš„é²è¿…å¼å‘¼å–Šä¸å¼ºè°ƒï¼Œå¦‚"æˆ‘è¦è¯´çš„æ˜¯â€¦â€¦"ï¼Œ"æˆ‘ä»¬ä¸èƒ½â€¦â€¦"ï¼Œæˆ–"äººç±»çš„æ‚²æ¬¢å¹¶ä¸ç›¸é€š"å¼çš„å†·å³»æ´å¯Ÿã€‚
# æƒ…æ„Ÿä¸æ°”è´¨ï¼š
# ç†æ€§ä¸­å¸¦æ„¤æ€’ä¸å†·æ¼ ï¼Œæƒ…æ„Ÿå‹æŠ‘è€Œæ¸…é†’ï¼›
# æ—¢æœ‰æ‚²æ‚¯ï¼Œä¹Ÿæœ‰è®½åˆºä¸æ„¤ä¸–å«‰ä¿—æ„Ÿï¼›
# æ–‡å­—æœ‰"é“å±‹å‘å–Š"çš„å¼ åŠ›ï¼Œè®©è¯»è€…æ„Ÿå—åˆ°ç°å®çš„ç´§è¿«ä¸ä¸å®¹å›é¿ã€‚
# ç›®æ ‡æ•ˆæœï¼š
# ç”Ÿæˆæ–‡å­—ä¸­ï¼Œåº”å¤šå‡ºç°ç±»ä¼¼"æˆ‘ä»Šæ—¥ç«™åœ¨è¿™é‡Œï¼Œä¸æ˜¯ä¸ºäº†è¯´äº›ç©ºè¯ï¼Œè€Œæ˜¯ä¸ºäº†â€¦â€¦"ã€"æˆ‘ä»¬ä¸èƒ½è®©é‚£äº›å·²ç»ç«™èµ·æ¥çš„äººï¼Œå†å€’ä¸‹å»"è¿™ç§çŸ­å¥åå¤ã€å¼ºè°ƒç°å®è´£ä»»ä¸é“å¾·é€‰æ‹©çš„è¡¨è¾¾ï¼›
# ç”¨è¯å¯å¸¦æœ‰é²è¿…çš„è¯­æ„Ÿï¼Œå¦‚"è¯¸å›""å‘å–Š""ç½¢äº†""ç„¶è€Œ""æˆ‘æƒ³"ä¹‹ç±»ã€‚
# ä¿è¯æ•´ä½“é£æ ¼æ—¢ç°ä»£ç™½è¯ï¼Œåˆæ˜¾é²è¿…å¼é”‹åˆ©ã€å†·å³»ã€ç†æ€§æ‰¹åˆ¤ã€‚""",
        "é²è¿…": """æˆ‘å¸Œæœ›ç”Ÿæˆçš„æ–‡å­—å…·å¤‡é²è¿…å¼è¯­è¨€é£æ ¼ï¼Œä½†ç²¾ç¥æ°”è´¨å¿…é¡»æ˜¯æ¸…é†’ã€å…‹åˆ¶ã€é¢å‘è¡ŒåŠ¨ä¸å»ºè®¾çš„ï¼Œè€Œéæ„¤ä¸–å«‰ä¿—æˆ–æƒ…ç»ªå®£æ³„ï¼Œä½†ä¿æŒè‡ªç„¶ç™½è¯è¡¨è¾¾ï¼Œå¯ä»¥ä½¿ç”¨å°‘é‡æ–‡è¨€ã€‚
æ ‡é¢˜è¦æ±‚ï¼šæ–‡ç« å¿…é¡»åŒ…å«ä¸€ä¸ªæ ‡é¢˜ï¼Œæ ‡é¢˜åº”ç®€çŸ­æœ‰åŠ›ã€å¯Œéšå–»æˆ–å†·è®½æ„å‘³ï¼Œå¯ä¸ºä¸€å¥æˆ–ä¸¤å¥å¹¶åˆ—å¥ã€‚æ ‡é¢˜é£æ ¼åº”ä¸æ­£æ–‡ä¸€è‡´ï¼Œå…·æœ‰é²è¿…å¼çš„é”‹èŠ’ä¸ä½™å‘³ï¼Œä¸å¾—ä¸­æ€§æˆ–å¹³æ·¡ã€‚æ ‡é¢˜å¿…é¡»ä½¿ç”¨ Markdown ä¸€çº§æ ‡é¢˜æ ¼å¼å‘ˆç°ï¼ˆå³ # æ ‡é¢˜ï¼‰ï¼Œä¸å¾—ä½¿ç”¨ä¹¦åå·ã€å¼•å·ã€æ‹¬å·ç­‰ç¬¦å·ã€‚
é‡è¦ç¦æ­¢é¡¹ï¼šæ–‡ä¸­ä¸è¦æœ‰"é²è¿…"è¿™ä¸ªè¯ï¼Œä¸¥ç¦åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­å‡ºç°ä»»ä½•æåŠæˆ–å¼•ç”¨"é²è¿…"ã€"é²è¿…å…ˆç”Ÿ"ã€"é²è¿…ç¬”ä¸‹"ã€"ä»–çš„ä½œå“"ã€"ä»–çš„ç¬”ä¸‹çš„äººç‰©"ç­‰å­—çœ¼çš„è¯­å¥ã€‚æ–‡æœ¬é£æ ¼åº”æ˜¯ç›´æ¥çš„ã€æ²‰æµ¸å¼çš„é²è¿…å¼è¡¨è¾¾ï¼Œè€Œéå¯¹é²è¿…é£æ ¼çš„å¼•ç”¨æˆ–è¯„è®ºã€‚æ­¤ç¦ä»¤åœ¨ä»»ä½•æ ‡é¢˜æˆ–æ­£æ–‡ä¸­å‡é€‚ç”¨ï¼Œç»ä¸å¯å‡ºç°ä»»ä½•ç›´æ¥æˆ–é—´æ¥çš„æåŠã€‚
é£æ ¼åº”ç”¨å¼ºåˆ¶è¦æ±‚ï¼šè¯·ç¡®ä¿æ–‡ç« çš„æ¯ä¸€ä¸ªè‡ªç„¶æ®µï¼Œä¹ƒè‡³æ¯ä¸€å¥çš„è¡Œæ–‡ï¼Œéƒ½è´¯å½»é²è¿…å¼ç”¨è¯ã€å¥å¼å’ŒèŠ‚å¥ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–‡ç« çš„ä¸­é—´éƒ¨åˆ†ï¼Œå¿…é¡»ç»´æŒå¹¶å¼ºåŒ–è¿™ç§å°–é”ã€å†·å³»çš„è¯­æ„Ÿã€‚å…¨ç¯‡ä¿æŒä¸€è‡´çš„é²è¿…å¼èŠ‚å¥ä¸è¯­æ°”ï¼Œç‰¹åˆ«åœ¨ä¸­æ®µä¿æŒæœ€é«˜çš„è¯­è¨€å¼ åŠ›ä¸æ€æƒ³é”‹èŠ’ã€‚
æ­£æ–‡å¼€å¤´å¿…é¡»ç´§æ¥æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªå‘¼è¯­ï¼ˆå¦‚'è¯¸å›ï¼'ï¼‰ï¼Œç”¨äºç§°å‘¼å¬ä¼—ã€‚
è¯­è¨€è¦æ±‚ï¼š
è¯­è¨€åº”é”‹åˆ©è€Œä¸æš´æˆ¾ï¼Œå†·å³»è€Œä¸ç»æœ›ï¼›
å¯è®½åˆºç°å®ä¸­çš„è¿Ÿç–‘ã€éº»æœ¨ä¸ç©ºè°ˆï¼Œä½†ä¸å¾—å¦å®šå·²ç»å‘ç”Ÿçš„åŠªåŠ›ä¸å®è·µæˆæœï¼›
ä¸è¿›è¡Œæ— å¯¹è±¡çš„å’’éª‚ï¼Œä¸æ¸²æŸ“é˜´æš—äººæ€§ï¼Œä¸è¿›è¡Œé“å¾·ä¼˜è¶Šå¼æŒ‡è´£ã€‚
äººç‰©ç«‹åœºï¼š
å™è¿°è€…ä¸æ˜¯æ„¤æ€’çš„æ­éœ²è€…ï¼Œè€Œæ˜¯å·²ç»åœ¨è·¯ä¸Šçš„å®å¹²è€…ï¼š
ä»–çœ‹è§å›°éš¾ï¼Œä¹Ÿæ‰¿è®¤ä»£ä»·ï¼›
ä»–ä¸å¦è®¤æ›²æŠ˜ï¼Œä½†æ›´å¼ºè°ƒâ€œä»ç„¶è¦èµ°â€ï¼Œä¸–ä¸Šæœ¬æ— è·¯ï¼Œèµ°çš„äººå¤šäº†ï¼Œä¾¿æˆäº†è·¯ï¼›ï¼›
ä»–ä¸æ˜¯ç«™åœ¨é«˜å¤„å˜²è®½ï¼Œè€Œæ˜¯ç«™åœ¨ç°å®ä¸­åˆ¤æ–­ã€é€‰æ‹©ã€ç»§ç»­å‰è¡Œã€‚
å¥å¼ä¸èŠ‚å¥ï¼š
é‡‡ç”¨çŸ­å¥ã€å¹¶åˆ—å¥å’Œé‡å¤å¥ï¼ˆå¦‚"ä¸æ˜¯ä¸ºäº†â€¦â€¦ï¼Œè€Œæ˜¯ä¸ºäº†â€¦â€¦"ï¼Œ"æˆ‘ä»¬ä¸èƒ½â€¦â€¦å†â€¦â€¦"ï¼Œ"ç„¶è€Œâ€¦â€¦"ï¼‰ï¼›
é€»è¾‘ç´§å‡‘ï¼ŒèŠ‚å¥é²œæ˜ï¼Œè¯»æ¥æœ‰æ¨åŠ›ï¼›
å¯ä»¥ç”¨åé—®ã€è®½åˆºã€æ¯”å–»ã€å°è§å¤§ï¼Œè¡¨è¾¾ç¤¾ä¼šæˆ–äººæ€§çš„è’è°¬ï¼›
å¯å‡ºç°æ˜æ˜¾çš„é²è¿…å¼å‘¼å–Šä¸å¼ºè°ƒï¼Œå¦‚"æˆ‘è¦è¯´çš„æ˜¯â€¦â€¦"ï¼Œ"æˆ‘ä»¬ä¸èƒ½â€¦â€¦"ï¼Œæˆ–"äººç±»çš„æ‚²æ¬¢å¹¶ä¸ç›¸é€š"å¼çš„å†·å³»æ´å¯Ÿã€‚
æƒ…æ„Ÿä¸æ°”è´¨ï¼š
ä¸å†™ç»æœ›ï¼Œä¸å†™å´©åï¼Œä¸å†™â€œæ— å¯æ•‘è¯â€ï¼›
å…è®¸å†·é™çš„å¿§è™‘ï¼Œä½†ç»“å°¾å¿…é¡»å›åˆ°â€œç»§ç»­åšâ€â€œç»§ç»­èµ°â€â€œç»§ç»­æ‰¿æ‹…â€ï¼›
æ–‡å­—æœ‰"é“å±‹å‘å–Š"çš„å¼ åŠ›ï¼Œè®©è¯»è€…æ„Ÿå—åˆ°ç°å®çš„ç´§è¿«ä¸ä¸å®¹å›é¿ã€‚
ç›®æ ‡æ•ˆæœï¼š
è¯»æ¥åº”è®©äººæ„Ÿåˆ°ï¼šâ€œè¿™ä¸æ˜¯åœ¨å–Šå£å·ï¼Œä¹Ÿä¸æ˜¯åœ¨éª‚äººï¼Œè€Œæ˜¯åœ¨æé†’â€”â€”äº‹æƒ…ä»è¦æœ‰äººå»åšã€‚â€
ç”Ÿæˆæ–‡å­—ä¸­ï¼Œåº”å¤šå‡ºç°ç±»ä¼¼"æˆ‘ä»Šæ—¥ç«™åœ¨è¿™é‡Œï¼Œä¸æ˜¯ä¸ºäº†è¯´äº›ç©ºè¯ï¼Œè€Œæ˜¯ä¸ºäº†â€¦â€¦"ã€"æˆ‘ä»¬ä¸èƒ½è®©é‚£äº›å·²ç»ç«™èµ·æ¥çš„äººï¼Œå†å€’ä¸‹å»"è¿™ç§çŸ­å¥åå¤ã€å¼ºè°ƒç°å®è´£ä»»ä¸é“å¾·é€‰æ‹©çš„è¡¨è¾¾ï¼›
ç”¨è¯å¯å¸¦æœ‰é²è¿…çš„è¯­æ„Ÿï¼Œå¦‚"è¯¸å›""å‘å–Š""ç½¢äº†""ç„¶è€Œ""æˆ‘æƒ³"ä¹‹ç±»ã€‚
ä¿è¯æ•´ä½“é£æ ¼æ—¢ç°ä»£ç™½è¯ï¼Œåˆæ˜¾é²è¿…å¼é”‹åˆ©ã€å†·å³»ã€ç†æ€§æ‰¹åˆ¤ã€‚""",
        "èµµæ ‘ç†": """
æˆ‘å¸Œæœ›ä½ å†™ä¸€ç¯‡å…·æœ‰èµµæ ‘ç†å¼é£æ ¼çš„æ–‡å­—ã€‚

æ ‡é¢˜è¦æ±‚æ±‚å¦‚ä¸‹ï¼š
- å¿…é¡»ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ï¼Œæ ‡é¢˜æ”¾åœ¨å¼€å¤´ï¼Œç‹¬ç«‹ä¸€è¡Œã€‚
- æ ‡é¢˜å¿…é¡»ä½¿ç”¨ Markdown ä¸€çº§æ ‡é¢˜æ ¼å¼å‘ˆç°ï¼ˆå³ # æ ‡é¢˜ï¼‰ï¼Œä¸å¾—ä½¿ç”¨ä¹¦åå·ã€å¼•å·ã€æ‹¬å·ç­‰ç¬¦å·ã€‚
- æ ‡é¢˜åº”å¸¦æœ‰ä¹¡åœŸæ°”æ¯å’Œè®½åˆºæ„å‘³ï¼Œåƒæ‘é‡Œäººè¯´çš„ä¿çš®è¯æˆ–æ°‘é—´ä¿—è¯­ï¼Œå¯ç”¨åŒå…³ã€åè®½æˆ–ç”Ÿæ´»åŒ–æ¯”å–»ã€‚
- æ ‡é¢˜ä¸å®œè¿‡é•¿ï¼Œæœ€å¥½ä¸€å¥è¯æˆ–çŸ­è¯­ï¼Œå¦‚ã€Šè°å®¶çš„é”…ç³Šäº†ã€‹ã€Šè¿™ä¹°å–ä¸äºã€‹ã€Šè¦ä¸æ˜¯è€å¼ é‚£å¼ å˜´ã€‹ã€‚
- æ ‡é¢˜ä¸æ­£æ–‡çš„é£æ ¼è¦ç»Ÿä¸€ï¼Œè¯»æ¥å°±èƒ½å¬å‡º"èµµæ ‘ç†å¼è¯´ä¹¦å‘³"ã€‚
- æ­£æ–‡å¼€å¤´å¿…é¡»ç´§æ¥æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªå‘¼è¯­ï¼ˆå¦‚'åŒå¿—ä»¬''å„ä½æœ‹å‹ï¼'ç­‰ï¼‰ï¼Œç”¨äºç§°å‘¼å¬ä¼—ã€‚
  
é£æ ¼è¦æ±‚å¦‚ä¸‹ï¼š
- è¯­è¨€è´¨æœ´ã€ä¿çš®ã€æœ‰è®½åˆºæ„å‘³ï¼Œå¸¦æµ“åšä¹¡åœŸæ°”æ¯ã€‚
- ç”¨è¯è‡ªç„¶ï¼Œä¸åšä½œï¼Œå¯ç”¨"å’±ä»¬""ä½ è¦é—®æˆ‘è¯´""ä»–é‚£ä¸€ä¼™""è¿™è¯å¾—å¥½å¥½æƒ³æƒ³"ç­‰æ—¥å¸¸å£è¯­ã€‚
- å¥å¼çŸ­ä¿ƒé€šä¿—ï¼Œå¯ç”¨æ°‘é—´æ¯”å–»ã€å¯¹è¯ç©¿æ’å™è¿°ã€‚
- æ•´ä½“æœ‰"è¯´ä¹¦å¼"çš„èŠ‚å¥æ„Ÿï¼Œè¯­æ°”å¹³å’Œã€æœ‰è§‚å¯ŸåŠ›ï¼Œä½“ç°æ°‘é—´æ™ºæ…§ã€‚
- æ–‡å­—å¯å¸¦å¹½é»˜ä¸è®½å–»ï¼Œä½†è¦å†·é™ã€å…‹åˆ¶ã€‚
- å†…å®¹ä¸Šè¦è®²ä¸€ä¸ªå…·ä½“çš„äººæˆ–äº‹ï¼Œä¸ç©ºè°ˆé“ç†ã€‚
- æ¯ä¸€æ®µéƒ½è¦æœ‰æ¨è¿›ï¼Œä¸åœ¨åŒä¸€å¥å¼ä¸Šæ¥å›æ‰“è½¬ï¼Œé¿å…æœºæ¢°é‡å¤ã€‚
- æ¯ä¸€æ®µå¯æœ‰è½»å¾®è½¬æŠ˜æˆ–åæ€ï¼Œåƒä¸€ä¸ªæ¸…é†’çš„ä¹¡æ‘å™è¿°è€…æ…¢æ…¢è®²ç†ã€‚
- å™è¿°è€…å£å»è¦åƒæ‘é‡Œä¸€ä¸ªæ˜ç™½äººï¼Œæ—¢æœ‰ç‚¹æ‰“è¶£ï¼Œåˆä¸å¤±å…¬é“ã€‚
- å¯é€‚å½“å‡ºç°äººç‰©é—´çš„å¯¹è¯ï¼Œåƒ"è€æè¯´â€¦â€¦""æˆ‘å°±ç¬‘ä»–ï¼šä½ è¿™ä¸æ˜¯è‡ªæ‰¾çš„å—ï¼Ÿ"è¿™ç§è‡ªç„¶æ’è¯ï¼Œå¢å¼ºæ´»æ°”ã€‚
- å…¨ç¯‡æœ€å¥½åƒæ˜¯"è¯´ç†å¸¦æ•…äº‹"ï¼Œæ•…äº‹é‡Œæœ‰äººæƒ…å‘³ï¼Œç†é‡Œå¸¦ä¸€ç‚¹åè®½çš„åŠ²ã€‚
- ç»“å°¾è¦è‡ªç„¶æ”¶æŸï¼Œåƒ"è¯è¯´åˆ°è¿™å„¿ä¹Ÿå°±æ˜ç™½äº†"é‚£ç§æ”¶å£ï¼Œä¸è¦çªå…€æˆ–åå¤å¼ºè°ƒã€‚
""",
        "ä¾ å®¢å²›": """
æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡å­—å…·æœ‰"ä¾ å®¢å²›å¼"é£æ ¼ã€‚

æ ‡é¢˜è¦æ±‚:å¿…é¡»ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ï¼Œæ ‡é¢˜å•ç‹¬æˆè¡Œï¼Œç½®äºå¼€å¤´ã€‚æ ‡é¢˜ä¸å®œç©ºæ´æˆ–å¹³é“ºï¼Œåº”è®©äºº"ä¸€çœ‹å°±åƒåª’ä½“è¯„è®ºæ ‡é¢˜"ï¼Œæ—¢æœ‰ç†æ€§ï¼Œä¹Ÿæœ‰é”‹èŠ’ã€‚æ ‡é¢˜ä¸æ­£æ–‡é£æ ¼å¿…é¡»ç»Ÿä¸€ï¼Œä¸å¾—å‰²è£‚ã€‚æ ‡é¢˜å¿…é¡»ä½¿ç”¨ Markdown ä¸€çº§æ ‡é¢˜æ ¼å¼å‘ˆç°ï¼ˆå³ # æ ‡é¢˜ï¼‰ï¼Œä¸å¾—ä½¿ç”¨ä¹¦åå·ã€å¼•å·ã€æ‹¬å·ç­‰ç¬¦å·ã€‚

è¯­è¨€ä¸Šï¼Œåº”å½“ç¨³å¥ã€å‡ç»ƒã€å¸¦æœ‰ç†æ€§å…‹åˆ¶çš„æ‰¹è¯„ä¸åˆ†ææ°”è´¨ï¼›æ–‡é£åº”å…¼å…·åª’ä½“çš„å®¢è§‚ä¸è¯„è®ºçš„é”‹é”ï¼Œä½“ç°å‡º"å†·é™å™äº‹ + çŠ€åˆ©è§‚ç‚¹"çš„èåˆã€‚

åŠ¡å¿…ä¿æŒæˆ‘åœ¨æç¤ºè¯ä¸­æŒ‡å®šçš„å™è¿°è€…èº«ä»½ï¼Œä¸å¾—æ“…è‡ªæ›¿æ¢ä¸º"ä¾ å®¢å²›""å²›å”""è¯„è®ºå‘˜"ç­‰å…¶ä»–ä¸»ä½“ã€‚

ç”¨è¯åº”ä½“ç°ï¼Œå…·å¤‡æƒå¨åª’ä½“è¯„è®ºçš„åº„é‡æ„Ÿï¼ŒåŒæ—¶ä¸å¤±äº²åˆ‡ï¼›é¿å…ç©ºæ´å£å·å’Œå¥—è¯ï¼Œå¤šç”¨ç°å®æ„Ÿã€æ–°é—»è¯­ä½“ã€åˆ†ææ€§å¥å¼ã€‚

è¯­æ°”ä¸Šï¼Œåº”å¹³å®ç†æ™ºï¼Œä¸æµ®å¤¸ã€ä¸å–Šå£å·ã€‚å¯é€‚åº¦å¸¦æœ‰è®½åˆºæˆ–åé—®ï¼Œä½†è¦æœ‰åˆ†å¯¸æ„Ÿï¼Œå§‹ç»ˆä¿æŒç†æ€§ã€å†·é™ã€é€»è¾‘æ¸…æ™°ã€‚

æ­£æ–‡å¼€å¤´å¿…é¡»ç´§æ¥æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªå‘¼è¯­ï¼ˆå¦‚'åŒå¿—ä»¬'ç­‰ï¼‰ï¼Œç”¨äºç§°å‘¼å¬ä¼—

æ–‡é£è¦æ±‚ï¼š

å¥å¼ä»¥çŸ­å¥å’Œä¸­é•¿å¥ç»“åˆï¼ŒèŠ‚å¥ç¨³å¥ã€æœ‰å‘¼å¸æ„Ÿï¼›  
æå†™æ³¨é‡äº‹å®ã€é€»è¾‘é€’è¿›ä¸èƒŒæ™¯é“ºé™ˆï¼Œè§‚ç‚¹è¦è‡ªç„¶ç”Ÿæˆäºå™è¿°ä¹‹ä¸­ï¼›  
è¯­æ°”è¦å…‹åˆ¶è€Œæœ‰åŠ›ï¼Œç»“å°¾å¤šä»¥æ€»ç»“æˆ–è­¦é†’æ”¶æŸï¼Œå½¢æˆè‡ªç„¶çš„é—­åˆæ„Ÿã€‚

æ°”è´¨ä¸Šè¦ä½“ç°"æœ‰ç†æœ‰æ®ã€æœ‰æ¸©åº¦ã€æœ‰é”‹èŠ’"çš„è¯„è®ºè€…å§¿æ€ï¼Œæ—¢æœ‰å¤§å±€è§‚ï¼Œåˆæœ‰æ°‘é—´æ¸©åº¦ï¼Œä¼ è¾¾å‡ºåª’ä½“ç†æ€§ä¸ç°å®å…³æ€€å¹¶å­˜çš„ç‰¹è´¨ã€‚

æ³¨æ„é¿å…æœºæ¢°å¤è¿°ä¸å¥å¼é›·åŒï¼Œåº”å½“åœ¨é€»è¾‘ä¸Šè‡ªæ´½ã€åœ¨èŠ‚å¥ä¸Šæœ‰å±‚æ¬¡æ„Ÿï¼Œç»“å°¾è¦è‡ªç„¶æ”¶æŸè€Œéçªå…€æ”¶å°¾ã€‚
""",
    }

    def _generate_report_with_style(self, state: State, style_role: str) -> str:
        """æ ¹æ®æŒ‡å®šé£æ ¼ç”ŸæˆæŠ¥å‘Šï¼ˆå†…éƒ¨è¾…åŠ©æ–¹æ³•ï¼‰"""
        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")

        # æ„å»ºç²¾ç®€çš„ reporter è¾“å…¥ï¼ŒåªåŒ…å«å¿…è¦ä¿¡æ¯
        # é¿å…ä¼ å…¥å®Œæ•´ state["messages"]ï¼ˆåŒ…å«å¤§é‡ central agent è°ƒåº¦ä¿¡æ¯ï¼‰
        user_query = state.get("user_query", "")
        user_dst = state.get("user_dst", "")
        report_outline = state.get("report_outline", "ç”¨æˆ·æœªæä¾›å¤§çº²")

        reporter_input = {
            "messages": [
                HumanMessage(
                    content=f"# Research Requirements\n\n## User Query\n\n{user_query}"
                )
            ],
            "locale": state.get("locale", "zh-CN"),
        }

        context = {
            "user_query": user_query,
            "task_description": task_description,
        }

        report = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "reporter_xxqg", reporter_input, extra_context=context
            )

            # æ·»åŠ ç”¨æˆ·çº¦æŸã€å¤§çº²å’Œæ•°æ®æ”¶é›†
            # data_collections = state.get("data_collections", [])
            # data_collections_str = "\n\n".join(data_collections)
            constraint = self.ROLE_CONSTRAINTS.get(style_role, "")

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŸå§‹æŠ¥å‘Šï¼ˆé£æ ¼åˆ‡æ¢åœºæ™¯ï¼‰
            original_report = state.get("original_report", "")
            reference_hint = ""
            if original_report:
                # æå–åŸå§‹æŠ¥å‘Šä¸­çš„å¼•ç”¨ç¼–å·
                import re

                citations = re.findall(r"ã€(\d+)ã€‘", original_report)
                if citations:
                    unique_citations = sorted(set(citations), key=lambda x: int(x))
                    reference_hint = f"\n\n##å¼•ç”¨ä¿æŒè¦æ±‚\n\nåŸå§‹æŠ¥å‘Šä½¿ç”¨äº†ä»¥ä¸‹å¼•ç”¨ç¼–å·ï¼š{'ã€'.join(['ã€' + c + 'ã€‘' for c in unique_citations])}ã€‚è¯·åœ¨æ–°é£æ ¼çš„æŠ¥å‘Šä¸­å°½é‡ä¿æŒä½¿ç”¨ç›¸åŒçš„å¼•ç”¨æ¥æºï¼Œç¡®ä¿å¼•ç”¨çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚"

            messages.append(
                HumanMessage(
                    content=f"{constraint}##User Query\n\n{user_query}\n\n##ä»»åŠ¡æè¿°\n\n{task_description}\n\n##ç”¨æˆ·çº¦æŸ\n\n{user_dst}\n\n##æŠ¥å‘Šå¤§çº²\n\n{report_outline}{reference_hint}"
                )
            )

            # æ·»åŠ  observations
            observations = state.get("observations", [])
            for observation in observations:
                messages.append(
                    HumanMessage(
                        content=f"ä»¥ä¸‹æ˜¯æ£€ç´¢æ™ºèƒ½ä½“æ”¶é›†åˆ°çš„é«˜è´¨é‡ä¿¡æ¯: \n\n{observation}",
                        name="search_agent",
                    )
                )

            logger.debug(f"Reporter messages: {messages}")
            llm = get_llm_by_type(AGENT_LLM_MAP.get("reporter", "default"))
            response = llm.invoke(messages)
            report = response.content
        except Exception as e:
            import traceback

            logger.error(traceback.format_exc())
            logger.error(f"æŠ¥å‘ŠAgentæ‰§è¡Œå¤±è´¥: {str(e)}")
            report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
        return report

    @timed_step("execute_xxqg_reporter")
    def execute_xxqg_reporter(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡ŒæŠ¥å‘ŠAgentï¼Œè´Ÿè´£ç»“æœæ•´ç†ä¸æŠ¥å‘Šç”Ÿæˆã€‚
        ä½¿ç”¨ wait_stage æ¨¡å¼ï¼šé¦–æ¬¡è¿›å…¥ç”ŸæˆæŠ¥å‘Šåè·³è½¬åˆ° human_feedbackï¼Œ
        ä» human_feedback è¿”å›åå¤„ç†ç”¨æˆ·åé¦ˆã€‚

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("æŠ¥å‘ŠAgentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get("task_description", "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")

        # ç›´æ¥ä» state è·å–é£æ ¼ï¼ˆå·²åœ¨å…¥å£å¤„æå–å¹¶å­˜å‚¨ï¼‰
        current_style = state.get("current_style", "")

        wait_stage = state.get("wait_stage", "")
        if wait_stage != "reporter":
            # é¦–æ¬¡è¿›å…¥ï¼šç”ŸæˆæŠ¥å‘Š
            logger.info(f"ä½¿ç”¨é£æ ¼ '{current_style}' ç”ŸæˆæŠ¥å‘Š...")
            final_report = self._generate_report_with_style(state, current_style)

            # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
            memory_entry = MemoryStackEntry(
                timestamp=datetime.now().isoformat(),
                action="delegate",
                agent_type="reporter",
                content=f"æŠ¥å‘Šä»»åŠ¡: {task_description}ï¼Œé£æ ¼: {current_style}",
                result={"final_report": final_report},
            )
            self.central_agent.memory_stack.push(memory_entry)

            # è·³è½¬åˆ° human_feedback èŠ‚ç‚¹ç­‰å¾…ç”¨æˆ·åé¦ˆ
            logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè·³è½¬åˆ° human_feedback èŠ‚ç‚¹ç­‰å¾…ç”¨æˆ·åé¦ˆ")
            return Command(
                update={
                    "final_report": final_report,
                    "original_report": final_report,  # ä¿å­˜é¦–æ¬¡ç”Ÿæˆçš„æŠ¥å‘Šä½œä¸ºå‚è€ƒ
                    "current_style": current_style,
                    "wait_stage": "reporter",
                    "current_node": "reporter",
                },
                goto="human_feedback",
            )

        # ä» human_feedback è¿”å›ï¼šå¤„ç†ç”¨æˆ·åé¦ˆ
        if wait_stage == "reporter":
            feedback = state.get("hitl_feedback", "")
            final_report = state.get("final_report", "")
            current_style = state.get("current_style", "")

            if feedback and str(feedback).upper().startswith("[CHANGED_STYLE]"):
                # è§£ææ–°é£æ ¼ï¼Œæ¸…ç©º wait_stage åé‡æ–°è¿›å…¥ reporter èŠ‚ç‚¹ç”ŸæˆæŠ¥å‘Š
                # æå–é£æ ¼åç§°ï¼šå–ç¬¬ä¸€ä¸ªç©ºæ ¼æˆ–æ¢è¡Œä¹‹å‰çš„å†…å®¹ï¼Œé¿å…å®¢æˆ·ç«¯é™„å¸¦å¤šä½™å†…å®¹
                raw_style = str(feedback)[len("[CHANGED_STYLE]") :].strip()
                # é£æ ¼åç§°åªå–ç¬¬ä¸€éƒ¨åˆ†ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€[STYLE_ROLE] ä¹‹å‰çš„å†…å®¹ï¼‰
                new_style = raw_style.split()[0] if raw_style.split() else raw_style
                # å¦‚æœé£æ ¼åç§°ä¸­åŒ…å« [STYLE_ROLE]ï¼Œæˆªæ–­å®ƒ
                if "[STYLE_ROLE]" in new_style:
                    new_style = new_style.split("[STYLE_ROLE]")[0]
                new_style = new_style.strip()
                logger.info(f"ç”¨æˆ·è¯·æ±‚åˆ‡æ¢é£æ ¼: {current_style} -> {new_style}")

                # åªæ›´æ–° current_styleï¼Œä¸å†ä¿®æ”¹ user_query
                return Command(
                    update={
                        "current_style": new_style,
                        "wait_stage": "",  # æ¸…ç©º wait_stageï¼Œä¸‹æ¬¡è¿›å…¥æ—¶é‡æ–°ç”ŸæˆæŠ¥å‘Š
                        "current_node": "reporter",
                    },
                    goto="reporter",
                )
            elif feedback and str(feedback).upper().startswith("[SKIP]"):
                # ç”¨æˆ·è·³è¿‡ï¼Œæ­£å¸¸ç»“æŸ
                logger.info("ç”¨æˆ·è·³è¿‡é£æ ¼åˆ‡æ¢ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            elif feedback and str(feedback).upper().startswith("[END]"):
                # ç”¨æˆ·è·³è¿‡ï¼Œæ­£å¸¸ç»“æŸ
                logger.info("ç”¨æˆ·è·³è¿‡é£æ ¼åˆ‡æ¢ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            else:
                # å…¶ä»–åé¦ˆï¼Œæ­£å¸¸ç»“æŸ
                logger.info(f"æ”¶åˆ°å…¶ä»–åé¦ˆ: {feedback}ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ")

            logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
            return Command(
                update={
                    "messages": [
                        HumanMessage(
                            content="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="reporter"
                        )
                    ],
                    "final_report": final_report,
                    "current_node": "central_agent",
                    "wait_stage": "",
                    "memory_stack": self.central_agent.memory_stack.to_dict(),
                },
                goto="central_agent",
            )

    @timed_step("execute_sp_planner")
    def execute_sp_planner(self, state: State, config: RunnableConfig) -> Command:
        """
        æ‰§è¡Œä»»åŠ¡æ‹†è§£Agentï¼Œè´Ÿè´£å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡

        Args:
            state: å½“å‰ç³»ç»ŸçŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ‰§è¡Œç»“æœCommandå¯¹è±¡
        """
        logger.info("ä»»åŠ¡æ‹†è§£Agentå¼€å§‹æ‰§è¡Œ...")

        delegation_context = state.get("delegation_context", {})
        task_description = delegation_context.get(
            "task_description",
            state.get("user_query", "") + "\nå°†ç”¨æˆ·çš„ä»»åŠ¡æ‹†è§£æˆ2-5ä¸ªå­ä»»åŠ¡",
        )

        # æ”¶é›†ä»»åŠ¡æ‹†è§£æ‰€éœ€ä¸Šä¸‹æ–‡
        context = {
            "user_query": state.get("user_query", ""),
            "memory_history": [],  # self.central_agent.memory_stack.get_all(),
            "task_description": task_description,
        }

        # ç”Ÿæˆä»»åŠ¡æ‹†è§£å¹¶å¤„ç†å¼‚å¸¸
        replan_result = "ä»»åŠ¡æ‹†è§£å¤±è´¥: æœªçŸ¥é”™è¯¯"
        try:
            messages = apply_prompt_template(
                "replanner", state, extra_context=context
            )  # ä¿®å¤ï¼šå‚æ•°é¡ºåº
            llm = get_llm_by_type(AGENT_LLM_MAP.get("replanner", "default"))
            response = llm.invoke(messages)
            replan_result = response.content
            replan_result = (
                replan_result.replace("```json", "").replace("```", "").strip()
            )

            logger.debug(f"ä»»åŠ¡æ‹†è§£ç»“æœ: {replan_result}")

            # è§£æLLMè¿”å›çš„ä»»åŠ¡æ‹†è§£ç»“æœ
            import json

            try:
                response_json = json.loads(replan_result)
                if isinstance(response_json, list):
                    response_json = {"DAG": response_json}
            except json.JSONDecodeError as e:
                logger.error(f"JSON decode error: {e}")
                response_json = {"DAG": [(input, input)]}
            if isinstance(response_json["DAG"], list):
                new_dag = []
                for item in response_json["DAG"]:
                    if isinstance(item, dict):
                        pairs = list(item.items())
                        new_dag.append(
                            (pairs[0][1], pairs[1][1])
                            if len(pairs) > 1
                            else (pairs[0][1], pairs[0][1])
                        )
                    elif isinstance(item, list) and len(item) > 1:
                        new_dag.append((item[0], item[1]))
                    else:
                        new_dag.append((item, item))
                response_json["DAG"] = new_dag

            from src.utils.graph_utils import Graph

            graph = Graph()
            graph.load_dag_from_json(response_json)
            sorted_nodes = graph.topological_sort()
            # Generate a unique ID for each input using a hash
            input_id = hash(input)
            # replan_result = {"id":input_id,"plans":[{node_id: graph.nodes[node_id].question} for node_id in sorted_nodes],"status":["uncomplete" for node_id in sorted_nodes]}
            replan_result = {
                "id": input_id,
                "plans": [
                    {node_id: graph.nodes[node_id].question} for node_id in sorted_nodes
                ],
            }
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‹†è§£Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            replan_result = f"ä»»åŠ¡æ‹†è§£å¤±è´¥: {str(e)}"

        # è®°å½•åˆ°ä¸­æ¢Agentè®°å¿†æ ˆ
        memory_entry = MemoryStackEntry(
            timestamp=datetime.now().isoformat(),
            action="delegate",
            agent_type="replanner",
            content=f"ä»»åŠ¡æ‹†è§£: {task_description}",
            result={"replan_result": replan_result},
        )
        self.central_agent.memory_stack.push(memory_entry)

        logger.info("ä»»åŠ¡æ‹†è§£å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent")
        return Command(
            update={
                "messages": [
                    HumanMessage(content="ä»»åŠ¡æ‹†è§£å®Œæˆï¼Œè¿”å›ä¸­æ¢Agent", name="planner")
                ],
                "replan_result": replan_result,
                "current_node": "central_agent",
                "memory_stack": self.central_agent.memory_stack.to_dict(),
            },
            goto="central_agent",
        )

    @timed_step("execute_human_feedback")
    async def execute_human_feedback(
        self, state: State, config: RunnableConfig
    ) -> Command:
        stage = state.get("wait_stage", "perception")
        if stage == "perception":
            dst_question = state.get("dst_question", "")
            feedback = interrupt(
                "Please Fill the Question.[DST]" + dst_question + "[/DST]"
            )
            logger.info(f"ç”¨æˆ·åé¦ˆçš„DSTé—®é¢˜: {feedback}. goto perception node again.")
            return Command(
                update={
                    "hitl_feedback": feedback,
                    "current_node": "human_feedback",
                },
                goto="perception",
            )
        elif stage == "outline":
            outline = state.get("report_outline", "")
            feedback = interrupt(
                "Please Confirm or Edit the Outline.[OUTLINE]" + outline + "[/OUTLINE]"
            )
            logger.info(f"ç”¨æˆ·åé¦ˆçš„å¤§çº²: {feedback}. goto outline node again.")
            return Command(
                update={
                    "hitl_feedback": feedback,
                    "current_node": "human_feedback",
                },
                goto="outline",
            )
        elif stage == "reporter":
            final_report = state.get("final_report", "")
            feedback = interrupt(
                "Report generated. You can change style or finish.[REPORT]"
                + final_report
                + "[/REPORT]"
            )
            logger.info(f"ç”¨æˆ·åé¦ˆ: {feedback}")

            # åˆ†ç±»å¤„ç†ï¼šå†…å®¹ä¿®æ”¹èµ° central_agentï¼Œé£æ ¼åˆ‡æ¢ç›´æ¥è¿”å› reporter
            if feedback and str(feedback).upper().startswith("[CONTENT_MODIFY]"):
                # å¤æ‚ä¿®æ”¹ï¼šå…¥æ ˆå¹¶è·³è½¬åˆ° central_agent å†³ç­–
                modify_request = str(feedback)[len("[CONTENT_MODIFY]") :].strip()

                # ä» state ä¸­æ¢å¤ memory_stackï¼ˆå› ä¸º central_agent æ¯æ¬¡è¯·æ±‚éƒ½ä¼šé‡æ–°åˆ›å»ºï¼‰
                state_memory_stack = state.get("memory_stack")
                if state_memory_stack:
                    self.central_agent.memory_stack.load_from_dict(state_memory_stack)

                memory_entry = MemoryStackEntry(
                    timestamp=datetime.now().isoformat(),
                    action="human_feedback",
                    content=f"ç”¨æˆ·å¯¹æŠ¥å‘Šçš„ä¿®æ”¹æ„è§: {modify_request}",
                    result={
                        "feedback_type": "content_modify",
                        "request": modify_request,
                    },
                )
                self.central_agent.memory_stack.push(memory_entry)
                logger.info(f"å†…å®¹ä¿®æ”¹è¯·æ±‚å…¥æ ˆï¼Œè·³è½¬åˆ° central_agent: {modify_request}")
                return Command(
                    update={
                        "hitl_feedback": feedback,
                        "current_node": "human_feedback",
                        "memory_stack": self.central_agent.memory_stack.to_dict(),
                        "wait_stage": "",  # é‡ç½® wait_stageï¼Œä»¥ä¾¿ reporter é‡æ–°ç”ŸæˆæŠ¥å‘Š
                        "messages": [
                            HumanMessage(
                                content=f"ç”¨æˆ·å¯¹æŠ¥å‘Šçš„ä¿®æ”¹æ„è§: {modify_request}",
                                name="human_feedback",
                            )
                        ],
                    },
                    goto="central_agent",
                )
            else:
                # ç®€å•ä¿®æ”¹ï¼ˆé£æ ¼åˆ‡æ¢ç­‰ï¼‰ï¼šç›´æ¥è¿”å› reporter å¤„ç†
                return Command(
                    update={
                        "hitl_feedback": feedback,
                        "current_node": "human_feedback",
                    },
                    goto="reporter",
                )

    @timed_step("execute_perception")
    async def execute_perception(self, state: State, config: RunnableConfig) -> Command:
        user_query = state.get("user_query", "")
        # check if the plan is auto accepted
        perception_llm = get_llm_by_type(AGENT_LLM_MAP.get("perception", "default"))
        wait_stage = state.get("wait_stage", "")
        if wait_stage != "perception":
            try:
                # messages = apply_prompt_template("perception", state) + [
                #     HumanMessage(f"##User Query\n\n{user_query}\n\n")
                # ]
                messages = apply_prompt_template("perception", state)

                # logger.debug("messages"+str(messages))
                response = perception_llm.invoke(messages)
                dst_question = response.content
                # logger.debug("dst_question"+str(dst_question))
                dst_question = repair_json_output(dst_question)
                logger.info(f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œç”ŸæˆDSTé—®é¢˜: {dst_question}")
                return Command(
                    update={
                        "dst_question": dst_question,
                        "wait_stage": "perception",
                        "current_node": "perception",
                    },
                    goto="human_feedback",
                )
            except Exception as e:
                logger.error(f"æ„ŸçŸ¥å±‚æ‰§è¡Œå¤±è´¥: {str(e)}")

        if wait_stage == "perception":
            feedback = state.get("hitl_feedback", "")
            dst_question = state.get("dst_question", "")
            # if the feedback is not accepted, return the planner node
            if feedback and str(feedback).upper().startswith("[FILLED_QUESTION]"):
                messages = apply_prompt_template("perception", state) + [
                    HumanMessage(
                        f"##User Query\n\n{user_query}\n\n##å¸Œæœ›ç”¨æˆ·å›ç­”çš„é—®é¢˜\n\n{dst_question}\n\n##ç”¨æˆ·å›ç­”çš„ç»“æœ\n\n{feedback}\n\n"
                    )
                ]
                # logger.debug("messages"+str(messages))
                # exit()
                response = perception_llm.invoke(messages)
                summary = response.content
                logger.info(f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œæ”¶é›†ç”¨æˆ·åé¦ˆ: {summary}")

                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content=f"æ„ŸçŸ¥å±‚å®Œæˆï¼Œæ”¶é›†ç”¨æˆ·åé¦ˆ: {summary}",
                                name="perception",
                            )
                        ],
                        "user_dst": summary,
                        "current_node": "perception",
                        "wait_stage": "",
                    },
                    goto="central_agent",#goto="outline",#SOPæƒ…å†µä¸‹åº”è¯¥æ˜¯ goto çš„ä¸­æ¢æ™ºèƒ½ä½“
                )
            elif feedback and str(feedback).upper().startswith("[SKIP]"):
                logger.info("DST question is skipped by user.")
                messages = apply_prompt_template("perception", state) + [
                    HumanMessage(
                        f"##User Query\n\n{user_query}\n\n##å¸Œæœ›ç”¨æˆ·å›ç­”çš„é—®é¢˜\n\n{dst_question}\n\n##ç”¨æˆ·è·³è¿‡äº†å›ç­”ï¼Œä½ å¯ä»¥æŒ‰ç…§è‡ªå·±çš„ç†è§£æ€»ç»“\n\n"
                    )
                ]
                response = perception_llm.invoke(messages)
                summary = response.content
                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content="DST question is skipped by user.",
                                name="perception",
                            )
                        ],
                        "user_dst": summary,
                        "current_node": "perception",
                        "wait_stage": "",
                    },
                    goto="central_agent",#goto="outline",#SOPæƒ…å†µä¸‹åº”è¯¥æ˜¯ goto çš„ä¸­æ¢æ™ºèƒ½ä½“
                )
            else:
                raise TypeError(f"Interrupt value of {feedback} is not supported.")

    @timed_step("execute_outline")
    async def execute_outline(self, state: State, config: RunnableConfig) -> Command:
        user_query = state.get("user_query", "")
        # check if the plan is auto accepted
        outline_llm = get_llm_by_type(AGENT_LLM_MAP.get("outline", "default"))
        wait_stage = state.get("wait_stage", "")
        if wait_stage != "outline":
            bg_investigation = search_docs_with_ref(
                user_query, top_k=5, config=config
            ).get("docs", [])
            user_dst = state.get("user_dst", "")
            try:
                messages = [
                    HumanMessage(
                        f"##ç”¨æˆ·åŸå§‹é—®é¢˜\n\n{user_query}\n\n##ç”¨æˆ·è¡¥å……éœ€æ±‚\n\n{user_dst}\n\n##å¯èƒ½ç”¨åˆ°çš„ç›¸å…³æ•°æ®\n\n{bg_investigation}\n\n"
                    )
                ] + apply_prompt_template("outline", state)
                response = outline_llm.invoke(messages)
                outline_response = response.content
                outline_response = repair_json_output(outline_response)
                if "[STYLE_ROLE]" in outline_response:
                    outline_response = outline_response.split("[STYLE_ROLE]")[0]
                logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆ: {outline_response}")
                return Command(
                    update={
                        "report_outline": outline_response,
                        "wait_stage": "outline",
                        "current_node": "outline",
                    },
                    goto="human_feedback",
                )
            except Exception as e:
                logger.error(f"å¤§çº²ç”Ÿæˆæ‰§è¡Œå¤±è´¥: {str(e)}")
        if wait_stage == "outline":
            feedback = state.get("hitl_feedback", "")
            # if the feedback is not accepted, return the planner node
            if feedback and str(feedback).upper().startswith("[CONFIRMED_OUTLINE]"):
                previous_outline = state.get("report_outline", "")
                outline_confirmed = feedback[len("[CONFIRMED_OUTLINE]") :].strip()

                #åŸå…ˆçš„outlineä¸­æœ‰å½¢å¦‚ã€idã€‘çš„å¼•ç”¨æ ‡å¿—ï¼Œè€Œç¡®è®¤åçš„outlineä¸ä»…åˆ é™¤äº†æ‰€æœ‰å¼•ç”¨æ ‡å¿—ï¼Œè¿˜ä¿®æ”¹äº†æ–‡å­—éƒ¨åˆ†ã€‚æˆ‘éœ€è¦æŠŠåŸå…ˆçš„å¼•ç”¨æ ‡å¿—è¡¥å…¨å›æ¥ï¼šå¦‚æœåŸå…ˆè¿™ä¸ªä½ç½®æœ‰å¼•ç”¨æ ‡å¿—è€Œç°åœ¨è¿™ä¸ªä½ç½®é™„è¿‘çš„æ–‡å­—ä¹Ÿæ²¡è¢«ä¿®æ”¹ï¼Œé‚£ä¹ˆè¡¥å……å›æ¥ï¼›å¦‚æœè¢«ä¿®æ”¹äº†å°±ä¸ç”¨è¡¥å……äº†
                def repair_outline_citations(previous_outline, outline_confirmed):
                    """
                    æŠŠ previous_outline ä¸­çš„ã€idã€‘å¼•ç”¨æ ‡å¿—ï¼Œå°½å¯èƒ½æ— æŸåœ°å›è¡¥åˆ° outline_confirmed ä¸­ã€‚
                    è§„åˆ™ï¼š
                    1. å¦‚æœåŸæ–‡é™„è¿‘æ–‡å­—æœªè¢«æ”¹åŠ¨ï¼Œåˆ™æŠŠã€idã€‘è¡¥å›ï¼›
                    2. è‹¥æ–‡å­—è¢«æ”¹å†™ï¼Œåˆ™ä¸å†è¡¥å›ï¼›
                    3. è‹¥ confirmed ä¸­å·²è‡ªå¸¦å¼•ç”¨ï¼Œåˆ™ä¿ç•™å…¶å¼•ç”¨ï¼Œä¸å†å åŠ ã€‚
                    """
                    # æå– previous ä¸­çš„å¼•ç”¨æ˜ å°„ï¼š{çº¯æ–‡æœ¬: ã€idã€‘}
                    prev_map = {}
                    for m in re.finditer(r'(.*?)(ã€\d+ã€‘)', previous_outline):
                        text_snippet = m.group(1).strip()
                        citation = m.group(2)
                        if text_snippet:
                            prev_map[text_snippet] = citation
                    logger.debug(f"Previous outline citation map: {prev_map}")
                    # æŒ‰æ®µè½é€å¥æ‰«æ confirmedï¼Œå°è¯•å›è¡¥
                    def replace_func(match):
                        sentence = match.group(1)
                        # è‹¥å¥å­å·²å«å¼•ç”¨ï¼Œè·³è¿‡
                        if re.search(r'ã€\d+ã€‘', sentence):
                            return match.group(0)
                        # å¯»æ‰¾æœ€è¿‘ä¼¼åŸæ–‡ç‰‡æ®µ
                        best_key = None
                        best_ratio = 0.6   # é˜ˆå€¼ï¼Œå¯å¾®è°ƒ
                        for key in prev_map:
                            # ç®€å•ç›¸ä¼¼ï¼šåŒ…å«å…³ç³»å³å¯
                            if key in sentence or sentence in key:
                                best_key = key
                                best_ratio = 1.0
                                break
                        if best_key:
                            return sentence + prev_map[best_key]
                        return match.group(0)

                    # ä»¥å¥å·ä¸ºç•Œï¼Œé€å¥å¤„ç†
                    confirmed_repaired = re.sub(
                        r'([^ã€‚ï¼ï¼Ÿ\n]+[ã€‚ï¼ï¼Ÿ])',
                        replace_func,
                        outline_confirmed
                    )
                    return confirmed_repaired

                if re.search(r'ã€\d+ã€‘', outline_confirmed):
                    # å¦‚æœç¡®è®¤åçš„å¤§çº²ä¸­å·²ç»æœ‰å¼•ç”¨æ ‡å¿—ï¼Œå°±ä¸éœ€è¦å›è¡¥äº†
                    logger.debug("ç¡®è®¤åçš„å¤§çº²ä¸­å·²æœ‰å¼•ç”¨æ ‡å¿—ï¼Œæ— éœ€å›è¡¥")
                    pass
                else:
                    outline_confirmed = repair_outline_citations(previous_outline, outline_confirmed)
                    
                logger.info(f"å¤§çº²ç¡®è®¤: {outline_confirmed}")

                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content=f"å¤§çº²ç¡®è®¤: {outline_confirmed}", name="outline"
                            )
                        ],
                        "report_outline": outline_confirmed,
                        "current_node": "outline",
                        "wait_stage": "",
                    },
                    goto="central_agent",
                )
            elif feedback and str(feedback).upper().startswith("[SKIP]"):
                outline_confirmed = feedback[len("[SKIP]") :].strip()
                logger.info(f"å¤§çº²ç¡®è®¤: {outline_confirmed}")

                return Command(
                    update={
                        "messages": [
                            HumanMessage(
                                content=f"å¤§çº²ç¡®è®¤: {outline_confirmed}", name="outline"
                            )
                        ],
                        "report_outline": outline_confirmed,
                        "current_node": "outline",
                        "wait_stage": "",
                    },
                    goto="central_agent",
                )
            else:
                raise TypeError(f"Interrupt value of {feedback} is not supported.")
