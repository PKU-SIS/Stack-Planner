"""
Batch-MAB: æ‰¹é‡-ä¿¡æ¯è§…é£Ÿå¤šè‡‚è€è™æœºä¸»ç®—æ³•

å®ç°äº† Stage 1 çš„æ ¸å¿ƒç®—æ³•ï¼šåŠ¨æ€å¤§çº²ç”Ÿæˆä¸ä¼˜åŒ–ã€‚
"""

import math
import numpy as np
from typing import List, Tuple, Optional, Callable
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.utils.logger import logger
from .outline_node import OutlineNode
from .document import FactStructDocument
from .memory import Memory
from .embedder import Embedder
from .reward_calculator import RewardCalculator
from .llm_wrapper import FactStructLLMWrapper
from langchain_core.runnables import RunnableConfig
from ..graph.types import State
import re
import json
import traceback
class BatchMAB:
    """
    æ‰¹é‡-ä¿¡æ¯è§…é£Ÿå¤šè‡‚è€è™æœºï¼ˆBatch-IF-MABï¼‰ç®—æ³•

    æ ¸å¿ƒæ€æƒ³ï¼š
        1. ä½¿ç”¨ UCB1 ç­–ç•¥é€‰æ‹© Top-K ä¸ªæœ€å€¼å¾—æ¢ç´¢çš„å¶å­èŠ‚ç‚¹
        2. æ‰¹é‡ç”ŸæˆæŸ¥è¯¢ã€æ‰¹é‡æ£€ç´¢ã€æ‰¹é‡è®¡ç®—å¥–åŠ±
        3. æ‰¹é‡ä¿®çº²ï¼ˆå•æ¬¡ LLM è°ƒç”¨ï¼‰

    ä¼˜åŠ¿ï¼š
        - ç›¸æ¯”åŸ IF-MAB æ–¹æ¡ˆï¼Œå¤§å¹…é™ä½ LLM è°ƒç”¨æ¬¡æ•°
        - ä¿æŒè‡ªé€‚åº”æ¢ç´¢ç­–ç•¥
    """

    def __init__(
        self,
        llm_wrapper: FactStructLLMWrapper,
        embedder: Embedder,
        search_engine: Callable[[str, int], List[FactStructDocument]],
        reward_calculator: Optional[RewardCalculator] = None,
        memory: Optional[Memory] = None,
        max_iterations: int = 20,
        batch_size: int = 5,
        reward_weights: Tuple[float, float] = (0.7, 0.3),  # (w_rel, w_nov)
    ):
        """
        åˆå§‹åŒ– Batch-MAB

        å‚æ•°:
            llm_wrapper: LLM æ–¹æ³•åŒ…è£…å™¨
            embedder: æ–‡æ¡£åµŒå…¥ç”Ÿæˆå™¨
            search_engine: æœç´¢å¼•æ“å‡½æ•°ï¼Œç­¾å (query: str, k: int) -> List[FactStructDocument]
            reward_calculator: å¥–åŠ±è®¡ç®—å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            memory: è®°å¿†æ¨¡å—ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•° T_maxï¼ˆé»˜è®¤ 20ï¼‰
            batch_size: æ‰¹é‡å¤§å° Kï¼ˆé»˜è®¤ 5ï¼‰
            reward_weights: å¥–åŠ±æƒé‡ (w_rel, w_nov)ï¼ˆé»˜è®¤ (0.7, 0.3)ï¼‰
        """
        self.llm_wrapper = llm_wrapper
        self.embedder = embedder
        self.search_engine = search_engine
        self.max_iterations = max_iterations
        self.batch_size = batch_size

        # åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨
        if reward_calculator is None:
            self.reward_calculator = RewardCalculator(
                w_rel=reward_weights[0],
                w_nov=reward_weights[1],
            )
        else:
            self.reward_calculator = reward_calculator

        # åˆå§‹åŒ–è®°å¿†æ¨¡å—
        if memory is None:
            self.memory = Memory(embedding_dim=embedder.get_embedding_dim())
        else:
            self.memory = memory

        logger.info(
            f"Initialized BatchMAB: max_iterations={max_iterations}, "
            f"batch_size={batch_size}, embedding_dim={embedder.get_embedding_dim()}"
        )

    def run(
        self,
        initial_query: str,
        initial_docs: Optional[List[FactStructDocument]] = None,
        central_guidance=None,
        replan_result=None,
        factstruct_outline=None,
        factstruct_memory=None,
        config:RunnableConfig=None,
    ) -> Tuple[OutlineNode, Memory]:
        
        # """
        # è¿è¡Œ Batch-MAB ç®—æ³•
        # å‚æ•°:
        #     initial_query: åˆå§‹æŸ¥è¯¢
        #     initial_docs: åˆå§‹æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨æ£€ç´¢ï¼‰
        # è¿”å›:
        #     (outline_root, memory): æœ€ç»ˆå¤§çº²æ ¹èŠ‚ç‚¹å’Œè®°å¿†æ¨¡å—
        # """
        
        logger.info(f"Starting Batch-MAB with query: {initial_query}")

        #åˆå§‹åŒ–
        if factstruct_outline==None:
            # --- åˆå§‹åŒ–é˜¶æ®µ ---
            # 1. åˆå§‹æ£€ç´¢ä¸å¤§çº²ç”Ÿæˆ
            if initial_docs is None:
                logger.info("Performing initial search...")
                initial_docs = self.search_engine(initial_query, k=5,config=config)

            # åµŒå…¥åˆå§‹æ–‡æ¡£
            initial_docs_with_embed = self.embedder.embed_docs(initial_docs)

            # å­˜å‚¨åˆ°è®°å¿†åº“
            self.memory.store_docs(initial_docs_with_embed)

            # ç”Ÿæˆåˆå§‹å¤§çº²ï¼ˆLLM Call #1ï¼‰
            logger.info("Generating initial outline...")
            
            outline_root = self.llm_wrapper.generate_initial_outline(
                initial_query,
                initial_docs_with_embed,
                central_guidance,#æ„Ÿè§‰plan_textå°±æ˜¯ç¬¬ä¸€æ¬¡çš„ feedback
                replan_result=replan_result,
            )
            logger.info(f"outline_root:{outline_root}")
            logger.info(f"outline_root.id:{outline_root.id}")
            # å°†åˆå§‹æ–‡æ¡£æ˜ å°„åˆ°æ ¹èŠ‚ç‚¹
            self.memory.map_node_to_docs(outline_root.id, initial_docs_with_embed)
            logger.info(f"self.memory:{self.memory}")
            # æ‰“å°åˆå§‹å¤§çº²
            try:
                from .integration import outline_node_to_markdown

                initial_outline_markdown = outline_node_to_markdown(
                    outline_root, max_depth=None, include_root=True
                )
                logger.info(f"Initial outline:\n{initial_outline_markdown}")
            except Exception as e:
                logger.warning(f"Failed to format initial outline for logging: {e}")
                logger.info(f"Initial outline:\n{outline_root.to_text_tree()}")

        # æ€»è¿­ä»£æ¬¡æ•°è®¡æ•°å™¨
        t = 0

        # --- è¿­ä»£å¾ªç¯ï¼ˆBatch-MAB è¿‡ç¨‹ï¼‰---
        num_rounds = math.ceil(self.max_iterations / self.batch_size)
        logger.info(
            f"Starting {num_rounds} rounds of batch optimization "
            f"(max_iterations={self.max_iterations}, batch_size={self.batch_size}, "
            f"total iterations={self.max_iterations})"
        )

        for round_num in range(num_rounds):
            logger.info(f"--- Round {round_num + 1}/{num_rounds} ---")

            # 1. è·å–å½“å‰æ‰€æœ‰å¯è¡ŒåŠ¨çš„"æ‰‹è‡‚"ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰
            current_leaf_nodes = outline_root.get_leaf_nodes()
            logger.info(f"current_leaf_nodes{current_leaf_nodes}")
            if not current_leaf_nodes:
                logger.info("No leaf nodes available, terminating early.")
                break

            logger.info(f"Found {len(current_leaf_nodes)} leaf nodes")

            # 2. UCB ç­–ç•¥é€‰æ‹© Top-K æ‰‹è‡‚
            selected_nodes = self._select_top_k_nodes(current_leaf_nodes, t)
            logger.info(f"selected_nodes{selected_nodes}")
            if not selected_nodes:
                logger.info("No nodes selected, terminating.")
                break

            logger.info(
                f"Selected {len(selected_nodes)} nodes: "
                f"{[node.title for node in selected_nodes]}"
            )

            # 3. "æ‰¹é‡æ‹‰åŠ¨æ‘‡è‡‚"ï¼ˆæ‰§è¡Œæ£€ç´¢ï¼‰
            # (LLM Call #Round*2)
            logger.info("Batch generating queries...")
            queries = self.llm_wrapper.batch_generate_queries(selected_nodes)

            # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢ï¼ˆæŒ‰ç…§ proposal è¦æ±‚å®ç°çœŸæ­£çš„å¹¶è¡Œæ£€ç´¢ï¼‰
            logger.info(f"Performing parallel search for {len(queries)} queries...")
            new_docs_list = self._parallel_search(queries, k=3,config=config)

            # é¢„å¤„ç†æ–°æ–‡æ¡£ï¼ˆåµŒå…¥ï¼‰
            new_docs_list_with_embed = []
            for docs in new_docs_list:
                docs_with_embed = self.embedder.embed_docs(docs)
                new_docs_list_with_embed.append(docs_with_embed)

            # 4. æ‰¹é‡è®¡ç®—å¹¶è®°å½•"å¥–åŠ±"
            all_embeddings = self.memory.get_all_doc_embeddings()
            node_doc_pairs_for_refine = []

            for i, node in enumerate(selected_nodes):
                t += 1  # å¢åŠ å…¨å±€è¿­ä»£è®¡æ•°å™¨
                new_docs = new_docs_list_with_embed[i]

                # è®¡ç®—å¥–åŠ±
                # ç”ŸæˆèŠ‚ç‚¹åµŒå…¥ï¼šä½¿ç”¨èŠ‚ç‚¹æ ‡é¢˜å’Œçˆ¶èŠ‚ç‚¹ä¸Šä¸‹æ–‡
                node_text = node.title
                parent_context = node.get_parent_context()
                if parent_context:
                    # å¦‚æœæœ‰çˆ¶èŠ‚ç‚¹ä¸Šä¸‹æ–‡ï¼Œå°†å…¶åŒ…å«åœ¨èŠ‚ç‚¹æ–‡æœ¬ä¸­
                    node_text = f"{parent_context} > {node.title}"

                # ä½¿ç”¨ embedder ç”ŸæˆèŠ‚ç‚¹åµŒå…¥
                node_embedding = self.embedder.embed_text(node_text)

                reward, breakdown = self.reward_calculator.calculate_reward(
                    new_docs=new_docs,
                    node_title=node.title,
                    all_doc_embeddings=all_embeddings,
                    node_embedding=node_embedding,
                )

                logger.info(
                    f"Node '{node.title}': reward={reward:.4f} "
                    f"(rel={breakdown['relevance']:.4f}, nov={breakdown['novelty']:.4f})"
                )

                # æ›´æ–° MAB çŠ¶æ€
                node.reward_history.append(reward)
                node.pull_count += 1

                # å‡†å¤‡ç”¨äº LLM ä¿®è®¢çš„æ•°æ®
                logger.info(f"node{node}")
                logger.info(f"new_docs{new_docs}")
                node_doc_pairs_for_refine.append((node, new_docs))

                # 5. æ›´æ–°è®°å¿†åº“
                self.memory.store_docs(new_docs)
                self.memory.map_node_to_docs(node.id, new_docs)

            # 6. (å…³é”®) LLM æ‰¹é‡æ›´æ–°å¤§çº²
            # (LLM Call #Round*2 + 1)
            logger.info(f"node_doc_pairs_for_refine{node_doc_pairs_for_refine}")
            logger.info(f"outline_root{outline_root}")
            logger.info(f"self.memory{self.memory}")
            if node_doc_pairs_for_refine:
                logger.info("Batch refining outline...")
                outline_root, expanded_nodes_list, new_node_doc_mapping = (
                    self.llm_wrapper.batch_refine_outline(
                        outline_root,
                        node_doc_pairs_for_refine,
                        memory=self.memory,  # ä¼ é€’ memory ä»¥è·å–ç´¯ç§¯æ–‡æ¡£
                    )
                )
                logger.info(f"expanded_nodes_list{expanded_nodes_list}")
                
                # --- (å·²ä¿®æ­£) å…³é”®çš„çŠ¶æ€ç»§æ‰¿æ­¥éª¤ ---
                # éå†é‚£äº›åˆšåˆšè¢«æ‰©å±•çš„èŠ‚ç‚¹ (ä»å¶å­èŠ‚ç‚¹å˜æˆäº†å†…éƒ¨èŠ‚ç‚¹)
                # expanded_nodes_list æ ¼å¼: [(parent_node, [new_child_node_1, ...]), ...]
                for parent_node, new_children in expanded_nodes_list:
                    if not new_children:
                        continue

                    # å°†çˆ¶èŠ‚ç‚¹çš„ MAB çŠ¶æ€å¤åˆ¶ç»™æ‰€æœ‰æ–°ç”Ÿæˆçš„å­èŠ‚ç‚¹
                    for child in new_children:
                        child.pull_count = parent_node.pull_count
                        # å¿…é¡»åˆ›å»º reward_history çš„å‰¯æœ¬
                        child.reward_history = list(parent_node.reward_history)

                    logger.info(
                        f"MAB state inherited: parent '{parent_node.title}' "
                        f"(pull_count={parent_node.pull_count}, "
                        f"rewards={len(parent_node.reward_history)}) -> "
                        f"{len(new_children)} children"
                    )
                # --- çŠ¶æ€ç»§æ‰¿ç»“æŸ ---

                # --- æ–°å¢ï¼šä¸ºæ–°å­èŠ‚ç‚¹å­˜å‚¨å¼•æ–‡æ˜ å°„ ---
                # new_node_doc_mapping æ ¼å¼: {node_id: [doc1, doc2, ...]}
                for node_id, docs in new_node_doc_mapping.items():
                    self.memory.map_node_to_docs(node_id, docs)
                    logger.debug(
                        f"New child node '{node_id}' mapped to {len(docs)} documents"
                    )

                if new_node_doc_mapping:
                    logger.info(
                        f"Citation mapping stored for {len(new_node_doc_mapping)} new child nodes"
                    )
                # --- å¼•æ–‡æ˜ å°„å­˜å‚¨ç»“æŸ ---

                logger.info(
                    f"Outline refined: {len(outline_root.get_all_nodes())} total nodes, "
                    f"{len(outline_root.get_leaf_nodes())} leaf nodes"
                )

            # æ‰“å°å½“å‰å¤§çº²ï¼ˆæ¯æ¬¡è¿­ä»£ç»“æŸï¼‰
            try:
                # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
                from .integration import outline_node_to_markdown

                current_outline_markdown = outline_node_to_markdown(
                    outline_root, max_depth=None, include_root=True
                )
                logger.info(
                    f"Current outline after round {round_num + 1}:\n{current_outline_markdown}"
                )
            except Exception as e:
                logger.warning(f"Failed to format outline for logging: {e}")
                # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„æ–‡æœ¬æ ‘
                logger.info(
                    f"Current outline after round {round_num + 1}:\n{outline_root.to_text_tree()}"
                )

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            if t >= self.max_iterations:
                logger.info(
                    f"Reached max iterations ({self.max_iterations}), terminating."
                )
                break

        logger.info(
            f"Batch-MAB completed: {t} iterations, "
            f"{len(outline_root.get_all_nodes())} nodes in final outline"
        )

        # å¯è§†åŒ–å¤§çº²æ ‘ä¸å¼•æ–‡æ˜ å°„å…³ç³»
        try:
            from .integration import visualize_outline_with_citations

            visualize_outline_with_citations(
                outline_root,
                self.memory,
                output_path="outline_with_citations",  # ç”Ÿæˆ outline_with_citations.png
                print_text=True,
            )
        except Exception as e:
            logger.warning(f"æ— æ³•å¯è§†åŒ–å¤§çº²æ ‘: {e}")

        return outline_root, self.memory


    def run_initialization(
        self,
        query: str,
        central_guidance=None,
        replan_result=None,
        instruction=None,
        initial_docs=None,
        k: int = 5,
        config: RunnableConfig=None,
    ):
        """
        åˆå§‹åŒ– FactStructï¼šæ£€ç´¢ â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ ç”Ÿæˆåˆå§‹å¤§çº²
        """
        logger.info(f"Starting Batch-MAB initialization with query: {query}")

        # --- Step 1: æ£€ç´¢ ---
        if not initial_docs:
            logger.info("Performing initial search...")
            initial_docs = self.search_engine(query, k=k, config=config)
        else:
            logger.info("Use existing")
            if not all(isinstance(doc, FactStructDocument) for doc in initial_docs):
                logger.info(f"initial_docs before:{initial_docs}")
                logger.info(f"Type of initial_docs: {type(initial_docs)}")
                initial_docs = self.wrap_raw_docs_to_factstruct(initial_docs)
                logger.info(f"initial_docs after:{initial_docs}")


        # --- Step 2: å‘é‡åŒ– ---
        initial_docs_with_embed = self.embedder.embed_docs(initial_docs)

        # --- Step 3: å­˜å…¥ memory ---
        self.memory.store_docs(initial_docs_with_embed)

        # --- Step 4: ç”Ÿæˆåˆå§‹å¤§çº² ---
        logger.info("Generating initial outline...")
        outline_root = self.llm_wrapper.generate_initial_outline(
            query=query,
            docs=initial_docs_with_embed,
            central_guidance=central_guidance,
            replan_result=replan_result,
            instruction=instruction,
        )

        logger.info(f"Generated outline root id: {outline_root.id}")

        # --- Step 5: æ–‡æ¡£ç»‘å®š ---
        self.memory.map_node_to_docs(outline_root.id, initial_docs_with_embed)
        logger.info(f"self.memory.node_to_docs{self.memory.node_to_docs}")
        
        #æ„Ÿè§‰è¿™ä¸ªåœ°æ–¹éœ€è¦æ€è€ƒä¸€ä¸‹ï¼Œåˆå§‹åŒ–çš„æ–‡æ¡£æ˜¯å½’è°çš„ï¼Œç°åœ¨æ˜¯æ”¾åˆ°äº† Root ä¸Š
        #æˆ‘è§‰å¾—ç¡®å®ä¸èƒ½ç»™å­èŠ‚ç‚¹ï¼Œå¦åˆ™æ–‡æ¡£è¦†ç›–ç‡è¿™ä¸ªä¸œè¥¿å°±ä¸å¤ªè¡Œäº†
        #åˆå§‹åŒ–çš„èŠ‚ç‚¹æ˜¯å¦éœ€è¦ç»™ä¸Šï¼Œå¥–åŠ±å¦‚æœä¸ç»™å¥–åŠ±çš„è¯ï¼Œä¸€å¼€å§‹çš„å¤§çº²æ‰©å±•å°±åªä¼šåœ¨ç¬¬ä¸€å±‚è¿›è¡Œæ‰©å±•
        #é‚£è¿™ä¹ˆè¯´ï¼Œfinish æ˜¯ä¸æ˜¯è¦å¤šä¸€ä¸ª ä¿è¯å¤§å®¶çš„reward éƒ½ä¸æ˜¯é›¶æ‰å¥½ç»“æŸã€‚
        #expansionçš„ reward æ›´æ–°åŒ…æ‹¬ä¸¤ä¸ªåœ°æ–¹ï¼Œä¸€ä¸ªæ˜¯æ£€ç´¢æ–‡æ¡£çš„æ›´æ–°ï¼Œä¸€ä¸ªæ˜¯å­èŠ‚ç‚¹çš„ç”Ÿæˆ
        #compassionçš„ rewardçš„æ›´æ–°åŒ…æ‹¬ä¸¤ä¸ªåœ°æ–¹ï¼Œä¸€ä¸ªæ˜¯select nodeéœ€è¦è®¡ç®— rewardä½†æ˜¯ä¸æ›´æ–°ã€‚ å¦ä¸€ä¸ªæ˜¯æ–°çš„èŠ‚ç‚¹çš„ç”Ÿæˆï¼Œè¿™ä¸ªåº”è¯¥æ˜¯ç”¨parent çš„ node å°±å¯ä»¥äº†
        #updat çš„ rewardçš„æ›´æ–°åŒ…æ‹¬ä¸¤ä¸ªåœ°æ–¹ï¼Œä¸€ä¸ªæ˜¯
        return outline_root, self.memory, initial_docs



    def run_expansion(
        self,
        outline_root,
        memory,
        max_iterations: int,
        batch_size: int,
        config: RunnableConfig,
    ):
        """
        æ‰§è¡Œ Batch-MAB é©±åŠ¨çš„å¤§çº²æ‰©å±•
        """
       # æ€»è¿­ä»£æ¬¡æ•°è®¡æ•°å™¨
        t = 0
        self.max_iterations=max_iterations
        self.batch_size=batch_size
        # --- è¿­ä»£å¾ªç¯ï¼ˆBatch-MAB è¿‡ç¨‹ï¼‰---
        num_rounds = math.ceil(self.max_iterations / self.batch_size)
        logger.info(
            f"Starting {num_rounds} rounds of batch optimization "
            f"(max_iterations={self.max_iterations}, batch_size={self.batch_size}, "
            f"total iterations={self.max_iterations})"
        )

        for round_num in range(num_rounds):
            logger.info(f"--- Round {round_num + 1}/{num_rounds} ---")

            # 1. è·å–å½“å‰æ‰€æœ‰å¯è¡ŒåŠ¨çš„"æ‰‹è‡‚"ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰
            current_leaf_nodes = outline_root.get_leaf_nodes()
            logger.info(f"current_leaf_nodes{current_leaf_nodes}")
            if not current_leaf_nodes:
                logger.info("No leaf nodes available, terminating early.")
                break

            logger.info(f"Found {len(current_leaf_nodes)} leaf nodes")

            # 2. UCB ç­–ç•¥é€‰æ‹© Top-K æ‰‹è‡‚
            selected_nodes = self._select_top_k_nodes(current_leaf_nodes, t)
            logger.info(f"selected_nodes{selected_nodes}")
            if not selected_nodes:
                logger.info("No nodes selected, terminating.")
                break

            logger.info(
                f"Selected {len(selected_nodes)} nodes: "
                f"{[node.title for node in selected_nodes]}"
            )

            # 3. "æ‰¹é‡æ‹‰åŠ¨æ‘‡è‡‚"ï¼ˆæ‰§è¡Œæ£€ç´¢ï¼‰
            # (LLM Call #Round*2)
            logger.info("Batch generating queries...")
            queries = self.llm_wrapper.batch_generate_queries(selected_nodes)

            # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢ï¼ˆæŒ‰ç…§ proposal è¦æ±‚å®ç°çœŸæ­£çš„å¹¶è¡Œæ£€ç´¢ï¼‰
            logger.info(f"Performing parallel search for {len(queries)} queries...")
            new_docs_list = self._parallel_search(queries, k=3,config=config)

            # é¢„å¤„ç†æ–°æ–‡æ¡£ï¼ˆåµŒå…¥ï¼‰
            new_docs_list_with_embed = []
            for docs in new_docs_list:
                docs_with_embed = self.embedder.embed_docs(docs)
                new_docs_list_with_embed.append(docs_with_embed)

            # 4. æ‰¹é‡è®¡ç®—å¹¶è®°å½•"å¥–åŠ±"
            all_embeddings = self.memory.get_all_doc_embeddings()
            node_doc_pairs_for_refine = []

            for i, node in enumerate(selected_nodes):
                t += 1  # å¢åŠ å…¨å±€è¿­ä»£è®¡æ•°å™¨
                new_docs = new_docs_list_with_embed[i]

                # è®¡ç®—å¥–åŠ±
                # ç”ŸæˆèŠ‚ç‚¹åµŒå…¥ï¼šä½¿ç”¨èŠ‚ç‚¹æ ‡é¢˜å’Œçˆ¶èŠ‚ç‚¹ä¸Šä¸‹æ–‡
                node_text = node.title
                parent_context = node.get_parent_context()
                if parent_context:
                    # å¦‚æœæœ‰çˆ¶èŠ‚ç‚¹ä¸Šä¸‹æ–‡ï¼Œå°†å…¶åŒ…å«åœ¨èŠ‚ç‚¹æ–‡æœ¬ä¸­
                    node_text = f"{parent_context} > {node.title}"

                # ä½¿ç”¨ embedder ç”ŸæˆèŠ‚ç‚¹åµŒå…¥
                node_embedding = self.embedder.embed_text(node_text)

                reward, breakdown = self.reward_calculator.calculate_reward(
                    new_docs=new_docs,
                    node_title=node.title,
                    all_doc_embeddings=all_embeddings,
                    node_embedding=node_embedding,
                )

                logger.info(
                    f"Node '{node.title}': reward={reward:.4f} "
                    f"(rel={breakdown['relevance']:.4f}, nov={breakdown['novelty']:.4f})"
                )

                # æ›´æ–° MAB çŠ¶æ€
                node.reward_history.append(reward)
                node.pull_count += 1

                # å‡†å¤‡ç”¨äº LLM ä¿®è®¢çš„æ•°æ®
                # logger.info(f"node{node}")
                # logger.info(f"new_docs{new_docs}")
                node_doc_pairs_for_refine.append((node, new_docs))

                # 5. æ›´æ–°è®°å¿†åº“
                self.memory.store_docs(new_docs)
                self.memory.map_node_to_docs(node.id, new_docs)

            # 6. (å…³é”®) LLM æ‰¹é‡æ›´æ–°å¤§çº²
            # (LLM Call #Round*2 + 1)
            # logger.info(f"node_doc_pairs_for_refine{node_doc_pairs_for_refine}")
            logger.info(f"outline_root{outline_root}")
            logger.info(f"self.memory.node_to_docs{self.memory.node_to_docs}")
            if node_doc_pairs_for_refine:
                logger.info("Batch refining outline...")
                outline_root, expanded_nodes_list, new_node_doc_mapping = (
                    self.llm_wrapper.batch_refine_outline(
                        outline_root,
                        node_doc_pairs_for_refine,
                        memory=self.memory,  # ä¼ é€’ memory ä»¥è·å–ç´¯ç§¯æ–‡æ¡£
                    )
                )
                logger.info(f"expanded_nodes_list{expanded_nodes_list}")
                
                # --- (å·²ä¿®æ­£) å…³é”®çš„çŠ¶æ€ç»§æ‰¿æ­¥éª¤ ---
                # éå†é‚£äº›åˆšåˆšè¢«æ‰©å±•çš„èŠ‚ç‚¹ (ä»å¶å­èŠ‚ç‚¹å˜æˆäº†å†…éƒ¨èŠ‚ç‚¹)
                # expanded_nodes_list æ ¼å¼: [(parent_node, [new_child_node_1, ...]), ...]
                for parent_node, new_children in expanded_nodes_list:
                    if not new_children:
                        continue

                    # å°†çˆ¶èŠ‚ç‚¹çš„ MAB çŠ¶æ€å¤åˆ¶ç»™æ‰€æœ‰æ–°ç”Ÿæˆçš„å­èŠ‚ç‚¹
                    # ç»™å­èŠ‚ç‚¹æä¾›æ–°çš„ reward
                    for child in new_children:
                        child.pull_count = parent_node.pull_count
                        # å¿…é¡»åˆ›å»º reward_history çš„å‰¯æœ¬
                        child.reward_history = list(parent_node.reward_history)

                    logger.info(
                        f"MAB state inherited: parent '{parent_node.title}' "
                        f"(pull_count={parent_node.pull_count}, "
                        f"rewards={len(parent_node.reward_history)}) -> "
                        f"{len(new_children)} children"
                    )
                # --- çŠ¶æ€ç»§æ‰¿ç»“æŸ ---

                # --- æ–°å¢ï¼šä¸ºæ–°å­èŠ‚ç‚¹å­˜å‚¨å¼•æ–‡æ˜ å°„ ---
                # new_node_doc_mapping æ ¼å¼: {node_id: [doc1, doc2, ...]}
                for node_id, docs in new_node_doc_mapping.items():
                    self.memory.map_node_to_docs(node_id, docs)
                    logger.debug(
                        f"New child node '{node_id}' mapped to {len(docs)} documents"
                    )

                if new_node_doc_mapping:
                    logger.info(
                        f"Citation mapping stored for {len(new_node_doc_mapping)} new child nodes"
                    )
                # --- å¼•æ–‡æ˜ å°„å­˜å‚¨ç»“æŸ ---

                logger.info(
                    f"Outline refined: {len(outline_root.get_all_nodes())} total nodes, "
                    f"{len(outline_root.get_leaf_nodes())} leaf nodes"
                )

            # æ‰“å°å½“å‰å¤§çº²ï¼ˆæ¯æ¬¡è¿­ä»£ç»“æŸï¼‰
            try:
                # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
                from .integration import outline_node_to_markdown

                current_outline_markdown = outline_node_to_markdown(
                    outline_root, max_depth=None, include_root=True
                )
                logger.info(
                    f"Current outline after round {round_num + 1}:\n{current_outline_markdown}"
                )
            except Exception as e:
                logger.warning(f"Failed to format outline for logging: {e}")
                # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„æ–‡æœ¬æ ‘
                logger.info(
                    f"Current outline after round {round_num + 1}:\n{outline_root.to_text_tree()}"
                )

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            if t >= self.max_iterations:
                logger.info(
                    f"Reached max iterations ({self.max_iterations}), terminating."
                )
                break

        logger.info(
            f"Batch-MAB completed: {t} iterations, "
            f"{len(outline_root.get_all_nodes())} nodes in final outline"
        )

        # å¯è§†åŒ–å¤§çº²æ ‘ä¸å¼•æ–‡æ˜ å°„å…³ç³»
        try:
            from .integration import visualize_outline_with_citations

            visualize_outline_with_citations(
                outline_root,
                self.memory,
                output_path="outline_with_citations",  # ç”Ÿæˆ outline_with_citations.png
                print_text=True,
            )
        except Exception as e:
            logger.warning(f"æ— æ³•å¯è§†åŒ–å¤§çº²æ ‘: {e}")

        return outline_root, self.memory




    def run_compression(
        self,
        outline_root,
        memory,
        merge_candidates: List["OutlineNode"],
        # max_merges: int, #ä¸åšè¿­ä»£äº†ã€‚
        # target_leaf_count: int, #è¿™ä¸ªæš‚æ—¶ä¹Ÿä¸è¦äº†ã€‚
        config: RunnableConfig,
    ):
        """
        æ‰§è¡Œå¤§çº²å‹ç¼©ï¼ˆBatch-MAB é£æ ¼ï¼Œæ¯æ¬¡é€‰æ‹©ä¸€ä¸ªçˆ¶èŠ‚ç‚¹è¿›è¡Œç»“æ„åˆå¹¶ï¼‰
        """

        logger.info("Running outline compression")
        # t = 0  # å‹ç¼©æ¬¡æ•°è®¡æ•°
        # compression_node_number=0

        # 1ï¸âƒ£ æŒ‰çˆ¶èŠ‚ç‚¹åˆ†ç»„ï¼ˆä»å¶å­èŠ‚ç‚¹å›æº¯ï¼‰
        parent_to_children = {}

        for node in merge_candidates:
            if node.parent:
                parent = node.parent
                parent_to_children.setdefault(parent, []).append(node)

        logger.info(f"å¯å‹ç¼©çˆ¶èŠ‚ç‚¹æ•°é‡: {len(parent_to_children)}")
        logger.info("çˆ¶èŠ‚ç‚¹åˆ—è¡¨:")
        for parent, children in parent_to_children.items():
            logger.info(
                f"- Parent: {parent.title} (id={parent.id}) "
                f"children_count={len(children)}"
            )

        parents = list(parent_to_children.keys())  # ç°åœ¨æ˜¯ OutlineNode å¯¹è±¡



        # 2ï¸âƒ£ Batch-MAB ä¸»å¾ªç¯ï¼ˆæ¯è½®åªå‹ä¸€ä¸ª parentï¼‰
        # while t < max_merges:
        ucb_scores = []
        for parent_iter in parents:
            # ç”¨ id åœ¨æœ€æ–°æ ‘ä¸­é‡æ–°å®šä½èŠ‚ç‚¹
            current_parent = outline_root.find_node_by_id(parent_iter.id)

            if current_parent is None:
                logger.warning(f"Parent {parent.id} not found in new tree, skipping")
                continue
            parent=current_parent
            children = parent_to_children.get(parent, [])
            
            # å­èŠ‚ç‚¹ç›¸å…³æ€§ï¼ˆæ˜¯å¦é€‚åˆå‹ï¼‰
            cohesion = self.compute_children_cohesion(parent, children)
            logger.info(f"parent:{parent},cohesion{cohesion}")
            # exploration / exploitation
            t_current = 0 + 1#æš‚æ—¶å…ˆè¿™ä¹ˆå†™
            if parent.pull_count == 0:
                exploration = float("inf")
                avg_reward = 0.0
            else:
                avg_reward = parent.avg_reward()
                exploration = math.sqrt(2 * math.log(t_current) / parent.pull_count)

            # âœ… Compression ä¸“ç”¨ UCB
            parent_depth=parent.get_depth()
            ucb_score = cohesion - (avg_reward + exploration)+parent_depth#åŠ ä¸ªæ·±åº¦ï¼Œè¦ä¸ root å®¹æ˜“è¢«é€‰ï¼Œå†åŠ ä¸ªæ–‡æ¡£æ•°å§
            ucb_scores.append((ucb_score, parent))

        if not ucb_scores:
            logger.info("No parents scored, stopping compression")
            # break
            return

        # 3ï¸âƒ£ é€‰æ‹©æœ€â€œå®‰å…¨å¯å‹â€çš„çˆ¶èŠ‚ç‚¹
        ucb_scores.sort(key=lambda x: x[0], reverse=True)
        parent = ucb_scores[0][1]
        children = parent_to_children.get(parent, [])

        logger.info(
            f"Compressing under parent '{parent.title}' "
            f"(children={len(children)})"
        )

        try:
            # 4ï¸âƒ£ è°ƒç”¨ LLM åšç»“æ„å‹ç¼©
            logger.info(f"compress_under_parentçš„parent{parent}")
            outline_root, compressed_nodes_list, new_node_doc_mapping, merged_node_mapping = (
                self.llm_wrapper.compress_under_parent(
                    outline_root=outline_root,
                    parent_node=parent,
                    child_nodes=children,
                    memory=memory,
                )
            )
            logger.info(f"compressed_nodes_list{compressed_nodes_list}")
            logger.info(f"new_node_doc_mapping{new_node_doc_mapping}")
            logger.info(f"merged_node_mapping{merged_node_mapping}")
            # 5ï¸âƒ£ çŠ¶æ€ç»§æ‰¿ï¼ˆå®Œå…¨å¯¹é½ expansionï¼‰
            for parent_node, new_children in compressed_nodes_list:
                for child in new_children:
                    child.pull_count = parent_node.pull_count
                    child.reward_history = list(parent_node.reward_history)


            # --- 6ï¸âƒ£ reward æ›´æ–°å’ŒMemory æ›´æ–°ï¼ˆCompression ä¸“ç”¨é€»è¾‘ï¼‰---
            parent.pull_count += 1
            logger.info(
                f"Compression success under '{parent.title}', "
                f"reward={parent.reward_history}"
            )
            # merged_node_mapping: { new_node_id: [old_node_id1, old_node_id2, ...] }


            #å…ˆåˆ é™¤ï¼Œåå¢åŠ ï¼Œè¢«ä¿®æ”¹çš„éƒ½æ˜¯ new_memory
            # new_memory=memory
            import copy
            new_memory = copy.deepcopy(memory)
            # compression_node_number=
            # --- åˆ é™¤è¢«å‹ç¼©èŠ‚ç‚¹çš„æ–‡æ¡£æ˜ å°„ï¼ˆéå¸¸é‡è¦ï¼‰---
            for old_node_ids in merged_node_mapping.values():
                for old_id in old_node_ids:
                    if old_id in new_memory.node_to_docs:
                        del new_memory.node_to_docs[old_id]
                        logger.debug(
                            f"Removed document mapping for compressed node '{old_id}'"
                        )
            
            # --- å¢åŠ æ–°èŠ‚ç‚¹çš„æ–‡æ¡£æ˜ å°„ï¼ˆéå¸¸é‡è¦ï¼‰---
            for new_node_id, old_node_ids in merged_node_mapping.items():
                merged_docs = []

                for old_id in old_node_ids:
                    docs = memory.get_docs_by_node(old_id)
                    # print("docs",docs)
                    merged_docs.extend(docs)
                # merged_docs æ˜¯ FactStructDocument å¯¹è±¡åˆ—è¡¨
                merged_docs = list({doc.id: doc for doc in merged_docs}.values())
                # print("merged_docs",merged_docs)
                if merged_docs:
                    new_memory.map_node_to_docs(new_node_id, merged_docs)

        except Exception as e:
            # æ‰“å°å®Œæ•´ traceback
            tb_str = traceback.format_exc()
            logger.error(
                f"Compression failed under parent '{parent.title}': {e}\n"
                f"Traceback:\n{tb_str}"
            )
            parents.remove(parent)
            # continue
            return

        # t += 1
        memory=new_memory
        outline_root=outline_root

        # 7ï¸âƒ£ æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼Œè¿™ä¸ªåœ°æ–¹æ˜¯ä¸å¯¹çš„
        # current_leaf_count = len(outline_root.get_leaf_nodes())
        # logger.info(f"Current leaf count: {current_leaf_count}")
        # if current_leaf_count <= target_leaf_count:
        #     logger.info("Target leaf count reached, stopping compression")
        #     break

        logger.info(
            f"Compression finished:"
            f"total_nodes={len(outline_root.get_all_nodes())}"
        )
        return outline_root, new_memory



    def run_update(
        self,
        outline_root,
        memory,
        update_candidates: List["OutlineNode"],
        config: RunnableConfig,
    ):
        """
        æ‰§è¡Œå¤§çº²èŠ‚ç‚¹æ›´æ–°ï¼ˆå•æ¬¡æ›´æ–°ï¼Œç±»ä¼¼ Compression + Expansionï¼Œä½†ä¸è¿­ä»£ï¼‰
        
        å‚æ•°:
            outline_root: å½“å‰å¤§çº²æ ¹èŠ‚ç‚¹
            memory: å½“å‰ Memory å¯¹è±¡
            update_candidates: å¾…æ›´æ–°çš„èŠ‚ç‚¹åˆ—è¡¨
            config: RunnableConfig é…ç½®å¯¹è±¡
        
        è¿”å›:
            æ›´æ–°åçš„ outline_root å’Œ memory
        """
        import copy
        import traceback

        logger.info("Running outline update")

        # 1ï¸âƒ£ æŒ‰çˆ¶èŠ‚ç‚¹åˆ†ç»„ï¼ˆä»å¶å­èŠ‚ç‚¹å›æº¯ï¼‰
        parent_to_children = {}
        for node in update_candidates:
            if node.parent:
                parent = node.parent
                parent_to_children.setdefault(parent, []).append(node)

        logger.info(f"å¯æ›´æ–°çˆ¶èŠ‚ç‚¹æ•°é‡: {len(parent_to_children)}")
        logger.info("çˆ¶èŠ‚ç‚¹åˆ—è¡¨:")
        for parent, children in parent_to_children.items():
            logger.info(
                f"- Parent: {parent.title} (id={parent.id}) "
                f"children_count={len(children)}"
            )

        parents = list(parent_to_children.keys())  # ç°åœ¨æ˜¯ OutlineNode å¯¹è±¡

        ucb_scores = []
        # 2ï¸âƒ£ Batch-MAB ä¸»é€»è¾‘
        for parent_iter in parents:
            # ç”¨ id åœ¨æœ€æ–°æ ‘ä¸­é‡æ–°å®šä½èŠ‚ç‚¹
            parent = outline_root.find_node_by_id(parent_iter.id)

            if parent is None:
                logger.warning(f"Parent {parent.id} not found in new tree, skipping")
                continue

            # è®¡ç®— exploration å’Œ exploitation
            t_current = 0 + 1  # æš‚æ—¶è¿™æ ·å¤„ç†
            if parent.pull_count == 0:
                exploration = float("inf")
                avg_reward = 0.0
            else:
                avg_reward = parent.avg_reward()
                exploration = math.sqrt(2 * math.log(t_current) / parent.pull_count)

            # âœ… update ä¸“ç”¨ UCB
            parent_depth=parent.get_depth()#åŠ ä¸ªæ·±åº¦
            parent_doc_num=len(memory.map_node_to_docs(parent))#åŠ ä¸ªæ–‡æ¡£æ•°
            ucb_score = avg_reward + exploration+parent_depth-parent_doc_num
            ucb_scores.append((ucb_score, parent))

        if not ucb_scores:
            logger.info("No parents scored, stopping update")
            return outline_root, memory

        # 3ï¸âƒ£ é€‰æ‹©æœ€â€œå®‰å…¨æ›´æ–°â€çš„çˆ¶èŠ‚ç‚¹
        ucb_scores.sort(key=lambda x: x[0], reverse=True)
        parent = ucb_scores[0][1]
        children = parent_to_children.get(parent, [])
        # ==========================================
        # ğŸ” 3. æ‰¹é‡æ£€ç´¢ï¼ˆæ‹‰åŠ¨æ‘‡è‡‚ï¼‰
        # ==========================================
        if len(children)==0:
            selected_nodes = children
        else:
            selected_nodes=[parent]
        # selected_nodes=[parent]

        logger.info("Batch generating queries...")
        queries = self.llm_wrapper.batch_generate_queries(selected_nodes)

        logger.info(f"Performing parallel search for {len(queries)} queries...")
        logger.info(f"Performing parallel search for {queries}")
        new_docs_list = self._parallel_search(queries, k=3, config=config)

        # æ–‡æ¡£åµŒå…¥
        new_docs_list_with_embed = []
        for docs in new_docs_list:
            docs_with_embed = self.embedder.embed_docs(docs)
            new_docs_list_with_embed.append(docs_with_embed)

        # ==========================================
        # ğŸ¯ 4. æ‰¹é‡è®¡ç®— reward
        # ==========================================

        all_embeddings = memory.get_all_doc_embeddings()

        for i, node in enumerate(selected_nodes):
            new_docs = new_docs_list_with_embed[i]

            # æ„é€ èŠ‚ç‚¹æ–‡æœ¬
            node_text = node.title
            parent_context = node.get_parent_context()
            if parent_context:
                node_text = f"{parent_context} > {node.title}"

            node_embedding = self.embedder.embed_text(node_text)

            reward, breakdown = self.reward_calculator.calculate_reward(
                new_docs=new_docs,
                node_title=node.title,
                all_doc_embeddings=all_embeddings,
                node_embedding=node_embedding,
            )

            logger.info(
                f"[UPDATE] Node '{node.title}': reward={reward:.4f} "
                f"(rel={breakdown['relevance']:.4f}, nov={breakdown['novelty']:.4f})"
            )

            # æ›´æ–° MAB çŠ¶æ€
            node.reward_history.append(reward)
            node.pull_count += 1

            # æ›´æ–° memory
            memory.store_docs(new_docs)
            memory.map_node_to_docs(node.id, new_docs)






        logger.info(f"Updating under parent '{parent.title}' (children={len(children)})")

        try:
            # 4ï¸âƒ£ è°ƒç”¨ LLM æ›´æ–°æ“ä½œ
            logger.info(f"update_under_parent çš„ parent: {parent}")
            outline_root, updated_nodes_list, new_node_doc_mapping, updated_node_mapping = (
                self.llm_wrapper.update_under_parent(
                    outline_root=outline_root,
                    parent_node=parent,
                    child_nodes=children,
                    memory=memory,
                )
            )

            logger.info(f"updated_nodes_list: {updated_nodes_list}")
            logger.info(f"new_node_doc_mapping: {new_node_doc_mapping}")
            logger.info(f"updated_node_mapping: {updated_node_mapping}")

            # 5ï¸âƒ£ çŠ¶æ€ç»§æ‰¿ï¼ˆå®Œå…¨å¯¹é½ï¼‰
            for parent_node, new_children in updated_nodes_list:
                for child in new_children:
                    child.pull_count = parent_node.pull_count
                    child.reward_history = list(parent_node.reward_history)

            # --- 6ï¸âƒ£ reward æ›´æ–°å’Œ Memory æ›´æ–° ---
            parent.pull_count += 1
            logger.info(
                f"Update success under '{parent.title}', reward={parent.reward_history}"
            )

            # æ–°å»º memoryï¼Œä»¥å…ä¿®æ”¹åŸ memory
            new_memory = copy.deepcopy(memory)

            # 7ï¸âƒ£ åˆ é™¤æ—§èŠ‚ç‚¹çš„æ–‡æ¡£æ˜ å°„
            for old_node_ids in updated_node_mapping.values():
                for old_id in old_node_ids:
                    if old_id in new_memory.node_to_docs:
                        del new_memory.node_to_docs[old_id]
                        logger.debug(f"Removed document mapping for updated node '{old_id}'")

            # 8ï¸âƒ£ å¢åŠ æ–°èŠ‚ç‚¹çš„æ–‡æ¡£æ˜ å°„
            for new_node_id, old_node_ids in updated_node_mapping.items():
                updated_docs = []

                for old_id in old_node_ids:
                    docs = memory.get_docs_by_node(old_id)
                    updated_docs.extend(docs)

                # æ›´æ–° docs åˆ—è¡¨ï¼Œå»é™¤é‡å¤
                updated_docs = list({doc.id: doc for doc in updated_docs}.values())

                if updated_docs:
                    new_memory.map_node_to_docs(new_node_id, updated_docs)
                    logger.debug(f"New node '{new_node_id}' mapped to {len(updated_docs)} documents")

        except Exception as e:
            tb_str = traceback.format_exc()
            logger.error(f"Update failed under parent '{parent.title}': {e}\nTraceback:\n{tb_str}")
            return outline_root, memory

        logger.info(f"Update finished: total nodes={len(outline_root.get_all_nodes())}")

        return outline_root, new_memory


    def _select_top_k_nodes(
        self,
        leaf_nodes: List[OutlineNode],
        current_t: int,
    ) -> List[OutlineNode]:
        """
        ä½¿ç”¨ UCB1 ç­–ç•¥é€‰æ‹© Top-K èŠ‚ç‚¹

        å‚æ•°:
            leaf_nodes: æ‰€æœ‰å¶å­èŠ‚ç‚¹
            current_t: å½“å‰æ€»è¿­ä»£æ¬¡æ•°

        è¿”å›:
            é€‰ä¸­çš„ Top-K èŠ‚ç‚¹åˆ—è¡¨
        """
        ucb_scores = []

        for node in leaf_nodes:
            t_current = current_t + 1  # UCB å…¬å¼ä¸­çš„ t

            if node.pull_count == 0:
                # æœªæ¢ç´¢è¿‡çš„èŠ‚ç‚¹ï¼Œç»™äºˆæ— é™å¤§çš„æ¢ç´¢å¥–åŠ±
                ucb_score = float("inf")
            else:
                # UCB1 å…¬å¼
                avg_reward = node.avg_reward()
                exploration_bonus = math.sqrt(2 * math.log(t_current) / node.pull_count)
                ucb_score = avg_reward + exploration_bonus

            ucb_scores.append((ucb_score, node))

        # æ’åºå¹¶é€‰å‡º Top-K
        ucb_scores.sort(key=lambda x: x[0], reverse=True)
        selected = [node for score, node in ucb_scores[: self.batch_size]]

        return selected

    def _parallel_search(
        self,
        queries: List[str],
        k: int = 3,
        config:RunnableConfig=None,
    ) -> List[List[FactStructDocument]]:
        """
        å¹¶è¡Œæ‰§è¡Œæ£€ç´¢

        å‚æ•°:
            queries: æŸ¥è¯¢åˆ—è¡¨
            k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æ–‡æ¡£æ•°é‡

        è¿”å›:
            æ–‡æ¡£åˆ—è¡¨çš„åˆ—è¡¨ï¼Œä¸ queries ä¸€ä¸€å¯¹åº”
        """
        if not queries:
            return []

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œæ£€ç´¢
        # æ£€ç´¢é€šå¸¸æ˜¯ I/O å¯†é›†å‹ä»»åŠ¡ï¼Œä½¿ç”¨ ThreadPoolExecutor æ¯”è¾ƒåˆé€‚
        results = [None] * len(queries)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨ï¼Œä¿æŒé¡ºåº

        with ThreadPoolExecutor(
            max_workers=min(len(queries), self.batch_size)
        ) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(self.search_engine, query, k,config): i
                for i, query in enumerate(queries)
            }

            # æ”¶é›†ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼Œä½†ä¿æŒåŸå§‹ç´¢å¼•ï¼‰
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    docs = future.result()
                    results[index] = docs
                # except Exception as e:
                #     logger.error(
                #         f"Search failed for query {index} ({queries[index]}): {e}"
                #     )
                #     results[index] = []  # å¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨
                except Exception as e:
                    import traceback

                    tb = traceback.format_exc()

                    logger.error(
                        f"[SEARCH ERROR] Query index={index}, "
                        f"query='{queries[index]}', "
                        f"error={str(e)}\nTraceback:\n{tb}"
                    )

                    results[index] = {
                        "error": True,
                        "query": queries[index],
                        "exception": str(e),
                        "traceback": tb,
                        "docs": []
                    }


        return results

    def wrap_raw_docs_to_factstruct(
        self,
        raw_docs,
        source_type="web",
    ):
        wrapped = []

        if (
            isinstance(raw_docs, list)
            and len(raw_docs) == 1
            and isinstance(raw_docs[0], str)
            and "tool_name" in raw_docs[0]
            and "web_search" in raw_docs[0]
        ):
            s = raw_docs[0]
            try:
                # 1ï¸âƒ£ æå– content='[...]' éƒ¨åˆ†ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰
                content_match = re.search(r"content='(.*?)' name=", s, re.DOTALL)
                if content_match:
                    content_str = content_match.group(1)
                    # 2ï¸âƒ£ æ›¿æ¢è½¬ä¹‰åŒå¼•å·
                    content_str = content_str.replace('\\"', '"')
                    # 3ï¸âƒ£ è½¬æˆ listï¼ˆå¼‚å¸¸æ•è·ï¼‰
                    try:
                        content_list = json.loads(content_str)
                    except json.JSONDecodeError:
                        # å¦‚æœ JSONDecodeErrorï¼Œå°è¯•è§£æåˆ°æœ€åä¸€ä¸ª JSON å¯¹è±¡
                        # è¿™é‡Œç®€å•æ–¹æ³•ï¼šç”¨ eval å®‰å…¨å­é›†
                        import ast
                        content_list = ast.literal_eval(content_str)

                    # 4ï¸âƒ£ æ‰¾åˆ° raw_results
                    raw_docs = []
                    for item in content_list:
                        if isinstance(item, dict) and "raw_results" in item:
                            raw_docs = item["raw_results"]
                            break
                else:
                    raw_docs = []

            except Exception as e:
                # æ”¹ç”¨å•å‚æ•°è¾“å‡ºï¼Œé¿å… logger æŠ¥é”™
                logger.error(f"æ–‡æ¡£è½¬æ¢å¤±è´¥: {e}")
                raw_docs = []

        # å°è£… FactStructDocument
        for i, d in enumerate(raw_docs):
            if not isinstance(d, dict):
                continue

            text = d.get("snippet", "")
            text = re.sub(r"[ã€‚.]{2,}", "", text).strip()
            if not text:
                continue

            doc = FactStructDocument(
                id=f"doc_{hash(text)}_{i}",
                cite_id=i + 1,
                text=text,
                source_type=source_type,
                timestamp=datetime.now(),
                url=d.get("link"),
                title=d.get("title"),
            )
            wrapped.append(doc)

        return wrapped
    


    def compute_children_cohesion(
        self,
        parent: "OutlineNode",
        children: List["OutlineNode"],
    ) -> float:
        """
        è®¡ç®—å­èŠ‚ç‚¹ä¹‹é—´çš„è¯­ä¹‰å†…èšåº¦ï¼ˆå¹³å‡ pairwise cosine similarityï¼‰

        cohesion âˆˆ [-1, 1]ï¼ˆé€šå¸¸åœ¨ [0, 1]ï¼‰
        è¶Šå¤§è¡¨ç¤ºè¿™äº›å­èŠ‚ç‚¹è¶Šåº”è¯¥è¢«åˆå¹¶
        """

        n = len(children)
        if n < 2:
            return 0.0

        # 1ï¸âƒ£ æ„é€ å­èŠ‚ç‚¹ embeddingsï¼ˆå¸¦çˆ¶ä¸Šä¸‹æ–‡ï¼Œä¿è¯è¯­ä¹‰ç©ºé—´ä¸€è‡´ï¼‰
        child_embeddings = []
        parent_context = parent.title  # åªç”¨ parentï¼Œä¸å†å¼•å…¥ parent.parent

        for child in children:
            node_text = f"{parent_context} > {child.title}"
            emb = self.embedder.embed_text(node_text)
            child_embeddings.append(emb)

        if len(child_embeddings) < 2:
            return 0.0

        # 2ï¸âƒ£ è®¡ç®— pairwise cosine similarity
        total_sim = 0.0
        pair_count = 0

        for i in range(len(child_embeddings)):
            for j in range(i + 1, len(child_embeddings)):
                sim = cosine_similarity(
                    child_embeddings[i],
                    child_embeddings[j],
                )
                total_sim += sim
                pair_count += 1

        if pair_count == 0:
            return 0.0

        cohesion = total_sim / pair_count
        return float(cohesion)





def cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)

    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    if norm1 == 0 or norm2 == 0:
        return 0.0

    return float(np.dot(vec1, vec2) / (norm1 * norm2))


if __name__ == "__main__":
    """
    æµ‹è¯• Batch-MAB run_compression åŠŸèƒ½
    åœºæ™¯ï¼šå‹ç¼© â€œä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€æ€¥æ€§æœŸçš„ä½œç”¨â€ ä¸‹çš„ 4 ä¸ªå­èŠ‚ç‚¹
    """

    from datetime import datetime
    from src.factstruct.memory import Memory
    from src.factstruct.document import FactStructDocument
    from src.factstruct.outline_node import OutlineNode
    from src.factstruct.llm_wrapper import FactStructLLMWrapper
    from src.llms.llm import get_llm_by_type
    from src.config.agents import AGENT_LLM_MAP
    from langchain_core.runnables import RunnableConfig
    from src.factstruct.embedder import Embedder
    from src.factstruct.batch_mab import BatchMAB
    # from src.search.adapter import create_search_engine_adapter  # å¦‚æœæœ‰çš„è¯
    
    print("========== START BATCH-MAB COMPRESSION DEBUG ==========")

    # -----------------------
    # 1ï¸âƒ£ æ„é€  Memory
    # -----------------------
    memory = Memory()

    # æµ‹è¯•æ–‡æ¡£
    doc1 = FactStructDocument(
        id="doc_1", cite_id="CIT001", source_type="journal",
        title="ä¸­æ€§ç²’ç»†èƒå‹Ÿé›†æœºåˆ¶ç ”ç©¶",
        text="æ€¥æ€§æœŸä¸­æ€§ç²’ç»†èƒé€šè¿‡è¶‹åŒ–å› å­è¢«å‹Ÿé›†åˆ°ç¼ºè¡€åŒºåŸŸã€‚",
        embedding=None, timestamp=datetime.now()
    )
    doc2 = FactStructDocument(
        id="doc_2", cite_id="CIT002", source_type="journal",
        title="è¡€è„‘å±éšœç ´åæœºåˆ¶",
        text="ä¿ƒç‚å› å­é‡Šæ”¾å¯¼è‡´è¡€è„‘å±éšœé€šé€æ€§å¢åŠ ã€‚",
        embedding=None, timestamp=datetime.now()
    )
    doc3 = FactStructDocument(
        id="doc_3", cite_id="CIT003", source_type="journal",
        title="ç‚ç—‡ä¸ç¥ç»æŸä¼¤",
        text="ç‚ç—‡ååº”åŠ å‰§è„‘æ°´è‚¿ä¸ç¥ç»æŸä¼¤ã€‚",
        embedding=None, timestamp=datetime.now()
    )

    # -----------------------
    # 2ï¸âƒ£ æ„é€  Outline
    # -----------------------
    # root = OutlineNode(id="node_0", title="ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€ä¸­çš„ä½œç”¨", pull_count=2, reward_history=[0.8, 0.9], word_limit=500)
    # acute = OutlineNode(id="node_1", title="ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€æ€¥æ€§æœŸçš„ä½œç”¨", pull_count=1, reward_history=[0.7], word_limit=300)
    
    # n2 = OutlineNode(id="node_2", title="ä¸­æ€§ç²’ç»†èƒçš„å‹Ÿé›†ä¸æ¿€æ´»æœºåˆ¶", pull_count=0, reward_history=[], word_limit=100)
    # n3 = OutlineNode(id="node_3", title="ä¿ƒç‚å› å­é‡Šæ”¾ä¸è¡€-è„‘å±éšœç ´å", pull_count=0, reward_history=[], word_limit=100)
    # n4 = OutlineNode(id="node_4", title="ç‚ç—‡ååº”å¯¹è„‘æ°´è‚¿ä¸ç¥ç»æŸä¼¤çš„å½±å“", pull_count=0, reward_history=[], word_limit=100)
    # n5 = OutlineNode(id="node_5", title="ä¸­æ€§ç²’ç»†èƒåœ¨ç¥ç»ä¿®å¤ä¸­çš„æ½œåœ¨ä½œç”¨", pull_count=0, reward_history=[], word_limit=100)

    # acute.add_child(n2)
    # acute.add_child(n3)
    # acute.add_child(n4)
    # acute.add_child(n5)
    # root.add_child(acute)
    
    # -----------------------
    # 2ï¸âƒ£ æ„é€  Outlineï¼ˆå¤š parent æµ‹è¯•ï¼‰
    # -----------------------

    root = OutlineNode(
        id="node_0",
        title="ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€ä¸­çš„ä½œç”¨",
        pull_count=0,
        reward_history=[],
        word_limit=500
    )

    # ===== Parent Aï¼ˆé«˜ rewardï¼Œä¸åº”è¯¥è¢«é€‰ï¼‰=====
    acute_A = OutlineNode(id="node_1",
        title="ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€æ€¥æ€§ç‚ç—‡é˜¶æ®µçš„åˆ†å­æœºåˆ¶",
        # pull_count=5,reward_history=[0.9, 0.88, 0.92, 0.91, 0.87],
        pull_count=1,reward_history=[0.9],
        word_limit=300
    )

    A1 = OutlineNode(id="node_2",
        title="ä¸­æ€§ç²’ç»†èƒå‹Ÿé›†çš„è¶‹åŒ–å› å­è°ƒæ§æœºåˆ¶",
        pull_count=0,reward_history=[]
    )

    A2 = OutlineNode(id="node_3",
        title="ä¸­æ€§ç²’ç»†èƒæ¿€æ´»åçš„ä¿ƒç‚ä¿¡å·çº§è”ååº”",
        pull_count=0,reward_history=[]
    )

    A3 = OutlineNode(id="node_4",
        title="ä¸­æ€§ç²’ç»†èƒè¯±å¯¼çš„è¡€è„‘å±éšœé€šé€æ€§æ”¹å˜",
        pull_count=0,reward_history=[]
    )

    # acute_A.add_child(A1)
    # acute_A.add_child(A2)
    # acute_A.add_child(A3)

    # ===== Parent Bï¼ˆä½ rewardï¼Œåº”è¯¥è¢«é€‰ï¼‰=====
    acute_B = OutlineNode(id="node_5",
        title="æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ä¸­ä¸­æ€§ç²’ç»†èƒä»‹å¯¼çš„ç‚ç—‡æŸä¼¤",
        pull_count=1,reward_history=[0.1],word_limit=300
    )

    B1 = OutlineNode(id="node_6",
        title="ä¸­æ€§ç²’ç»†èƒä»‹å¯¼çš„æ€¥æ€§ç‚ç—‡ååº”æœºåˆ¶",
        pull_count=0,reward_history=[]
    )

    B2 = OutlineNode(id="node_7",
        title="æ€¥æ€§æœŸä¸­æ€§ç²’ç»†èƒé‡Šæ”¾ç‚ç—‡å› å­çš„æœºåˆ¶",
        pull_count=0,reward_history=[]
    )

    B3 = OutlineNode(id="node_8",
        title="ä¸­æ€§ç²’ç»†èƒåœ¨æ€¥æ€§è„‘ç¼ºè¡€ç‚ç—‡çº§è”ä¸­çš„ä½œç”¨",
        pull_count=0,reward_history=[]
    )


    acute_B.add_child(B1)
    acute_B.add_child(B2)
    acute_B.add_child(B3)

    root.add_child(acute_A)
    root.add_child(acute_B)




    # # -----------------------
    # # 3ï¸âƒ£ æ„å»ºèŠ‚ç‚¹-æ–‡æ¡£æ˜ å°„
    # # -----------------------
    # memory.map_node_to_docs("node_2", [doc1])
    # memory.map_node_to_docs("node_3", [doc2])
    # memory.map_node_to_docs("node_4", [doc3])
    # -----------------------
    # 3ï¸âƒ£ æ„å»ºèŠ‚ç‚¹-æ–‡æ¡£æ˜ å°„ï¼ˆå¢å¼ºç‰ˆï¼‰
    # -----------------------

    docs = []
    docs = [
        FactStructDocument(
            id="doc_1",
            cite_id="CIT001",
            source_type="journal",
            title="ä¸­æ€§ç²’ç»†èƒåœ¨è„‘ç¼ºè¡€æŸä¼¤ä¸­ä½œç”¨çš„ç ”ç©¶è¿›å±•",
            text="ç»¼è¿°ä¸­æ€§ç²’ç»†èƒåœ¨æ€¥æ€§è„‘ç¼ºè¡€ä¸­çš„ç‚ç—‡æœºåˆ¶...",
            embedding=None,
            timestamp=datetime.now(),
        ),
        FactStructDocument(
            id="doc_2",
            cite_id="CIT002",
            source_type="journal",
            title="æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ä¸­ç‚ç—‡çº§è”ååº”æœºåˆ¶",
            text="ç‚ç—‡å› å­é‡Šæ”¾ä¸è„‘ç»„ç»‡æŸä¼¤å¯†åˆ‡ç›¸å…³...",
            embedding=None,
            timestamp=datetime.now(),
        ),
        FactStructDocument(
            id="doc_3",
            cite_id="CIT003",
            source_type="journal",
            title="è¡€è„‘å±éšœç ´åä¸ä¸­æ€§ç²’ç»†èƒæµ¸æ¶¦å…³ç³»ç ”ç©¶",
            text="BBBé€šé€æ€§å˜åŒ–ä¿ƒè¿›ç‚ç—‡ç»†èƒè¿›å…¥è„‘ç»„ç»‡...",
            embedding=None,
            timestamp=datetime.now(),
        ),
        FactStructDocument(
            id="doc_4",
            cite_id="CIT004",
            source_type="journal",
            title="æ€¥æ€§è„‘å’ä¸­åå…ç–«ç»†èƒåŠ¨æ€å˜åŒ–ç ”ç©¶",
            text="ä¸­æ€§ç²’ç»†èƒåœ¨æ—©æœŸç‚ç—‡ååº”ä¸­å ä¸»å¯¼åœ°ä½...",
            embedding=None,
            timestamp=datetime.now(),
        ),
    ]


    # Parent A
    # memory.map_node_to_docs("node_2", [docs[0]])
    # memory.map_node_to_docs("node_3", [docs[1]])
    # memory.map_node_to_docs("node_4", [docs[2]])

    # Parent B
    memory.map_node_to_docs("node_6", [docs[0], docs[3]])
    memory.map_node_to_docs("node_7", [docs[1]])
    memory.map_node_to_docs("node_8", [docs[0], docs[1]])



    # -----------------------
    # 4ï¸âƒ£ åˆå§‹åŒ– LLM Wrapper
    # -----------------------
    llm_type = AGENT_LLM_MAP.get("outline", "basic")
    llm = get_llm_by_type(llm_type)
    wrapper = FactStructLLMWrapper(llm)


    # -----------------------
    # 5ï¸âƒ£ åˆå§‹åŒ– Embedder + LLM + BatchMABï¼ˆä»¿çœŸå®ç³»ç»Ÿï¼‰
    # -----------------------

    # === Embedderï¼ˆçœŸå®æ¨¡å‹ï¼‰===
    embedder = Embedder(model_name="../../Model/MiniLM/all-MiniLM-L6-v2")

    # === Search Engineï¼ˆå¦‚æœä¸æƒ³çœŸçš„æœï¼Œå¯ä»¥ç»™ dummyï¼‰===
    from src.factstruct.integration import create_search_engine_adapter

    search_engine = create_search_engine_adapter()
    # search_engine = lambda q, k: []   # æµ‹è¯•å‹ç¼©ä¸éœ€è¦æœç´¢

    # === LLM ===
    llm_type = AGENT_LLM_MAP.get("outline", "basic")
    llm = get_llm_by_type(llm_type)

    # === Wrapper ===
    wrapper = FactStructLLMWrapper(llm)
    print("\n--- BEFORE COMPRESSION ---")
    print(root.to_text_tree(include_word_limit=True, include_mab_state=True))
    print("Memory:", memory.node_to_docs)
    wrapper._inherit_mab_state_for_existing_nodes(old_root=None, new_root=root)
    
    # === Batch-MAB ===
    batch_mab = BatchMAB(
        llm_wrapper=wrapper,
        embedder=embedder,
        search_engine=search_engine,
        max_iterations=4,
        memory=memory,        # â­ å…³é”®ï¼šæŠŠä½ æ„é€ çš„ memory ä¼ è¿›å»
        batch_size=2,
    )



    # # -----------------------
    # # 6ï¸âƒ£ æ‰§è¡Œ run_compression
    # # -----------------------
    # # merge_candidates = [n3, n4, n5]
    # merge_candidates = [A1, A2, A3, B1, B2, B3]

    # # merge_candidates = [n3, n4]
    # # merge_candidates = [n2, n3]
    # # merge_candidates = [n2]
    # max_merges = 2
    # target_leaf_count = 2
    # config = RunnableConfig()

    # new_root, new_memory = batch_mab.run_compression(
    #     outline_root=root,
    #     memory=memory,
    #     merge_candidates=merge_candidates,
    #     # max_merges=max_merges,
    #     # target_leaf_count=target_leaf_count,
    #     config=config
    # )

    # # -----------------------
    # # 7ï¸âƒ£ è¾“å‡ºç»“æœ
    # # -----------------------
    # print("\n--- AFTER COMPRESSION ---")
    # print(new_root.to_text_tree(include_word_limit=True, include_mab_state=True))
    # print("Memory:", new_memory.node_to_docs)

    # éªŒè¯ parent-child é“¾è·¯
    def validate_tree(node):
        for child in node.children:
            assert child.parent == node, f"Parent pointer broken at {child.title}"
            validate_tree(child)

    # validate_tree(new_root)
    # print("\nâœ… Tree structure valid.")
    # print("========== END DEBUG ==========")
    print("\n========== START BATCH-MAB UPDATE DEBUG ==========")

    print("\n--- BEFORE UPDATE ---")
    print(root.to_text_tree(include_word_limit=True, include_mab_state=True))
    print("Memory:", memory.node_to_docs)

    # update_candidates = [A1, A2, A3, B1, B2, B3]
    update_candidates = [acute_A,B1, B2, B3]

    # config = RunnableConfig()
    config = {
        "configurable": {
            "thread_id": "debug-session"
        }
    }
    
    new_root, new_memory = batch_mab.run_update(
        outline_root=root,
        memory=memory,
        update_candidates=update_candidates,
        config=config
    )

    print("\n--- AFTER UPDATE ---")
    print(new_root.to_text_tree(include_word_limit=True, include_mab_state=True))
    print("Memory:", new_memory.node_to_docs)

    validate_tree(new_root)
    print("\nâœ… Tree structure valid.")
    print("========== END UPDATE DEBUG ==========")




