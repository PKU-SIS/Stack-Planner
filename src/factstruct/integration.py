"""
FactStruct Stage 1 é›†æˆæ¨¡å—

æä¾›äº†ä¸ç°æœ‰ç³»ç»Ÿé›†æˆçš„ä¾¿æ·æ¥å£ã€‚
"""

from typing import List, Optional, Callable, Tuple
from langchain_core.language_models import BaseChatModel
import traceback


from src.utils.logger import logger
from src.llms.llm import get_llm_by_type
from src.config.agents import AGENT_LLM_MAP
from src.tools.get_docs_info import search_docs
from src.tools.bocha_search.web_search_en import web_search
from .batch_mab import BatchMAB
from .embedder import Embedder
from .llm_wrapper import FactStructLLMWrapper
from .document import FactStructDocument
from .outline_node import OutlineNode
from .memory import Memory
from datetime import datetime
from src.utils.reference_utils import global_reference_map
from langchain_core.runnables import RunnableConfig

import re
from collections import defaultdict
import json
# from modelscope.pipelines import pipeline
# from modelscope.utils.constant import Tasks
from sentence_transformers import CrossEncoder
from .cite_verify import filter_content_by_relevant_docs,mark_content_with_support,repair_unknown_citations
def create_search_engine_adapter(
    search_func: Callable = None,
) -> Callable[[str, int,RunnableConfig], List[FactStructDocument]]:
    """
    åˆ›å»ºæœç´¢å¼•æ“é€‚é…å™¨

    å°†ç°æœ‰çš„ search_docs å‡½æ•°é€‚é…ä¸º FactStruct éœ€è¦çš„æ ¼å¼ã€‚

    å‚æ•°:
        search_func: æœç´¢å‡½æ•°ï¼Œç­¾å (question: str, top_k: int) -> List[dict]
                    å¦‚æœä¸æä¾›ï¼Œä½¿ç”¨é»˜è®¤çš„ search_docs

    è¿”å›:
        é€‚é…åçš„æœç´¢å‡½æ•°ï¼Œç­¾å (query: str, k: int) -> List[FactStructDocument]
    """
    if search_func is None:
        #è¿™ä¸ªåœ°æ–¹è¦æ”¹æˆç½‘ç»œæœç´¢
        # search_func = search_docs
        search_func = web_search
    def adapter(query: str, k: int, config:RunnableConfig=None) -> List[FactStructDocument]:
        """
        é€‚é…åçš„æœç´¢å‡½æ•°

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡

        è¿”å›:
            FactStructDocument åˆ—è¡¨
        """
        from datetime import datetime

        # è°ƒç”¨åŸå§‹æœç´¢å‡½æ•°
        results = search_func(query, top_k=k)
        logger.info(f"results:{results}")
        ids = None
        if config!=None:
            session_id = config["configurable"]["thread_id"]
            # logger.info(f"config:{config}")
            ids = global_reference_map.add_references(session_id, results)
        else:
            # logger.debug("configä¸ºNoneï¼Œæ— æ³•å­˜å‚¨ reference_map")
            logger.debug("configä¸ºNoneï¼Œæ— æ³•å­˜å‚¨ reference_map\n" + "".join(traceback.format_stack()))        
        if not ids:
            # æ²¡æœ‰ idsï¼Œè¯´æ˜ config=None æˆ– add_references å¤±è´¥
            # ç›´æ¥ fallbackï¼šç”¨ enumerate çš„é¡ºåºä½œä¸ºä¸´æ—¶ id
            ids = list(range(1, len(results) + 1))
        ids, sorted_results = zip(*sorted(zip(ids, results), key=lambda x: x[0]))


        
        # è½¬æ¢ä¸º FactStructDocument
        documents = []        
        for cite_id, result in zip(ids, sorted_results):
            doc_id = f"doc_{hash(result.get('content', ''))}_{cite_id}"
            doc = FactStructDocument(
                id=doc_id,            # ç›´æ¥ä½¿ç”¨ reference idï¼ˆæ’åºåçš„ï¼‰
                cite_id=cite_id,            # cite_id åŒ doc_id
                text=result.get("content", ""),
                source_type=result.get("source", "unknown"),
                timestamp=datetime.now(),
                url=result.get("url", None),
                title=result.get("title", None),
            )
            documents.append(doc)


        return documents

    return adapter


def run_factstruct_stage1(
    query: str,
    llm: Optional[BaseChatModel] = None,
    max_iterations: int = 20,
    batch_size: int = 5,
    task_description=None,
    replan_result=None,
    factstruct_outline=None,
    factstruct_memory=None,
    initial_docs: Optional[List[FactStructDocument]] = None,
    search_engine: Optional[Callable] = None,
    config: RunnableConfig=None,
) -> Tuple[OutlineNode, Memory]:
    """
    è¿è¡Œ FactStruct Stage 1ï¼ˆä¾¿æ·æ¥å£ï¼‰

    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        llm: LLM å®ä¾‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ "outline" ç±»å‹çš„ LLMï¼‰
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰
        batch_size: æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤ 5ï¼‰
        initial_docs: åˆå§‹æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        search_engine: æœç´¢å¼•æ“å‡½æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ search_docsï¼‰

    è¿”å›:
        (outline_root, memory): æœ€ç»ˆå¤§çº²æ ¹èŠ‚ç‚¹å’Œè®°å¿†æ¨¡å—
    """
    # åˆå§‹åŒ–ç»„ä»¶
    if llm is None:
        # ä½¿ç”¨ AGENT_LLM_MAP è·å– outline å¯¹åº”çš„ LLM ç±»å‹ï¼ˆæ˜ å°„åˆ° "basic"ï¼‰
        llm_type = AGENT_LLM_MAP.get("outline", "basic")
        llm = get_llm_by_type(llm_type)

    if search_engine is None:
        search_engine = create_search_engine_adapter()

    # embedder = Embedder()
    embedder = Embedder(model_name="../../Model/MiniLM/all-MiniLM-L6-v2") 
    llm_wrapper = FactStructLLMWrapper(llm)

    # åˆ›å»º Batch-MAB å®ä¾‹
    batch_mab = BatchMAB(
        llm_wrapper=llm_wrapper,
        embedder=embedder,
        search_engine=search_engine,
        max_iterations=max_iterations,
        batch_size=batch_size,
    )

    # è¿è¡Œç®—æ³•
    central_guidance = json.dumps(task_description,ensure_ascii=False,indent=2,)
    replan_result= json.dumps(replan_result,ensure_ascii=False,indent=2,)
    logger.info(f"central_guidance{central_guidance}")
    outline_root, memory = batch_mab.run(
        initial_query=query,
        initial_docs=initial_docs,
        central_guidance=central_guidance,
        replan_result=replan_result,
        factstruct_outline=factstruct_outline,
        factstruct_memory=factstruct_memory,
        config=config,
    )

    return outline_root, memory


def outline_node_to_text(outline_root: OutlineNode) -> str:
    """
    å°† OutlineNode è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆç”¨äºä¿å­˜åˆ° Stateï¼‰

    å‚æ•°:
        outline_root: å¤§çº²æ ¹èŠ‚ç‚¹

    è¿”å›:
        æ–‡æœ¬æ ¼å¼çš„å¤§çº²
    """
    return outline_root.to_text_tree()


def outline_node_to_markdown(
    outline_root: OutlineNode,
    max_depth: Optional[int] = None,
    include_root: bool = True,
) -> str:
    """
    å°† OutlineNode è½¬æ¢ä¸º Markdown æ ¼å¼

    å‚æ•°:
        outline_root: å¤§çº²æ ¹èŠ‚ç‚¹
        max_depth: æœ€å¤§å±‚çº§æ·±åº¦ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶æ·±åº¦ï¼Œæ‰“å°å®Œæ•´å¤§çº²ï¼‰
                  - æ ¹èŠ‚ç‚¹ä¸ºç¬¬1å±‚
                  - å¦‚æœ include_root=Trueï¼Œmax_depth=3 è¡¨ç¤ºæ ¹+2å±‚å­èŠ‚ç‚¹
        include_root: æ˜¯å¦åŒ…å«æ ¹èŠ‚ç‚¹ï¼ˆé»˜è®¤Trueï¼‰

    è¿”å›:
        Markdown æ ¼å¼çš„å¤§çº²å­—ç¬¦ä¸²
    """

    def node_to_markdown(
        node: OutlineNode, current_level: int = 1, parent_indent: str = ""
    ) -> str:
        """
        é€’å½’å°†èŠ‚ç‚¹è½¬æ¢ä¸ºMarkdownæ ¼å¼

        å‚æ•°:
            node: å½“å‰èŠ‚ç‚¹
            current_level: å½“å‰å±‚çº§ï¼ˆ1è¡¨ç¤ºæ ¹èŠ‚ç‚¹ï¼Œ2è¡¨ç¤ºç¬¬ä¸€å±‚å­èŠ‚ç‚¹ï¼Œä»¥æ­¤ç±»æ¨ï¼‰
            parent_indent: çˆ¶èŠ‚ç‚¹çš„ç¼©è¿›å­—ç¬¦ä¸²
        """
        # æ£€æŸ¥æ·±åº¦é™åˆ¶
        if max_depth is not None and current_level > max_depth:
            return ""

        result = ""

        # æ ¹èŠ‚ç‚¹ç‰¹æ®Šå¤„ç†
        if current_level == 1 and include_root:
            result = f"- {node.title}\n"
            # æ ¹èŠ‚ç‚¹çš„å­èŠ‚ç‚¹åº”è¯¥æœ‰2ä¸ªç©ºæ ¼ç¼©è¿›
            child_indent = "  "
        else:
            # éæ ¹èŠ‚ç‚¹ï¼šç¼©è¿› = çˆ¶èŠ‚ç‚¹ç¼©è¿› + 2ä¸ªç©ºæ ¼
            child_indent = parent_indent + "  "

        # å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            # è®¡ç®—å½“å‰å­èŠ‚ç‚¹çš„ç¼©è¿›
            # å¦‚æœæ˜¯æ ¹èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼ˆlevel 2ï¼‰ï¼Œç¼©è¿›æ˜¯2ä¸ªç©ºæ ¼
            # å¦‚æœæ˜¯å­èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼Œç¼©è¿›æ˜¯çˆ¶èŠ‚ç‚¹ç¼©è¿› + 2ä¸ªç©ºæ ¼
            result += f"{child_indent}- {child.title}\n"

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
            if max_depth is None or current_level + 1 <= max_depth:
                child_markdown = node_to_markdown(
                    child, current_level + 1, child_indent
                )
                result += child_markdown

        return result

    markdown_text = node_to_markdown(outline_root, 1)
    return markdown_text.strip()


def outline_node_to_json(outline_root: OutlineNode) -> str:
    """
    å°† OutlineNode è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆå…¼å®¹ç°æœ‰çš„ outline æ ¼å¼ï¼‰

    å‚æ•°:
        outline_root: å¤§çº²æ ¹èŠ‚ç‚¹

    è¿”å›:
        JSON å­—ç¬¦ä¸²æ ¼å¼çš„å¤§çº²
    """

    def node_to_dict(node: OutlineNode) -> dict:
        """é€’å½’å°†èŠ‚ç‚¹è½¬æ¢ä¸ºå­—å…¸"""
        result = {"title": node.title, "children": []}
        for child in node.children:
            result["children"].append(node_to_dict(child))
        return result

    import json

    outline_dict = node_to_dict(outline_root)
    return json.dumps(outline_dict, ensure_ascii=False, indent=2)


def memory_to_dict(memory: Memory) -> dict:
    """
    å°† Memory å®ä¾‹è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºåºåˆ—åŒ–åˆ° Stateï¼‰

    æ³¨æ„ï¼šembedding ä¿¡æ¯ä¼šè¢«ä¸¢å¼ƒï¼Œåªä¿ç•™æ–‡æ¡£å…ƒæ•°æ®ã€‚

    å‚æ•°:
        memory: Memory å®ä¾‹

    è¿”å›:
        å­—å…¸æ ¼å¼çš„å†…å­˜æ•°æ®
    """
    return {
        "total_documents": len(memory.documents),
        "node_to_docs": {
            node_id: list(doc_ids) for node_id, doc_ids in memory.node_to_docs.items()
        },
        "documents": {
            doc_id: doc.to_dict() for doc_id, doc in memory.documents.items()
        },
    }




def outline_node_to_dict(node: OutlineNode) -> dict:
    """
    å°† OutlineNode å®Œæ•´è½¬æ¢ä¸ºå­—å…¸ï¼ˆä¿ç•™æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬ MAB çŠ¶æ€ï¼‰

    å‚æ•°:
        node: OutlineNode å®ä¾‹

    è¿”å›:
        å­—å…¸æ ¼å¼çš„èŠ‚ç‚¹æ•°æ®ï¼ˆå¯é€’å½’åŒ…å«å­èŠ‚ç‚¹ï¼‰
    """
    return {
        "id": node.id,
        "title": node.title,
        "pull_count": node.pull_count,
        "reward_history": node.reward_history,
        "word_limit": node.word_limit,
        "children": [outline_node_to_dict(child) for child in node.children],
    }


def dict_to_outline_node(data: dict, parent: Optional[OutlineNode] = None) -> OutlineNode:
    """
    ä»å­—å…¸æ¢å¤ OutlineNodeï¼ˆé€’å½’æ„å»ºå­æ ‘ï¼‰

    å‚æ•°:
        data: èŠ‚ç‚¹å­—å…¸æ•°æ®
        parent: çˆ¶èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰

    è¿”å›:
        OutlineNode å®ä¾‹
    """
    node = OutlineNode(
        id=data["id"],
        title=data["title"],
        parent=parent,
        children=[],
        pull_count=data.get("pull_count", 0),
        reward_history=data.get("reward_history", []),
        word_limit=data.get("word_limit", 0),
    )

    for child_data in data.get("children", []):
        child = dict_to_outline_node(child_data, parent=node)
        node.children.append(child)

    return node


def dict_to_memory(data: dict) -> Memory:
    """
    ä»å­—å…¸æ¢å¤ Memory å®ä¾‹

    å‚æ•°:
        data: å†…å­˜å­—å…¸æ•°æ®

    è¿”å›:
        Memory å®ä¾‹
    """
    from .memory import Memory
    from .document import FactStructDocument

    memory = Memory(embedding_dim=384)

    for doc_id, doc_data in data.get("documents", {}).items():
        doc = FactStructDocument.from_dict(doc_data)
        memory.documents[doc_id] = doc

    for node_id, doc_ids in data.get("node_to_docs", {}).items():
        memory.node_to_docs[node_id] = set(doc_ids)

    return memory

def run_factstruct_stage2(
    outline_dict: dict,
    memory_dict: dict,
    user_query: str,
    llm_type: str = "basic",
    locale: str = "zh-CN",
) -> str:
    """
    FactStruct Stage 2: åŸºäºå¤§çº²çš„é€’å½’åˆ†æ®µæ–‡æœ¬ç”Ÿæˆ

    é‡‡ç”¨æ·±åº¦ä¼˜å…ˆéå†ç­–ç•¥ï¼Œé€’å½’ç”ŸæˆæŠ¥å‘Šï¼š
    1. ä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’éå†æ•´æ£µå¤§çº²æ ‘
    2. é‡åˆ°å¶å­èŠ‚ç‚¹ï¼š
       - ä» Memory ä¸­ç›´æ¥è·å– Stage 1 å…³è”çš„æ–‡æ¡£ï¼ˆä½¿ç”¨èŠ‚ç‚¹-æ–‡æ¡£æ˜ å°„ï¼‰
       - ä½¿ç”¨ LLM ç”Ÿæˆè¯¥èŠ‚ç‚¹çš„æ®µè½å†…å®¹
       - æ·»åŠ åˆ°æŠ¥å‘Šä¸­
    3. ä¸­é—´èŠ‚ç‚¹ï¼šæ·»åŠ æ ‡é¢˜ï¼Œç»§ç»­é€’å½’å­èŠ‚ç‚¹

    å‚æ•°:
        outline_dict: OutlineNode åºåˆ—åŒ–å­—å…¸
        memory_dict: Memory åºåˆ—åŒ–å­—å…¸
        user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
        llm_type: LLM ç±»å‹ï¼ˆé»˜è®¤ "basic"ï¼‰
        locale: è¯­è¨€åŒºåŸŸè®¾ç½®ï¼ˆé»˜è®¤ "zh-CN"ï¼‰ï¼Œç”¨äº prompt æ¨¡æ¿

    è¿”å›:
        å®Œæ•´çš„ Markdown æ ¼å¼æŠ¥å‘Š
    """
    from src.llms.llm import get_llm_by_type
    from src.prompts.template import apply_prompt_template

    logger.info(f"å¼€å§‹ FactStruct Stage 2: åŸºäºå¤§çº²åˆ†æ®µç”Ÿæˆå†…å®¹...")

    outline_root = dict_to_outline_node(outline_dict)
    memory = dict_to_memory(memory_dict)

    # ç”Ÿæˆå®Œæ•´å¤§çº²çš„ Markdown è¡¨ç¤º
    full_outline = outline_node_to_markdown(outline_root, max_depth=None, include_root=True)

    llm = get_llm_by_type(llm_type)
    report_parts = []
    path_stack = [[]]

    #åˆå§‹åŒ– NLI æ¨¡å‹
    nli_model_path="/data1/Yangzb/Model/nlp_structbert_nli_chinese-tiny"
    # semantic_cls = pipeline(Tasks.nli,nli_model_path,model_revision='master')
    semantic_cls = CrossEncoder("/data1/Yangzb/Model/StructBert/cross-encoder/nli-deberta-v3-small")


    def get_progress_context(stack, will_complete_chapters: list, next_chapter: str):
        context_lines = []
        
        context_lines.append("å½“å‰æ–‡ç« å†™ä½œè¿›åº¦ï¼š")

        for i, level_nodes in enumerate(stack):
            indent = "  " * i
            current_node_title = level_nodes[-1]
            completed_siblings = level_nodes[:-1]
            if completed_siblings:
                siblings_str = "ã€".join(completed_siblings)
                context_lines.append(f"{indent}å…¶ä¸­å·²å®Œæˆ{siblings_str}ï¼Œ")
            context_lines.append(f"{indent}æ­£åœ¨å®Œæˆ{current_node_title}")

        if will_complete_chapters:
            chapters_str = "ã€".join([f"ã€Œ{title}ã€" for title in will_complete_chapters])
            context_lines.append(f"\nå®Œæˆå½“å‰ç« èŠ‚åï¼Œä»¥ä¸‹çˆ¶ç« èŠ‚ä¹Ÿå°†å®Œæˆï¼š{chapters_str}")

        if next_chapter:
            context_lines.append(f"å½“å‰ç« èŠ‚å®Œæˆåçš„ä¸‹ä¸€ä¸ªç« èŠ‚ä¸ºï¼š{next_chapter}")
        else:
            context_lines.append("å½“å‰ç« èŠ‚å®Œæˆåæ•´ç¯‡æ–‡ç« å°†å…¨éƒ¨å®Œæˆ")

        return "\n".join(context_lines)

    def generate(node: OutlineNode, level: int = 1, will_complete_chapters: list = None, next_chapter: str = None,semantic_cls=None):
        logger.debug(f"æ­£åœ¨ç”Ÿæˆå­ç« èŠ‚: {node.title}ï¼ˆID: {node.id}ï¼‰")

        path_stack[-1].append(node.title)

        if level <= 6:
            report_parts.append(f"{'#' * level} {node.title}\n")

        if node.is_leaf():
            relevant_docs = memory.get_docs_by_node(node.id)

            # å¤„ç†å­—æ•°é™åˆ¶
            word_limit = None#æ˜¯é›¶å°±ä¸å¤„ç†
            logger.info(f"node{node}")
            logger.info(f"node.word_limit = {node.word_limit}, type = {type(node.word_limit)}")

            if isinstance(node.word_limit, int) and node.word_limit > 0: #æ˜¯æ­£æ•´æ•°å°±å¤„ç†
                word_limit = node.word_limit

            if not relevant_docs:
                logger.warning(
                    f"èŠ‚ç‚¹ '{node.title}' (ID: {node.id}) æœªæ‰¾åˆ°å…³è”æ–‡æ¡£"
                )
                relevant_docs_text = "ï¼ˆæ— ç›¸å…³èµ„æ–™ï¼‰"
            else:
                # logger.debug(
                #     f"è·å–åˆ° {len(relevant_docs)} ä¸ª Stage 1 å…³è”æ–‡æ¡£"
                # )
                relevant_docs_text = "\n\n".join(
                    [
                        f"[{doc.cite_id}] æ¥æº: {doc.source_type}\n{doc.text[:500]}..."
                        for idx, doc in enumerate(relevant_docs)
                    ]
                )
                # logger.info(f"relevant_docs_text :{relevant_docs_text }")
                logger.info(f"relevant_docs :{relevant_docs}")
            progress_context = get_progress_context(path_stack, will_complete_chapters, next_chapter)

            completed_content = "".join(report_parts).strip()
            if not completed_content:
                completed_content = "ï¼ˆå°šæœªç”Ÿæˆä»»ä½•å†…å®¹ï¼‰"

            temp_state = {
                "messages": [],
                "user_query": user_query,
                "full_outline": full_outline,
                "progress_context": progress_context,
                "completed_content": completed_content,
                "reference_materials": relevant_docs_text,
                "locale": locale,
                "word_limit": word_limit,   #è¯æ•°é™åˆ¶
            }

            try:
                messages = apply_prompt_template(
                    "reporter_factstruct",
                    temp_state,
                    extra_context={
                        "user_query": user_query,
                        "full_outline": full_outline,
                        "progress_context": progress_context,
                        "completed_content": completed_content,
                        "reference_materials": relevant_docs_text,
                        "locale": locale,
                        "word_limit": word_limit,   #è¯æ•°é™åˆ¶
                    }
                )
                logger.info(f"messages:{messages}")
                response = llm.invoke(messages)
                content = response.content.strip()
                report_parts.append(f"{content}\n")
                logger.debug(f"  ç”Ÿæˆäº† {len(content)} ä¸ªå­—ç¬¦")
                
                #å¦‚æœæ²¡æ–‡æ¡£å°±ä¸åšå¼•ç”¨æ£€æŸ¥äº†ï¼Œåé¢å†è€ƒè™‘ä¸Šæ–‡çš„å¼•ç”¨
                if not relevant_docs:
                    logger.warning(
                        f"èŠ‚ç‚¹ '{node.title}' (ID: {node.id}) æœªæ‰¾åˆ°å…³è”æ–‡æ¡£ï¼Œä¸è¿›è¡Œå¼•ç”¨æ£€æŸ¥"
                    )
                else:
                    logger.info(f"content :{content}")
                    logger.info(f"relevant_docs:{relevant_docs}")
                    #è¿™ä¸ªæ˜¯åˆ¤æ–­å¼•ç”¨å’Œå¥å­çš„å…³ç³»
                    supported = filter_content_by_relevant_docs(
                        content=content,
                        relevant_docs=relevant_docs,
                        semantic_cls=semantic_cls
                    )
                    logger.info(f"supported :{supported}")
                    
                    #è¿™ä¸ªæ˜¯æŠŠå…³ç³»åº”ç”¨åˆ°ç”Ÿæˆæ–‡ç« ä¸Š
                    new_content = mark_content_with_support(
                        content=content,
                        nli_results=supported
                    )
                    logger.info(f"new_content :{new_content}")
                    
                    #è¿™ä¸ªæ˜¯æŠŠé”™è¯¯å¼•ç”¨è¿›è¡Œå¤„ç†çš„
                    content=repair_unknown_citations(
                        content=new_content,
                        relevant_docs=relevant_docs,
                        semantic_cls=semantic_cls
                    )
                    logger.info(f"content :{content}")
                    
            except Exception as e:
                logger.error(f"  ç”Ÿæˆå¤±è´¥: {str(e)}")

        if node.children:
            path_stack.append([])
            for i, child in enumerate(node.children):
                if (i == len(node.children) - 1):
                    child_will_complete = will_complete_chapters + [node.title]
                    child_next_chapter = next_chapter
                else:
                    child_will_complete = []
                    child_next_chapter = node.children[i + 1].title
                generate(child, level + 1, child_will_complete, child_next_chapter,semantic_cls=semantic_cls)
            path_stack.pop()

    generate(outline_root, level=1, will_complete_chapters=[], next_chapter=None,semantic_cls=semantic_cls)

    final_report = "\n".join(report_parts)

    logger.info(
        f"FactStruct Stage 2 å®Œæˆ: ç”Ÿæˆäº† {len(final_report)} ä¸ªå­—ç¬¦çš„æŠ¥å‘Š"
    )

    return final_report



def visualize_outline_with_citations(
    outline_root: OutlineNode,
    memory: Memory,
    output_path: Optional[str] = None,
    print_text: bool = True,
) -> str:
    """
    å¯è§†åŒ–å¤§çº²æ ‘åŠå…¶å¼•æ–‡æ˜ å°„å…³ç³»ã€‚

    å‚æ•°:
        outline_root: å¤§çº²æ ¹èŠ‚ç‚¹
        memory: Memory å®ä¾‹ï¼ŒåŒ…å«èŠ‚ç‚¹åˆ°æ–‡æ¡£çš„æ˜ å°„
        output_path: å›¾ç‰‡è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼Œéœ€è¦å®‰è£… graphvizï¼‰
        print_text: æ˜¯å¦æ‰“å°æ–‡æœ¬æ ¼å¼

    è¿”å›:
        æ–‡æœ¬æ ¼å¼çš„å¤§çº²æ ‘ï¼ˆå¸¦å¼•æ–‡æ˜ å°„ï¼‰
    """
    lines = []
    lines.append("=" * 80)
    lines.append("å¤§çº²æ ‘ä¸å¼•æ–‡æ˜ å°„å…³ç³»")
    lines.append("=" * 80)

    def format_node(node: OutlineNode, indent: int = 0) -> None:
        """é€’å½’æ ¼å¼åŒ–èŠ‚ç‚¹"""
        prefix = "  " * indent

        # è·å–è¯¥èŠ‚ç‚¹çš„å¼•æ–‡
        doc_ids = memory.node_to_docs.get(node.id, set())
        docs = [
            memory.documents.get(doc_id)
            for doc_id in doc_ids
            if doc_id in memory.documents
        ]

        # èŠ‚ç‚¹æ ‡é¢˜
        citation_count = len(docs)
        if citation_count > 0:
            lines.append(
                f"{prefix}â”œâ”€ {node.title} [ID: {node.id}] ğŸ“š {citation_count} ç¯‡å¼•æ–‡"
            )
        else:
            lines.append(f"{prefix}â”œâ”€ {node.title} [ID: {node.id}] âš ï¸ æ— å¼•æ–‡")

        # æ˜¾ç¤ºå¼•æ–‡è¯¦æƒ…ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
        for i, doc in enumerate(docs[:3]):  # æœ€å¤šæ˜¾ç¤º3ç¯‡
            if doc:
                doc_title = (doc.title or doc.text[:50] + "...") if doc.text else "æœªçŸ¥"
                lines.append(f"{prefix}â”‚    â””â”€ ğŸ“„ [{i+1}] {doc_title[:60]}")

        if len(docs) > 3:
            lines.append(f"{prefix}â”‚    â””â”€ ... è¿˜æœ‰ {len(docs) - 3} ç¯‡å¼•æ–‡")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            format_node(child, indent + 1)

    format_node(outline_root)

    # ç»Ÿè®¡ä¿¡æ¯
    all_nodes = outline_root.get_all_nodes()
    nodes_with_citations = sum(
        1
        for n in all_nodes
        if n.id in memory.node_to_docs and memory.node_to_docs[n.id]
    )

    lines.append("")
    lines.append("=" * 80)
    lines.append("ç»Ÿè®¡ä¿¡æ¯")
    lines.append("=" * 80)
    lines.append(f"æ€»èŠ‚ç‚¹æ•°: {len(all_nodes)}")
    lines.append(f"æœ‰å¼•æ–‡çš„èŠ‚ç‚¹æ•°: {nodes_with_citations}")
    lines.append(f"æ— å¼•æ–‡çš„èŠ‚ç‚¹æ•°: {len(all_nodes) - nodes_with_citations}")
    lines.append(f"å¼•æ–‡è¦†ç›–ç‡: {nodes_with_citations / len(all_nodes) * 100:.1f}%")
    lines.append(f"æ€»æ–‡æ¡£æ•°: {len(memory.documents)}")
    lines.append("=" * 80)

    text_output = "\n".join(lines)

    if print_text:
        logger.info(f"\n{text_output}")

    # å°è¯•ç”Ÿæˆ graphviz å›¾ç‰‡
    if output_path:
        try:
            _generate_graphviz_image(outline_root, memory, output_path)
            logger.info(f"å¤§çº²å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.warning(f"æ— æ³•ç”Ÿæˆ graphviz å›¾ç‰‡: {e}")

    return text_output


def _generate_graphviz_image(
    outline_root: OutlineNode,
    memory: Memory,
    output_path: str,
) -> None:
    """
    ä½¿ç”¨ graphviz ç”Ÿæˆå¤§çº²æ ‘å¯è§†åŒ–å›¾ç‰‡ã€‚

    å‚æ•°:
        outline_root: å¤§çº²æ ¹èŠ‚ç‚¹
        memory: Memory å®ä¾‹
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
    """
    try:
        from graphviz import Digraph
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… graphviz: pip install graphviz")

    dot = Digraph(comment="Outline Tree with Citations")
    dot.attr(rankdir="TB", splines="ortho")
    dot.attr("node", shape="box", style="rounded,filled", fontname="SimHei")

    def add_node(node: OutlineNode) -> None:
        """é€’å½’æ·»åŠ èŠ‚ç‚¹"""
        doc_ids = memory.node_to_docs.get(node.id, set())
        doc_count = len(doc_ids)

        # æ ¹æ®å¼•æ–‡æ•°é‡è®¾ç½®é¢œè‰²
        if doc_count == 0:
            color = "#ffcccc"  # çº¢è‰² - æ— å¼•æ–‡
        elif doc_count <= 2:
            color = "#ffffcc"  # é»„è‰² - å°‘é‡å¼•æ–‡
        else:
            color = "#ccffcc"  # ç»¿è‰² - æœ‰å¼•æ–‡

        # æˆªæ–­æ ‡é¢˜
        title = node.title[:30] + "..." if len(node.title) > 30 else node.title
        label = f"{title}\\nğŸ“š {doc_count} docs"

        dot.node(node.id, label, fillcolor=color)

        for child in node.children:
            add_node(child)
            dot.edge(node.id, child.id)

    add_node(outline_root)

    # ä¿å­˜å›¾ç‰‡
    dot.render(output_path, format="png", cleanup=True)
