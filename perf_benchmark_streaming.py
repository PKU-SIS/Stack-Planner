#!/usr/bin/env python3
"""
LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•æŒ‡æ ‡ï¼š
- TTFT (Time to First Token): é¦–å­—å»¶è¿Ÿ
- Generation Speed: ç”Ÿæˆé€Ÿåº¦ (tokens/s)
- Max Concurrent Requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
"""

import argparse
import asyncio
import statistics
import time
from dataclasses import dataclass, field
from typing import List, Optional

from openai import OpenAI, AsyncOpenAI


# ============ é…ç½® ============

API_KEY = "sk-d47ad54165ee456093bc9ffd599e354e"
BASE_URL = "http://123.57.228.132:8285/v1"
MODEL = "Qwen2.5-32B-Instruct"
MAX_TOKENS = 2048  # è®¾ç½®è¾ƒå¤§çš„ max_tokens ä»¥è·å–æ›´å‡†ç¡®çš„é€Ÿåº¦æµ‹é‡
TEST_PROMPT = "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ï¼ŒåŒ…æ‹¬é‡å­æ¯”ç‰¹ã€å åŠ æ€ã€çº ç¼ ç­‰æ¦‚å¿µï¼Œä»¥åŠå®ƒä¸ç»å…¸è®¡ç®—çš„åŒºåˆ«ã€‚"


# ============ æ•°æ®ç»“æ„ ============


@dataclass
class StreamingMetrics:
    """æµå¼è¾“å‡ºæ€§èƒ½æŒ‡æ ‡"""

    ttft: float = 0.0  # Time to First Token (ç§’)
    total_time: float = 0.0  # æ€»è€—æ—¶ (ç§’)
    generation_time: float = 0.0  # ç”Ÿæˆæ—¶é—´ï¼ˆæ’é™¤ TTFTï¼‰
    total_tokens: int = 0  # æ€» token æ•°ï¼ˆä» API usage è·å–ï¼‰
    output_tokens: int = 0  # è¾“å‡º token æ•°
    tokens_per_second: float = 0.0  # tokens/ç§’ï¼ˆåŸºäºç”Ÿæˆæ—¶é—´ï¼‰
    success: bool = False
    error: Optional[str] = None


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""

    ttft_results: List[float] = field(default_factory=list)
    speed_results: List[float] = field(default_factory=list)
    total_times: List[float] = field(default_factory=list)
    total_tokens_list: List[int] = field(default_factory=list)
    success_count: int = 0
    fail_count: int = 0
    errors: List[str] = field(default_factory=list)


# ============ å•æ¬¡æµå¼è¯·æ±‚ ============


def run_streaming_request(prompt: str, verbose: bool = True) -> StreamingMetrics:
    """
    æ‰§è¡Œå•æ¬¡æµå¼è¯·æ±‚å¹¶æµ‹é‡æ€§èƒ½æŒ‡æ ‡
    """
    metrics = StreamingMetrics()
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

    try:
        request_start_time = time.perf_counter()
        first_token_received = False

        if verbose:
            print("      --- å®æ—¶è¾“å‡º ---", flush=True)

        stream = client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=MAX_TOKENS,
            stream=True,
            stream_options={"include_usage": True},  # è¯·æ±‚è¿”å› token ä½¿ç”¨ç»Ÿè®¡
        )

        first_token_time = None
        chunk_count = 0

        for chunk in stream:
            # è·å– usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunkï¼‰
            if hasattr(chunk, "usage") and chunk.usage:
                metrics.total_tokens = chunk.usage.total_tokens
                metrics.output_tokens = chunk.usage.completion_tokens

            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                chunk_count += 1

                if not first_token_received:
                    metrics.ttft = time.perf_counter() - request_start_time
                    first_token_time = time.perf_counter()
                    first_token_received = True
                    if verbose:
                        print(
                            f"\n      [é¦–å­—åˆ°è¾¾] TTFT={metrics.ttft:.3f}s\n", flush=True
                        )

                if verbose:
                    print(content, end="", flush=True)

        metrics.total_time = time.perf_counter() - request_start_time

        # è®¡ç®—ç”Ÿæˆæ—¶é—´ï¼ˆæ’é™¤ TTFTï¼‰
        if first_token_time:
            metrics.generation_time = time.perf_counter() - first_token_time

        # å¦‚æœ API æ²¡è¿”å› usageï¼Œç”¨ chunk æ•°ä¼°ç®—
        if metrics.output_tokens == 0:
            metrics.output_tokens = chunk_count

        # åŸºäºç”Ÿæˆæ—¶é—´è®¡ç®—é€Ÿåº¦ï¼ˆæ›´å‡†ç¡®ï¼‰
        if metrics.generation_time > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.generation_time
        elif metrics.total_time > 0:
            metrics.tokens_per_second = metrics.output_tokens / metrics.total_time

        metrics.success = True

        if verbose:
            print(f"\n      --- è¾“å‡ºç»“æŸ ---", flush=True)
            print(
                f"      [å®Œæˆ] æ€»è€—æ—¶ {metrics.total_time:.2f}s, ç”Ÿæˆ {metrics.output_tokens} tokens (ç”Ÿæˆæ—¶é—´ {metrics.generation_time:.2f}s)",
                flush=True,
            )

    except Exception as e:
        metrics.error = f"{type(e).__name__}: {str(e)}"
        if verbose:
            print(f"\n      [é”™è¯¯] {metrics.error}", flush=True)

    return metrics


async def run_streaming_request_async(
    prompt: str, request_id: int = 0
) -> StreamingMetrics:
    """
    å¼‚æ­¥æ‰§è¡Œå•æ¬¡æµå¼è¯·æ±‚ï¼ˆç”¨äºå¹¶å‘æµ‹è¯•ï¼‰
    """
    metrics = StreamingMetrics()
    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

    try:
        request_start_time = time.perf_counter()
        first_token_received = False

        stream = await client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
        )

        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                if not first_token_received:
                    metrics.ttft = time.perf_counter() - request_start_time
                    first_token_received = True
                metrics.total_tokens += 1

        metrics.total_time = time.perf_counter() - request_start_time
        if metrics.total_time > 0:
            metrics.tokens_per_second = metrics.total_tokens / metrics.total_time
        metrics.success = True

    except Exception as e:
        metrics.error = f"{type(e).__name__}: {str(e)}"

    return metrics


# ============ TTFT & Speed æµ‹è¯• ============


def run_ttft_speed_benchmark(
    iterations: int = 3, verbose: bool = True
) -> BenchmarkResult:
    """
    è¿è¡Œ TTFT å’Œ Generation Speed åŸºå‡†æµ‹è¯•
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š TTFT & Generation Speed æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)")
    print(f"{'='*60}\n")

    result = BenchmarkResult()

    for i in range(iterations):
        print(f"è¿­ä»£ {i+1}/{iterations}:")
        metrics = run_streaming_request(TEST_PROMPT, verbose=verbose)

        if metrics.success:
            result.ttft_results.append(metrics.ttft)
            result.speed_results.append(metrics.tokens_per_second)
            result.total_times.append(metrics.total_time)
            result.total_tokens_list.append(metrics.output_tokens)
            result.success_count += 1
            print(
                f"    TTFT: {metrics.ttft:.3f}s | "
                f"Speed: {metrics.tokens_per_second:.1f} tokens/s | "
                f"GenTime: {metrics.generation_time:.2f}s | "
                f"Tokens: {metrics.output_tokens}"
            )
        else:
            result.fail_count += 1
            result.errors.append(metrics.error or "Unknown error")
            print(f"    âŒ å¤±è´¥: {metrics.error}")

        print()

    return result


# ============ å¹¶å‘æµ‹è¯• ============


async def run_concurrent_test(concurrency: int) -> BenchmarkResult:
    """
    è¿è¡ŒæŒ‡å®šå¹¶å‘æ•°çš„æµ‹è¯•
    """
    result = BenchmarkResult()

    tasks = [run_streaming_request_async(TEST_PROMPT, i) for i in range(concurrency)]

    metrics_list = await asyncio.gather(*tasks, return_exceptions=True)

    for metrics in metrics_list:
        if isinstance(metrics, Exception):
            result.fail_count += 1
            result.errors.append(str(metrics))
        elif metrics.success:
            result.ttft_results.append(metrics.ttft)
            result.speed_results.append(metrics.tokens_per_second)
            result.total_times.append(metrics.total_time)
            result.total_tokens_list.append(metrics.total_tokens)
            result.success_count += 1
        else:
            result.fail_count += 1
            result.errors.append(metrics.error or "Unknown error")

    return result


def run_concurrency_benchmark(max_concurrency: int = 10, step: int = 2) -> dict:
    """
    è¿è¡Œæœ€å¤§å¹¶å‘è¯·æ±‚æ•°æµ‹è¯•
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Max Concurrent Requests æµ‹è¯•")
    print(f"{'='*60}\n")

    results = {}

    for concurrency in range(1, max_concurrency + 1, step):
        print(f"æµ‹è¯•å¹¶å‘æ•°: {concurrency}")

        result = asyncio.run(run_concurrent_test(concurrency))
        results[concurrency] = result

        if result.success_count > 0:
            avg_ttft = statistics.mean(result.ttft_results)
            avg_speed = statistics.mean(result.speed_results)
            print(
                f"  æˆåŠŸ: {result.success_count}/{concurrency} | "
                f"å¹³å‡ TTFT: {avg_ttft:.3f}s | "
                f"å¹³å‡ Speed: {avg_speed:.1f} tokens/s"
            )
        else:
            print(f"  å…¨éƒ¨å¤±è´¥!")

        # å¦‚æœå¤±è´¥ç‡è¶…è¿‡ 50%ï¼Œåœæ­¢æµ‹è¯•
        if result.fail_count > concurrency / 2:
            print(f"\nâš ï¸ å¤±è´¥ç‡è¿‡é«˜ï¼Œåœæ­¢å¹¶å‘æµ‹è¯•")
            break

    return results


# ============ ç»“æœæ±‡æ€» ============


def print_summary(
    ttft_result: BenchmarkResult, concurrency_results: Optional[dict] = None
):
    """
    æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœæ‘˜è¦")
    print(f"{'='*60}\n")

    print(f"æµ‹è¯•é…ç½®:")
    print(f"  - API åœ°å€: {BASE_URL}")
    print(f"  - æ¨¡å‹: {MODEL}")
    print(
        f"  - æˆåŠŸç‡: {ttft_result.success_count}/{ttft_result.success_count + ttft_result.fail_count}"
    )
    print()

    # TTFT ç»Ÿè®¡
    if ttft_result.ttft_results:
        print("ã€TTFT (Time to First Token)ã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.ttft_results):.3f}s")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.ttft_results):.3f}s")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.ttft_results):.3f}s")
        if len(ttft_result.ttft_results) > 1:
            print(f"  æ ‡å‡†å·®: {statistics.stdev(ttft_result.ttft_results):.3f}s")
        print(f"  P50:    {statistics.median(ttft_result.ttft_results):.3f}s")
        print()

    # ç”Ÿæˆé€Ÿåº¦ç»Ÿè®¡
    if ttft_result.speed_results:
        print("ã€Generation Speedã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.speed_results):.1f} tokens/s")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.speed_results):.1f} tokens/s")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.speed_results):.1f} tokens/s")
        if len(ttft_result.speed_results) > 1:
            print(
                f"  æ ‡å‡†å·®: {statistics.stdev(ttft_result.speed_results):.1f} tokens/s"
            )
        print(f"  P50:    {statistics.median(ttft_result.speed_results):.1f} tokens/s")
        print()

    # æ€» token æ•°ç»Ÿè®¡
    if ttft_result.total_tokens_list:
        print("ã€Total Tokensã€‘")
        print(f"  æœ€å°å€¼: {min(ttft_result.total_tokens_list)}")
        print(f"  æœ€å¤§å€¼: {max(ttft_result.total_tokens_list)}")
        print(f"  å¹³å‡å€¼: {statistics.mean(ttft_result.total_tokens_list):.0f}")
        print()

    # å¹¶å‘æµ‹è¯•ç»“æœ
    if concurrency_results:
        print("ã€Max Concurrent Requestsã€‘")
        max_stable_concurrency = 0
        for concurrency, result in concurrency_results.items():
            if result.success_count == concurrency:
                max_stable_concurrency = concurrency

        if max_stable_concurrency > 0:
            print(f"  âœ… æœ€å¤§ç¨³å®šå¹¶å‘æ•°: {max_stable_concurrency}")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°ç¨³å®šå¹¶å‘çº§åˆ«")
        print()

    # é”™è¯¯æ±‡æ€»
    if ttft_result.errors:
        print("ã€é”™è¯¯æ±‡æ€»ã€‘")
        for i, err in enumerate(ttft_result.errors[:5], 1):
            print(f"  {i}. {err}")
        if len(ttft_result.errors) > 5:
            print(f"  ... è¿˜æœ‰ {len(ttft_result.errors) - 5} ä¸ªé”™è¯¯")
        print()


# ============ ä¸»å‡½æ•° ============


def main():
    parser = argparse.ArgumentParser(description="LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument(
        "-n",
        "--iterations",
        type=int,
        default=3,
        help="TTFT/Speed æµ‹è¯•è¿­ä»£æ¬¡æ•° (é»˜è®¤: 3)",
    )
    parser.add_argument(
        "-c",
        "--max-concurrency",
        type=int,
        default=0,
        help="æœ€å¤§å¹¶å‘æµ‹è¯•æ•° (é»˜è®¤: 0, ä¸æµ‹è¯•å¹¶å‘)",
    )
    parser.add_argument(
        "--concurrency-step", type=int, default=2, help="å¹¶å‘æµ‹è¯•æ­¥é•¿ (é»˜è®¤: 2)"
    )
    parser.add_argument(
        "-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºå®æ—¶è¾“å‡º"
    )
    parser.add_argument("--prompt", type=str, default=None, help="è‡ªå®šä¹‰æµ‹è¯• prompt")
    args = parser.parse_args()

    global TEST_PROMPT
    if args.prompt:
        TEST_PROMPT = args.prompt

    print(f"\n{'='*60}")
    print(f"ğŸš€ LLM æµå¼è¾“å‡ºæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print(f"{'='*60}\n")

    print(f"é…ç½®:")
    print(f"  - API åœ°å€: {BASE_URL}")
    print(f"  - æ¨¡å‹: {MODEL}")
    print(f"  - TTFT/Speed è¿­ä»£æ¬¡æ•°: {args.iterations}")
    print(
        f"  - æœ€å¤§å¹¶å‘æµ‹è¯•: {'è·³è¿‡' if args.max_concurrency == 0 else args.max_concurrency}"
    )

    # è¿è¡Œ TTFT å’Œ Speed æµ‹è¯•
    ttft_result = run_ttft_speed_benchmark(args.iterations, verbose=not args.quiet)

    # è¿è¡Œå¹¶å‘æµ‹è¯•
    concurrency_results = None
    if args.max_concurrency > 0:
        concurrency_results = run_concurrency_benchmark(
            args.max_concurrency, args.concurrency_step
        )

    # æ‰“å°æ±‡æ€»
    print_summary(ttft_result, concurrency_results)


if __name__ == "__main__":
    main()
